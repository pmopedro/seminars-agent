{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6795146-6efa-4e3e-8273-ddf392461333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3902fbb4-a866-46e2-871a-1b150a6cdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ['USER_AGENT']='ProfessorAgent'\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a8fa91e-54a7-4f76-8f1a-04205ef6caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Vald\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "raw_documents = TextLoader(\"../data/data.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0414236e-c636-48a0-beb0-e9987db5254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0dbbb7a7-bfed-4e82-a0f8-a59626fb1da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324993"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2f86686-6b14-4f73-b1c0-607e9097f85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tion virtualization because I will come back to this once I explained Holograms and volumetric media delivery then I will also Explain how virtual Network functions can be of interest so if we consider a home environments then there are multiple devices and in your home and typically there is a home Gateway. And on the home K3, we have different functionality. There is the HTTP server. There is an entity router firewall also quality of experience monitoring software for instance. if we have TV streams or video streams that we display on screens inside your home then we also monitor the quality of experience inside the home Gateway. And then typical Auto device is a setup box. so stb. Which contains? for a digital TV storage so that you can record movies and TV programs. Middleware that allows you to interact through remote control. And to start recording stop recording play a certain TV program. and so on for unless we need to middleware on the setup box. And also we have a streaming client. So this is the software that starts streaming the files and make sure that we can see the videos on the screen. so if you look at these functionalities on the home Gateway and Books, these are typically called Network functions. And when they are executed on these boxes Then they are executed inside your homes. but if we virtualize them so virtualized Network functions vnfs then They will be. executed not in your home, but in a remote Data Center. and the data center can be close by but can also be far away and another country, but then we software rise this this functions and and they run as software components inside remote Data Center. And this is a very important Evolution because then we don't need these devices in your home. You just need to connect the different screens. to the network sockets so that they have internet connectivity, but but all the other functions that are listed here. They are no longer executed inside your home. and there are many advantages first from point of view of a network operator. Because we can maximize resource utilization. and also optimize energy usage because if all the functions they run on separate devices in each home. They need to run permanently. but if if they are in a data center then we can run many of them on on the same machine. making use of virtual machines or containers inside the data center. and also in terms of Updates, this is a very important Advantage when we update Network functions, then we can very fast and easy deploy them. And and configure them. and it's much easier than changing software on a dedicated device at home in a data center. We can overnight quickly and efficiently updates Network functions. And also for the service provider there are many advantages. Which are listed here. so we can dynamically scale Network and Computing and storage resources. based on on service requirements. so we can make use of elasticity properties of cloud computing. and we have a reduced time to market for se\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[5000:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cffff3fb-443b-4129-97fb-34fb2fa8af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = DocArrayInMemorySearch.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e623107-eaa9-4e0c-9da9-bb87caf9d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05265df2-f7f7-4f18-a73c-aee55b719e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daf54f0a-38b9-42d0-9247-630cdee34d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\", max_tokens=2048)\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    all_splits,\n",
    "    embeddings\n",
    ")\n",
    "retriever = db.as_retriever()\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab57fd54-2851-48e2-8e9d-8bece5d8f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def find_seminar_quote(text: str) -> str:\n",
    "    \"\"\" Receives a question from the user about a seminar and returns a description of it from the transcript.\n",
    "    \"\"\"\n",
    "\n",
    "    query =  f\"\"\"Please gather more information about '''{text}''' and summarize the output\"\"\"\n",
    "    response = qa_stuff.run(query)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "772ae1eb-5423-4783-8a48-ff3916dfe78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools=[find_seminar_quote]\n",
    "tools = tools # +  [TavilySearchResults(max_results=1)]# + load_tools([\"wikipedia\"], llm=llm)\n",
    "agent= initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b3fb817-0175-44f3-a124-5b2c47ebfa5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use the `find_seminar_quote` tool to gather information on the most efficient technique to prompt an LLM.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"most efficient technique to prompt an LLM\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI don't have specific information on the most efficient technique to prompt a Large Language Model (LLM). However, based on the context provided, it seems that formatting or prompting engineering is a powerful tool to maximize the capabilities of autocomplete systems like LLMs. One approach mentioned is using demonstrations or chaining thoughts to provide examples for the LLM to learn from. Another technique discussed is editing within a sentence by reshuffling and adding special words to train the model to predict in the middle of a sentence. These methods can enhance the performance of LLMs without compromising their overall capabilities. Additionally, the importance of providing a sufficient number of demonstrations (possibly around 50) to help the LLM understand patterns and examples effectively was highlighted. Overall, leveraging formatting techniques and providing ample demonstrations are key strategies to prompt LLMs efficiently.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The most efficient technique to prompt a Large Language Model (LLM) involves leveraging formatting techniques and providing ample demonstrations to help the LLM understand patterns and examples effectively. This includes using demonstrations or chaining thoughts, editing within a sentence, and ensuring a sufficient number of demonstrations (around 50) to enhance the performance of LLMs without compromising their overall capabilities.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Based on what you know what is the most efficient techinique to prompt an llm?',\n",
       " 'output': 'The most efficient technique to prompt a Large Language Model (LLM) involves leveraging formatting techniques and providing ample demonstrations to help the LLM understand patterns and examples effectively. This includes using demonstrations or chaining thoughts, editing within a sentence, and ensuring a sufficient number of demonstrations (around 50) to enhance the performance of LLMs without compromising their overall capabilities.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"Based on what you know what is the most efficient techinique to prompt an llm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "804f1816-890b-4745-9771-b30dcea1ef67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use the find_seminar_quote tool to see if there is any information about sending holograms through networks in the seminar transcript.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"sending holograms through networks\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe technology discussed involves sending holograms through networks using techniques like point clouds and adaptive streaming. Point clouds represent objects by considering all possible points in three-dimensional space, with color information for each point. This data is transferred over the network to create a holographic effect, allowing users to view objects from different angles. Adaptive streaming is used to transport holograms through point cloud scenes, adjusting the quality based on available bandwidth. Quality levels are dynamically changed based on user attention and bandwidth availability. Subjective studies involving user interaction with holograms are conducted to assess quality levels. The output includes mean opinion scores from users, indicating their satisfaction with the holographic quality.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will try using the find_seminar_quote tool again to gather more information about sending holograms through networks.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"sending holograms through networks\"\n",
      "}\n",
      "``` \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe technology discussed involves sending holograms through networks using techniques like point clouds and adaptive streaming. Point clouds represent objects by considering all possible points in three-dimensional space, with color information for each point. This data is transferred over the network to create a holographic effect, allowing users to view objects from different angles. Adaptive streaming is used to transport holograms through point cloud scenes, adjusting the quality based on available bandwidth. Quality levels are dynamically changed based on user attention and bandwidth, ensuring optimal viewing experience. Subjective studies involving user feedback are conducted to assess the quality of holograms. The output typically includes mean opinion scores from users, indicating their satisfaction with the holographic quality.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have gathered information about sending holograms through networks using point clouds and adaptive streaming techniques. Now, I will answer the question about how much data will be necessary to send a hologram through networks.\n",
      "\n",
      "Final Answer: The amount of data necessary to send a hologram through networks will depend on factors such as the complexity of the holographic scene, the quality levels desired, available bandwidth, and user attention. The data includes point cloud representations with color information for each point, and adaptive streaming adjusts the quality based on these factors to ensure an optimal viewing experience.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How much data will be necessary to send an hologram through our networks?',\n",
       " 'output': 'The amount of data necessary to send a hologram through networks will depend on factors such as the complexity of the holographic scene, the quality levels desired, available bandwidth, and user attention. The data includes point cloud representations with color information for each point, and adaptive streaming adjusts the quality based on these factors to ensure an optimal viewing experience.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"How much data will be necessary to send an hologram through our networks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "674d071e-3719-4056-a9ab-62d61d956262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use the tool to find a relevant quote about AI's impact on Medical Imaging Diagnostics from the seminar transcript.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"AI impact on Medical Imaging Diagnostics\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI don't have specific information on \"AI impact on Medical Imaging Diagnostics\" from the provided context.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will try to find a relevant quote about AI's impact on Medical Imaging Diagnostics from the seminar transcript.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"AI impact on Medical Imaging Diagnostics\"\n",
      "}\n",
      "``` \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI don't have specific information on \"AI impact on Medical Imaging Diagnostics\" from the provided context.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will try to find a relevant quote about AI's impact on Medical Imaging Diagnostics from the seminar transcript.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"AI impact on Medical Imaging Diagnostics\"\n",
      "}\n",
      "``` \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI don't have specific information on \"AI impact on Medical Imaging Diagnostics\" from the provided context.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will try to find a relevant quote about AI's impact on Medical Imaging Diagnostics from the seminar transcript.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"AI impact on Medical Imaging Diagnostics\"\n",
      "}\n",
      "``` \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI don't have specific information on \"AI impact on Medical Imaging Diagnostics\" from the provided context.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will try to find a relevant quote about AI's impact on Medical Imaging Diagnostics from the seminar transcript.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"find_seminar_quote\",\n",
      "  \"action_input\": \"AI impact on Medical Imaging Diagnostics\"\n",
      "}\n",
      "``` \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI don't have specific information on \"AI impact on Medical Imaging Diagnostics\" from the provided context.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: I don't have specific information on \"AI impact on Medical Imaging Diagnostics\" from the provided context.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How AI is impacting Medical Imaging Diagnostics?',\n",
       " 'output': 'I don\\'t have specific information on \"AI impact on Medical Imaging Diagnostics\" from the provided context.'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"How AI is impacting Medical Imaging Diagnostics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc9f779e-1460-4f1d-b6ec-e4714b115140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" How AI is impacting Medical Imaging Diagnostics?\"\n",
    "\n",
    "querry = f\"\"\"Please gather more information about '''{text}''' and always send me a string\"\"\"\n",
    "\n",
    "qa_stuff.run(querry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d154c9d-80c9-464a-a4ee-0d7081bae507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
