>> Christian Esteve Rothenberg: Okay, so according. To the notification the streaming just started. >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: Let's wait a couple of minutes for students to join. Let's start. Thanks One more minute. We'll start. for some reason over YouTube your picture is not appearing your video Gianni just me. Okay? >> Gianni Antichi: Yeah, well, maybe because you are the host. I don't know. >> Christian Esteve Rothenberg: Yeah. Yeah, we are always This is streaming directly from meat. Let's >> Gianni Antichi: Yeah, and I never tried that. but >> Christian Esteve Rothenberg: yes look convenient, but it's a bit enough not having you and just oh wait. No you are appearing. >> Gianni Antichi: okay, maybe it's h >> Christian Esteve Rothenberg: nice, so but it one window >> Gianni Antichi: H okay >> Christian Esteve Rothenberg: after the other, okay, okay. Good. >> Gianni Antichi: That's nice. >> Christian Esteve Rothenberg: Okay, so we'll manage the interaction. >> Gianni Antichi: Yeah, no worries. >> Christian Esteve Rothenberg: Okay, so I think we can start. >> Gianni Antichi: Okay. >> Christian Esteve Rothenberg: Three minutes past one PM Brazil time. That's the and very glad to be here today the first seminar of this season this special season 100% remote is grad class in computer engineering at the faculty of computer and electrical engineering at the University of Campinas. I'm very happy to have Gianni Yani and Tiki. I'm not going to to read his bio for that you we put the information online and you can go to the personal Pages. Just say that I made Gianni long time ago. Well why we were doing our phds in some random conferences, I don't recall for some some reasons that were certainly worth. and and also very glad that we have been able to collaborate number of research efforts precise around the topics in networking that he will be presenting today. And and he will be presenting from somewhere because Gianni's jumping between UK and Italy at some point. I will try to understand how he manages and I learned that his nickname is the cloud because you never see him, but he's always available and that I can confirm. so Gianni with that introduction in formal as usual, I pass you the ball around 40 minutes. I will be back for 50 minute chat and >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: some interactions with our YouTube followers. We have 18 people plus the other students over 23, so they are probably half. And yeah, the florist is yours. So please sharing >> Gianni Antichi: Yep, let me start sharing right now. Here you go. Let me know if you can see. >> Christian Esteve Rothenberg: Yes, it's arriving. >> Gianni Antichi: Okay. Well, that's awesome. Um, okay then then he's I think here we can start. Okay. Yeah, first of all, thank you very much for the kind invitation. I'm super looking in our own Earth for me to be the first About that. Okay. so let me start with just jumping into the problem here, right? So the problem is basically we we can't longer expect the new general purpose processor to improve performance exponentially and this is what probably you have learned as a student to be like the mood low, right? So we are going to end to the more low. So basically, you know, the line speed of traffic is improve is increasing but the CPU count cope with that. And you know and I found this paper very interesting in this regards that this is a paper presented on and as the eye from Microsoft and basically if you see here like, you know, or you between the fact that the CPU count cope and networking stocks are becoming also increasingly complex and there are like a lot of features a running these tax on cpu-course takeaway processing powers from VMS. And this is a problem because increasing the cost of running cloud services, and I've been latency and variability to metal performance, and this is bad, right? So so that's why there is being a movement towards recently about trying to understand if we can find offload, right? So basically trying to reduce the the impact that we have on the application on the CPU cores and when we talk about offloads, basically, there are like a two type of low flows are really, you know, we can distinct and well, you know you want to do that for saving CPU cycles and the first type of category of a floods are really what will you call harder offloads right? So which means moving some of logic from the application down to Either a switch or or unique in terms of a programmable leak a programmable switch and this allows you to reduce the number of BCA Express transactions their the reduced the amount of allocated as Escape buffer, but in general like it reduced the pressure on the application and the CPU cores. This is one set and there has been like a loud really lots of work here that has been trying to see how you can leverage these new programmability to to make a better use of CPU cores. but this second set is really about instead instead of moving down here underneath or in the switch is really about moving down to the kernel because if you move to the current you you basically you have a less kernel user space transaction transaction. And then this means there you have a reviews amount of data returned by Cisco. And again you you are basically helping in savings if you Cycles and again in this context here, there is being really lots of work and Company really working on trying to see how you can programmatically change the current in a way to have a lighters for example a network stack and having like a better performance in your hand system. now take a step back. This is really really what what's the underlying your trade-offs and system right? So you might have a had a feeling already with that right? So in some sense every time we are talking about software, we have a lot of generality because you know, you're running on a CPU but the performance is definitely not great, Right. So but on the other side of the spectrum is, you know, you can have your features in hardware and the common sense is really hard there is generally less General, but it provides you performance. This is like a kind of a classic. type of trade-off that we have seen in systems and not surprisingly where I really wish to be and I think where also you wish to be is like a really be here, right? So basically find a system where we have high generality but also high performance. Now during this school during this talk. I will try to and do this lecture. We'll try to convince you that this is possible and with possible through to approaches moving the hardware towards better generality moving the software Tower better performance, and I'm gonna start with the first one and and the idea of having moving the harder towards generality means thinking about Primitives. and I'm gonna take as an example to explain what I mean with new Computing Primitives with a database application, right? So and let me provide a little bit of context here. Like I think about you are in a data center Network. This is a classic. Let's say figure of a data center networks. You have switches rocks with a with a servers. So usually when when you when you want to do a Telemetry system in a network, you have a basically switches that they report the data, so they send some data and then you have a collectors. They are specialized or software installed in dedicated server that they grasp the data and they understand what is going on in the network. Now here is the problem a data center can comprise of hundreds of thousands of switches. And at least I'm talking about a big scale Data Center and you know, there was an interesting paper. I see come a couple of years ago where Alibaba was showing that you can have it even few million Telemetry reports per second per switch. So you have lots of switches a lot of telemetry reports that the switch sends to those collector. so here is the problem. We we took like one of the state of the art and network called collector that there was a freely available and this is a paper and we try to understand how many really reports that that collector can inject, Right? So can ingesting can get of course. So if you need to reduce the if you get the more if you assign More chords or to these application you a better throughput and the reason why there is like a lot of a processing is because basically these collector has to First receive the report which is in a form of a pocket and this is ideal Riser then it has to parse the report understand understand. what type of report it is from where which is coming from and then it has to insert that these report. you know, that's a structure right? So in the memory in order that allows then afterwards to be queried it's clear that the more reports to process. The more cores are needed. And this is a problematic because has the the network size grows as you can see here on the x-axis the number of cores that you might need. It might be even more right? so and you can see like of course here you can see like a sort of a simulation where we try to understand specific reporting Be like the number of course that you will need when you increase are the of switches. Now we try to understand why this is happening. So and profile an application that into you before to try to understand we're really the time was a spent here. So we will there are like many tools if you are interested in these things. There are like many tools that really allows you to do that like the most the most common is a Linux perf that tries to get you where you run an application and you can basically understand from running curve where the CPU clock cycle spends in the cause of these application. So by running these basically what we figured out that you know, 13% of time was a spend in the io so, you know receiving the pocket. In a 13 of time was spanned in a parse in the report. So try to understand what's you know, which type of report it is from which which is coming from but really a lot of time and around the 7273 and of course depending on implementation of the stack you might have a slightly different, you know numbers, but basically really they take away is a lot of time really is spent into Inserting data in memory. So this that what we understand from those two figures here is a really that these these networks. These collector is a CPU bonded and we see that because we are the new course and we get a sort of a linear increase but also the most of the CPU Cycles are really spent now we want to get something better out of it what we can do. So first of all the takeaway that if we use a fancy IO system like having new I don't know Network stack that supports better. These application will not really help when we really help because anyway that that the most of the time or at least you might help a little bit but most of the time is really about inserting data into memory and this is a this is what an IO will not help. So then we can think about hey we can do something. What about if we try to have a some harder solution and that's where this type of start them from right so and our question was can switch insert queryable Telemetry data in collector's memory entirely by passing the CPU. So basically the question was if we have a switch that has to report data can this which access directly the memory of The Collector and just dump the data there without having to let's say send this data The Collector has to you know to parse the data and and understand where to put it. So it turns out that there is like a technique you might be aware or not, which is a pretty family pretty famous and used in HPC context and now even in data centers that allows you to basically write daytime memory by passing the CPU. This is what it is called RDMA, right? So RDMA stands for remote to them direct memory access. So basically the idea would be then huh if we can have a switch just do a RDMA call to an ink this is the collector, right? So this is a this is a server and here in the CPU. There is the logic of The Collector. But what if we have a switch that just trigger our demical on a Nick and this goes directly into the memory, then we can save the CPU cycles that we were saying before right? So this is great. Unfortunately, this is a little bit problematic and adjusts, you know do it because RDMA is limited to simple operations. So basically you can do just to read. Or write in a memory and some other type of operation. but also RDMA requires explicit addressing so as which has to know exactly in which location of memory they have to write and this makes sense, right? So I'm gonna try to show you with the thought example why these these limitation here they make it hard to do. These has we envisioned at the beginning. So, let's assume we have a you know a number of which three switches that they want to write in a data structure in a collector and this is like a sort of a ring buffer or an array, right? So oh. so and this is the case where for example you want to switch that needs to report some event data into a list in a collector. So, how do you do that while I should Rene coherence if you think about it that this is a classic problem of systems. so the first option you can think about is well, you can have the switch that they talked themself they coordinates and then they decide hey, I am gonna write here or hey or this one. Hey, I'm gonna write it here, but the problem is this impose too much over it and if you think about it, you don't want to wait right so Telemetry you want to have a switches that just send the data as much as they have because they will have a continuous amount of data. Well, we can think about the different option we can think about having a sort of a lock so you can basically the switch reading these acquiring a lock. Is there something we do like in multicore programming in a sense and then once the core the lock is acquired, you know this which can write this is all good. But again, this is delaying the process of what you really want to do is sending the data because which acquire a lock then after acquired the locker can write the message then I can you know can be zoned a lock and I think this is good. But in the meantime, there's Richard will have something else to report this is bad, right? So and also what about explicit addressing. how does which will know where to write you will have a problem to acquire the lock read the last address available? And then right and this is like kind of the laser process. This is bad. So then let's think about let's take a step back at this point, right? So what do we really wish to have so we wish to have something that is a lightweight protocol that that has a low hovering the switches because those witches needs to be used really for doing like a processing of pockets. not to yeah, they need to send Telemetry, but you know, this is not the main point. but we don't want to rely on The Collector CPU and this is what where we started right really, so we want to have a sort of er, DNA like protocol. But we want to have also coordination free updates. So what we want is we don't want the switches that they have to talk themselves to decide where to write. So we want just as which is to just push Telemetry report as soon as they have. And finally, we want to preserve the data organization and what I mean by that is that we want to directly organize us. So we want to push this data not in a random way, but we want to push this data in a way that then if I really did the data, I understand what's going on. In some sense. What we want is really memory layout aware RDM aware verbs so some sort of the RDMA verbs not just right and read the whole thing before but something that is aware on the type of data structure. that is that is in the collector. So that's what we come from and we decided to build after all this consideration and this is a work led by my students. I'm very proud of him and we present the last year a cecom and basically the idea is we do have And we do have a collector here. So a collector will be connected at some point to a switch there is gonna be a switch that is directly connected to our collector. and we are gonna have others which is that they will report the data to The Collector. Our idea is to build a protocol. We call it the direct Telemetry access where those witches they report to using our protocol then these these these data arrived to the last hop which that converts these into simple RDMA code. I'm gonna try to show you why this work and why this is a in our in our opinion and good solution, right? So here we have a bright only data structure. which makes sense. Right? So you just right the data that you want to report you have here the switches that report data using our custom protocol. Yeah what I call it before memory layout are dma verbs and then we do. Have these last Opus which that is in charge on translating standard are chemical and doing the explicit addressing that we were saying before. so intense is really about you know as we just send a report and the last stops, which says oh if you write it this way the report then this means that I need to write in memory into the collector these value at these address. Now, I'm gonna of course. We we have a limited amount of time and I will be happy to talk more but I'm gonna give you just an Insight an example on how this works just to give you a feeling what I mean by that with the more memory layout verbs and so forth, right? so and idea is let's have on the collector a sort of a key Value Store like a sort of a matrix where we store keys in value, right? So for all for each key there is associated the value the idea that we have is really to create a stateless mapping between memory and Telemetry key using hand hash function to Collision. Let me show you what I mean by that. Let's assume that there's a switch wants to report. you know a sort of a set of Statistics like a fluid and this is the number of pocket that I've seen for this specific flow ID. So the key in some sense is the flow ID in a sense. That is the classic five double and the value is accounted the number of pockets so that I have seen for this specific fluid. So this is the memory on The Collector which is organized like as a sort of metrics. So for the for the first you you have a flow ID one you have brought you you compute the three Ash function They will redirected to three different memory location on The Collector and you will write the count. So the value here here and here so you will trigger basically three RDMA calls a three different random location that they follow the ash Function One change. Another report will come over and we'll do the same will I have a different flow ID you and you will store here. The number of packets are the address that is Ash function of flow id2 and Ash function two or fluid too large function three of Liz read three and of course, you will write that other tree random location, of course as you can see from these you might end up to have these that was orange before now becomes green because you might have over written some data. The problem is like how to read this memory. Let's assume you have this data here. How how do we read this? So first of all, I need to know the flow idea. I want to query and the ashes that are used and this is good. This is okay. Right? So it's a reasonable type of things to request right? So because you feel like okay as an operator, I'm I want to know how many packets I have seen for this fluid so I know the flow ID, this is a system that I have in place in my network. So I know the hash is that they're being used so I can do that. the problem is really how can I know if these memory for example was not a very written right? So how do I know if there are like others with other flows that they collide with these memory in this case in this example that was over with written if you remember here there was orange and then after another flows arrived, you know, this becomes green. So what we did is that basically we add here the information not only the key see, right, so we are CRC a basically what you can do is to read here the information check if they CRC is correct. If it is not correct. This means that there was a collision so you can check another address current with the hash function if it is not then that's the value that you want. now the example of a primitive that we provide we call it the key, right and this gives you like also it helps you to implement the value stores, but we implemented different type of primitive. I'm not going really to to talk over those. But the point is the idea was we want want to make a set of Primitives that they help no matter the type of data that you want to report or the type of a monitoring system that you have on the switch You can make it compatible with our solution in a sensitives becomes a sort of a way for a serializing the information from a switch to the memory of The Collector. the creators out to be pretty We compare this wisdom that they basically use a CPU instead of just RDMA as we do and actually depending on the type of Primitives that you use depending on the type of data that you want to report you might have 4X or 14 One X Improvement. And of course this depends on many factor and again, I'm gonna I'm not gonna spend a lot of time on this because I want to just get the gift of a system right so But that's a point. It's also interesting that DTA. Our protocol is a lightweight we compare the results utilization of implementing on the switch. just a simple UDP, which is a certainly the most lightweight protocol that I can think of or implementing directing on this which are dma which is a pretty complex of DTA. It turns out to be instead the pretty light weight as a European, right so This is all good. Right? So if you think about it, and the question that you can think about is that do we really need to give up with software, right? So it's a can we do everything in Hardware. So and this tree gives me really to go on the other part of a talk is my argument is we don't need to we don't have to give up with software we can just build the better better compilers that will allow the movement of software. And I'm gonna try to make a case of for what I mean by that by taking an example of a pocket processing programs here. So if you think about how you run a generally software you have basically a programming framework or language then you compile it then you build you give it to a compiler and Optimizer and these builds likely let's say the set of table the data part of your program, right? So the idea that we had is the What if if I run time we provide some sort of information back to the compiler on the code Behavior a workload information. Then we produce around Time new shiny code. that is actually optimized for that specific code behavior and workload information. now I'm gonna give you like I'm gonna try to make a case on why this is useful, right? So, so first of all if you think about it, the network configuration is really unknown and compile time, right? So when you compile for example a software program you write your program that can support any type of things like Viola Villa and VXL and mpls, but that maybe after you run your switch your software switch you might decide to insert some rules that they have a villan or not because it is depends on the that's what configuration that is unknown you compile the software. we tried like what we did is that we took a load balancer from a meta which is a production layer grade the load balancer that that has open source code and we and what we did is okay. Let's assume we want to run a catron as a just an HTTP load balancer. So if if we analyze as an HTTP load balancer, basically we need to so it's about inserting rules that they are more layer for oriented, right? So so what if we if I remove any processing in the code that is already that is related to TCP because if I know that I'm running as HTTP load balancer it will receive only TCP drop Pockets. only TCP packets and not UD. Is actually turns out if just removing some branches that we knew they couldn't be used we could improve a 10% to the performance. so the system but simply because you help the CPU with the branch prediction and so forth. So the takeaway is really specializing networking code for slowly changing input like a network configuration improve the performance, but that's not really the end of it. Right? So let's really go beyond the little bit from code specialization for stable configuration. Let's see if we cannot musician at the packet level what I mean by that is think about UI processing program a packet enter you control you will check the pocket with the table and then you do an action if you think about it you are basically bounded the performance of your program by the fact that every packet that arise you need to check the table and then you need to do a memory access, right? So every packet one memory Acts now what if if for a specific flows that they are like a heavy flows and every time these I know that I'm gonna go do the same memory access instead of instead of doing these memory access. I will embed in into the code directly and say hey if this is a top flow and I know this is a top flow. I know already what is gonna be the answer. This is the code. This is what you need to do. It's a sort of a caching inside the code. So if you do that. Is actually this is of course in the presence of a skewed traffic because you need to have a heavy eaters it you know entries that they are hit a lot of time you get even a plus 23% of performance and if you consider plus the 12 before then it starts to be like something interesting. So the takeaway really here is the for maximum performance called must be specialized with respect the inbound traffic patterns. so the question when I when we realize that I might my first comment was This is not really new right. So if you have been here for longer you might have heard about what is called what they are cool profile guy that optimization a pogo type of tool that they use a results of profiling to optimize code, right? So then the question is, okay, so you are not really inventing anything. Can you just use those tool? So what we did is that we took a tool from Google how to say FDA yo and we took a code for a compiler from Matt a bolt that they basically they do this sort of things. Basically the idea what I do is they monitoring the the application are you works and then basically reoptimize which is exactly what I was saying. we apply that in the same similar circumstances that we tried before and actually we saw that the Improvement was not that great. So I was expecting to see plus 40% as I was seeing before, but I can't see that. so then that is really why because the problem is standard the Pogo tools are built for application but packet processing code needs a domain specific knowledge to reoptimize bucket processing code. You need to understand. What's the content in the into the the tables in what? what are the how many entries are heating? Which table while these are standard out of jio Bolt. They look more Akash misses. They look us more higher level type of properties that they know really helpful in the contest of optimizing code. So what we did is that hey, we should build a tool that does that. And basically the idea of the tool is you have some source code you produce llvm which is the intermediate representation. Then you go you analyze it you instrument to understand what's going on and then you you optimize it you get your optimized code that goes back in input the llvm and so forth and based on the results of the analysis. now this is gonna take too long to to explain everything to you and a time is running out. So I'm gonna just give you like an Insight on the most I think a juicy any interesting part, but if you are interested in the code, you can just just look it into the GitHub. Of course. so I'm gonna look into just the optimization, right so and we have this a lot of different optimization, but but I think I want to and some of them they depends on the input traffic some others, but I want to focus on just you right so I just want to give you a in hint of how this system work specifically I want to focus first on that coding limitation that removes branches that are not being used. So let's take us an example these code. This is a code that really taking from katron the load balancer from that right? So this is basically you think you can see it does a layer tree bucket headers the parts that you're for parsing then it does something right. So, let's assume that there are no quick services in configure in these systems. So in the in the table, so if there are no quick service is in our compiler realize our work our analysis the first we look at the table, we realize that there is no Is a quick services and then as you can see here, you will write you will override The Code by imposing 0 if it imposes zero here, then the classic compilers we realized that this is that this is a dead sort of a branch and then we kill the branch. which is great because then you basically you make the code lighter you make it easier for the CPU to understand what's going on for the branch prediction. And I'm going to show you another example related to one of the things that I was telling before are that we call it just in time compilation, which is basically inlining frequently hit the table entrance into the code. And this is I'm starting from a similar a code snippet where basically we do we we do a look up in a connection table, right? so here we do a look up in a table. and instrumentation basically instrumentation realize that there are like some flows that there are heavily heating to this table. So what is gonna do is basically instead of instead of doing the you know, just these that look up for every entry will do if the packet is is a topple then just go to this backhand. Otherwise go to this back and otherwise, yes, do they look up on the table? And this is like this is a right in the compiler we do and we run again the code. Does this work? Of course it works? Otherwise, probably you would not be here presenting it but but actually was pretty nicely right. So of course it is always depend on the amount of locality because you know, if you have a lot of editors you can play a lot with that and then you will get an improvement of 25 to 97% of the throughput if you have a less locality, then you will not have really a lot of improvement and those this is a trade off of the system we try this with different type of solution as which is a router the Catherine IP tables and so forth, but the takeaway is that this is like can improve really performance of system and why is that is basically because if you see here we try to understand what is happening that the micro architecture level. It is really about it to reduce. This is the percentage of the crease it reduce. basically the number of distraction it reduce the number of branches and as a result you reduce the number of Lord misses a layer last layer cache and this is why you get These improved so the lesson that I really learn and I'm going towards the end here. Is that the scanning application scaling application performance is possible right so we can do that. There are like we can have a better Hardware more generic Primitives for Hardware or better compilers. and now we have really we are in the right time. We have a fully programmable stack and a Common Language. We have a P4 for example that allows you to push something in the kernel something in the neck, but also in the switch. so the problem is really and this is more about more looking forward. here is a picking what functionality to offload desired and you know, because I again, let's think about we started from an application. We need to decide the front this application what we should put in the current in the nick or in the switch. And the problem is the programmer if you want to do that. The programming is to understand both application and offloading capabilities. So what the kernel and eakers, which can do that he needs to reason in some sense before implementing these about the improvements from a floating. implemented those separate components and you know if you think about ET can also means opportunities because you can reason and you can only test so many options right so And then you end up with the question which is the title of the talk really to offload or not to offload, right? So And that's why we need the smart compilers and compilers that basically given an application and doing some analysis a selection and generation can really understand where to push the right functionality and where either in the curler in the making the switch. We start going into this direction. We had a paper a lot less last year. We were mostly considering kernel application in kernel, but I think we can expose this to nickens which is as well. But then we need also layer seven. Let's say vertical Co design because in the moment that you start the pushing application in down into the Knick then you need to think about you handle transport protocols. how you handle this other things they usually the operating system does for you greatly. Um, since I promise Christian took me to make it like a really sharp in a 40, I'm gonna conclude by saying that to achieve that we need to have like a better integration between communities like system networking programming languages computer architecture, and I really want to have a special. Thanks for all the people. So I'm here presenting it because all those people contributed to this works. Otherwise, I wouldn't never be available to do that. I am grateful to my cats to a lot of cables and I'm super happy to take up question if you have thank you very much. >> Christian Esteve Rothenberg: Yeah, thank you Gianni. Yeah, the great overview congrats for the achievements. and and I have of course some questions that we don't know the answers. >> Gianni Antichi: Huh, of course. Otherwise, it would have been talking about those in the yes. >> Christian Esteve Rothenberg: writing another paper to get but I wanted to share with you on there and the audience and A few formations. Okay, so I will share. Here I was explaining Gianni how this seminar class work. Okay, so Lens, they did a leader to review based on the abstract that you provided for your seminar and here they identified. The papers and they need to reason why and of course I see your name here in in a couple of them. So the the reason of selecting some sort of I'm very satisfied that in a short exercise they were able to to identify. what are relevant pieces studies that they would bring to an island if they wanted to to learn more and we have over 30 30 32 responses. So some of you are late and And each of them they also elaborated at least one question. So I have here over 40 questions because some of them they're elaborated like three or four and some of them are really really good Unfortunately. We don't only have a little bit more than 15 minutes, but I will share this with you because there are some good ones. I identified that many of them going to the direction of the Moors law. >> Gianni Antichi: right >> Christian Esteve Rothenberg: So they because you you hated your dog. So the more law and the wall of the memory. is he so specialized Hardware comes to the rescue in this morning and I know it's not totally our field, but do you know something of Novel advances breakthroughs to bring this this more low in semiconductor or I decides or something. that is hitting. or >> Gianni Antichi: so I I know gonna straight away answer to your question, but I think I will answer to your question and actually and and I think one one things that excites me that I saw recently in an effort to bring more together. Let's say let's say accelerators and cores are the idea of the middle building better interconnect. And what what I mean by that thing building a connect I think is the realization that you really need the harder because otherwise you can't cope with a with a with the speeds of of the processing and since you need to have a harder then you know, the harder is there you need to have it how we can think the way we connect the CPU to the hardware, right? So and and that's like a couple of ways of that I can think of lately there has been like for example one way was a nano Pew type of approach. That was a paper. I was the I from a Stanford floor folks not only stand for folks, but most of them that the leading student was for Stanford, um showing how to redesign the meek interface for like two for sending data from the Nick directly to the to the CPU but also recently advancements like a cxl which I'm not sure you're aware of but basically Excel is a is a star stands for compute express link is a sort of a PCA Express, but the vision of cxl is really to have a sort of providing a coherency between the memory between ICS Alexa attached device in the knee and in the CPU and this is a way to I think in my mind the movement towards these like a sort of realization we need accelerator. The accelerator are here to stay and you know if they are there, but we also need the CPU. How can we re-archy text the way they do the the two of them? They collaborate in a better way. >> Christian Esteve Rothenberg: Yes, nice. >> Gianni Antichi: So I don't really answer your question, but I >> Christian Esteve Rothenberg: Yeah that the question is really hard. What is the future of more slow, right many fold directions? Yeah, I've interconnects. I've read something around Optical interconnects. And so the energy consumption aspect. that was another on my on my Questions. We are trying in our research group to try to account for the full energy consumption with uploading without uploading because we know at the end of the day and then of the month the >> Gianni Antichi: right >> Christian Esteve Rothenberg: bill comes and for many years. This bill can can pay off. Let me pluck, of course some on the world will be doing together. I'm presenting so in June in netsoft, we will be having this tutorial the smart Nicks the next leap in networking and me and Marcelo will be there. But we need to as you did recognize all the contributors. So we have slides being presented that we're contributed by giannii and also earlier in in May in our main Symposium of the Brasilia and of networking and distributed systems as BRC by the way, we need to invite you. We'll have also a short course that delivers not just the tutorial but also a forty forty five page and >> Gianni Antichi: one >> Christian Esteve Rothenberg: material and we are working on on these Francisco Marcelo and also designing the types of experimental Hands-On demos that we want. to also learn about so that's a plaque if you want to learn more about smartnix join us in sbrc or in netsov. But let's go back to the to the questions. Do you think the energy? And account with Martinique as a total system is is well understood so they performance extra performance and together with the overloading of potentially CPU cycles and reconstruction of the CPU down to the more efficient Hardware is well understood and based off or maybe >> Gianni Antichi: I I >> Christian Esteve Rothenberg: depends sometimes yes, sometimes not you have inside on that. >> Gianni Antichi: yeah, I I think he highly depends definitely like introducing new devices introduce more consumption in some sense, you know, but if you can make up with a with a performance that they can be like a sort of a trade-offs there, right? So but a trade-offs will depends a lot I think on the specific harder that you have and what I mean by that is you know, there are like You know gpus fpgas Asic. So asynch that they have as some form of programmability. you have a programmables, which is they all have a way different trade-offs, right? So and in terms of how you know the energy consumption in terms of how they behaving in terms of also how you can manage them from an Energy Efficiency point of view, right? So there has been like recently a lot of interesting work in the area of a trying for example to rethink a networking in the context of making let's say decision or routing decision to be more carbon aware There was an interesting paper Comics last year about that and you know, people are moving towards these >> Christian Esteve Rothenberg: Mm-hmm. Good good. Yeah here in the faculty that we do a lot of work on Energy Systems we should come up also with ways of measuring an understanding this this flows of Energies. and and so in the chat, we have 29 people and we have a couple of questions. So in addition to that more than 30 we have a couple very low level. >> Gianni Antichi: All right. Yeah. >> Christian Esteve Rothenberg: before I pick one and things for for the interactions, I want to to bring one of my own. So the the people language was really promising in potentially becoming a language for the networking Community, right? But then we saw that for different targets. some the compiler had to be redesigned the You used to see in in the sovere compilation the programming mean that they offloading choices by analyzing the software that in it will be in some language to you. See some That we will have another over the top language from their lower languages can be compiled like ebpa dpdk or even before I see before and and even lower or maybe before still assumptions is to become the de facto highest level language or should we stop with that? hope and get ready that there will be silos and ecosystems or I know that's another this one, but I had to >> Gianni Antichi: now that's that's an interesting question. Okay. so let's start from Let me remind you that I might my background is fpga design, right so for me before is even too high level, right so for being used to very long and so forth, but you know Jokes Aside like before was built like as a sort of an idea of a really really tighty coupled with the match action a type of a framework of r&t r&t, and he's in some sense a low level type of programming language. Raisson it is a software like but it's a pretty low level right? So and I think in an effort to move higher higher level probably like you know that this is I something that will help right. So one of the things why I am a little bit puzzled, and I don't know a good answer is for example. I understand that P4 is a is a is a he makes a total sense for programming accelerators. But when we when we think about endos networking you have let's say as you also mentioned a BPF, which is basically sort of a de facto way to program the Linux kernel and this is I'm still a little bit Unsure how those things that can be like a connected and nicely together in a way that that can be form like a career in the way to see from the application to their flows, right? So there has been a recent effort being pushing a P4 to delinos Kernel. There is a compiler called p4tc that allows you to express a P4 programs and attach them at the TC level of the of the Linus kernel. Um, and this is an interesting way to see like, how what can you do with before we did completely different? It with respect what he was born before that. It was a hardware, right? So now instead it's something like a more CPU Target, right? So and I'm curious to see how those things that can be connected together at some point. I am not sure to be honest, but I do see these as a valuable really research direction to think about the programming model. >> Christian Esteve Rothenberg: Yeah. Yeah, and I like your point this bringing together different communities the programming languages hardware and we see a really good pieces of work when you get the folks right together, and it's not easy because the language of which focus is different. We see with the machine learning when we try to bring a machine learning expert to our field of networking and it's not so easy, right? And he in the chat. We have questions by sunith who you also know. He asked about the the micro second scale the report stats to the collector in microsecum Scales. Could you give some examples or use cases where we are really facing this this problem is his question is micros the micro second scale for stats report. Is this the >> Gianni Antichi: um, so so let me see if I cannot plug back the presentation. I think it should be able to see it. Right. so I think the argument and going back here. Yeah, so the argument is really. million of telemetry reports per second the prez which is really about the type of queries, right? So is that you might have a lot of aquaries right? So you might have queries in relation to flow statistics queries in relation to path tracing flow queries in relation to Kio capacious and and so forth, right so and these all of them they might create a really lots of telemetry these at least this was what was claiming in the specific paper from Alibaba, right? So all I if we think about a single query and you ask me, is there a case of forever single query at microsecond scale? I think that there was an interesting work from for example Facebook and I am see 2017. We're basically we're advocating for having a microsecond counters in their And the reason was the following. they wear like using like a classic SNMP and so forth. They were seeing like a sort of a usage of the network that he was very low, but they were seeing like a packet drops and they couldn't understand why then they went they increased the resolution. They realize they were spikes in the traffic and with those spikes of the traffic. They basically wear microburst that they were creating for a short term like a queue like as high queue utilization and then packet drops. So in that specific case, I think that these can be an interesting more for a troubleshooting for example my micro or microburst the type of evidence. >> Christian Esteve Rothenberg: We have another from Rodrigo's. This is really low level and he is asking and the air dma with DTA approach similar to count means sketch what but what are the differences by adding this CRC? >> Gianni Antichi: um >> Christian Esteve Rothenberg: I honestly I'm not sure I fully understood. So the question hopefully you did. >> Gianni Antichi: So no the so okay. I think that this is the this is something that we didn't consider in a sense. I I agree with you that it might sound like in a sense account mean I think the difference is the way I do see that is if You would ever use account mean that you have a probabilistic approach, right? So you have a probabilistic approach in a sense that you take then the mean and then you get sort of an estimation right? So we wanted to have the sort of a similar, you know as much as possible the exact value and if we couldn't get the exact value then you know returning a failure for for the specific for this specific query and well and See give us is the innocence is a sort of a confidence if that entry was overwritten or not. If it is not of a written then is great. That's the value that we really looking forward to if it is was of a written it this means we need to go to calculate another hash and check on another member location. We're using one of the other hashes and of course if you all the three hashes in the case of the example the three ashes they return a fail then is a fail. But again, is that something that we show in the paper is a trade-off between how big you want to have the memory and then the harsh space and and the type of the amount of reports that you have. >> Christian Esteve Rothenberg: Okay, and now I got the question. So thank you >> Gianni Antichi: I mean, it was my interpretation of the question, right? >> Christian Esteve Rothenberg: I think it was I will take out with Rodrigo. He's around in people so on doing a CRC and hashes and doing it some interesting stuff within people. So last one. >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: and I like that one this comes from one of the class student reports. and how fast and often this issues whether to upload or not should we should be made? Okay this I understood it. You you what you propose this analyzing the application the code before it runs, but maybe around time something changes. >> Gianni Antichi: No. >> Christian Esteve Rothenberg: That's that's a reputation often and then the second one which I also like because we are in there machine learning here and AI good. And we apply machine learning to this problem or is it an Overkill and probably something more static and with guarantees? initially will be the approach and >> Gianni Antichi: yeah, so so talking about the the the decision that a compilation decision Okay, so, oh, I'm gonna answer this in two parts, right? So if we take these and if we take it just a pure software approach. Oh and we consider that the tool that was presenting. of course here you are bounded by the sort of your these this sort of a round trip time, right? So the time to go here and back here and and these this is a as a as a sort of effect on how good the analysis is they need to instrumentation and the optimization that you run and how much it takes to load the specific code. For example, a vpf code into the kernel. on now if your traffic patterns input traffic pattern do not change a lot. Then this is a good approach. Of course, if you have a traffic patterns that they change a lot then you might end up in a kind of a following like a sort of completely changing in circle without the gifting getting like a too much better benefits in performance. And again, this is a trade off of the system, right? So if we stand to talk about more generic on the should I your float something or not, which is what I was saying here. So the way that I was envisioning here is mostly something that is a should be in some sense a compile time before I running right? So I have my application I get the feeling on all the application I studying application and then automatically I decide what to offload currently because which are so not something I run time. He couldn't be like exciting doing our own time. To be honest with you. I Know if what would be like a really the mechanism why you want to do that in which case is can be convenient. I might Envision that maybe if you have a multiple application running then you might want to do these for some application at some fun time at some point. You want to have a kernel of Nick support for somebody then at some point the traffic pattern change and you might realize that you want to have kernel of mixed support for another application and this is where I can see the dinamia dynamicity of things. Um, I don't have I would be happy to talk more about that. I don't have a good understanding on how to do that around time because these requires reconfiguration of harder, which is at least for fpga is a kind of well done the student. There is a techniques that they are called partially configuration. For example for other type of Ariel. They are less understood so that that's something that you know, it could be like an interesting. Uh, let's say way to see the problem as a Next Step. >> Christian Esteve Rothenberg: and the Machine learning a part is if you have a data sets Pro having done this multiple times and I observed and then maybe you identified >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: patterns in the code that and then over time with data. The machine learning could be handed to to help you this decision. Like oh every time I saw this and this type of code flow structure or or whatever this work or that didn't >> Gianni Antichi: this is a this is interesting the we had some ideas in a relation to that. We didn't start because we couldn't find the really a person that was enthusiastic doing that. But that's something that we wanted to do at some point. >> Christian Esteve Rothenberg: or Gianni yes us check GPT putting the the code in the actors then. >> Gianni Antichi: There was they solve everything for you. >> Christian Esteve Rothenberg: yeah, everything that's how that's a lesson that we will take that it doesn't work like this. and I'm joking because I am playing with the judgity both in undergrad and ingrat even in ensuring the answers so that they are >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: critically and they analyze and they will take them with very carefully. So Gianni it was great. So we are just in time other students just starting classes 2PM Brasil time. Thank you. >> Gianni Antichi: So much. Thank you very much for having. >> Christian Esteve Rothenberg: Enjoy the rest of if they forward to meeting you. >> Gianni Antichi: Yeah. Thank you. Bye. >> Christian Esteve Rothenberg: Thank you. See you. 