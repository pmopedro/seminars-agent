>> Christian Esteve Rothenberg: Good afternoon. Thursday seminar Thursday together and Today we have another alumni Gabriela surita. It was not so long. Ago that you were around in the faculty. and now you are in Google working on the Hot Topic probably today Hot Topic generative Ai and talking to machines hopefully Are real. I know you are real. And but with these times you never know so as usual I don't spend time in presenting our speakers. I let you to introduce yourself. We can Google you as as well and find more about you and thanks again for for being here and talk to you in about 40 minutes with the Q&A and the traditional chat. >> Gabriela Surita: No worries. Thanks for the introduction and the invite it's a pleasure to be here and also like always a pleasure to to talk. at unicamp, whatever and whenever I can for a curious like a fun fact Christian was my I think the professor of my last. a last class that I attended at fact, so yeah, and it was oh, I don't know. I'm very glad that yeah. It was a very good A network systems and network Labs. Yeah, so the title of this talk that I want to talk today is talking to machines practical introduction to generative AI so as as you might know, this is a very hot topic and I'll try to bring like a balanced and motivationary like overview of the topic. But yeah try to save some time for questions in the end as well. It's a new content. So you be my guinea pigs, and I hope that you enjoy it very yeah, it might not be super well-rounded a little bit about me. I work at Google deepmind. I've been here for slightly over two years now before that I spend five years. Trying to blogging some problem. and their internship at Mozilla and as you already know I worked I studied at unicomp a couple years ago. I started 2012 then I changed to Computer Engineering 10 2013. so, yeah, and For the last two years. I've been working Text generative Ai and mostly co-generation applications. What once you talk about today is I think first. I'm more introductionary presentation of generative AI. so I think how does one train so large sequence Model A Very like one million meters overview, but how does a modern chatbot works and this first part will be more Technical and more basic. So this is content that you can probably find elsewhere online, but I'll try to squeeze the most content I can in 20 minutes. I think the second part is more abstract. And it's about interpretations philosophy and discourse of AI. I think this is one of the particular topics. I'd like to study and research not only at my job, but at my free time it's more abstract, but hopefully very useful as well. It's also like a compilation that you won't find very easily in the internet. So feel free to ask questions and email me afterwards if you want to learn more. and finally, his applications so it's very Hands-On tips and traps to using these models I think like it's also basic very applied but also some of the tricks you only learn when you use these models on a daily basis. some assumptions we'll College of classification regression in your networks. This is not mandatory, but it will help it will be a data abstract if you don't know this, but I hope you can get something out of it. I highly recommend some pass interactions with GPT and Gemini if you haven't played with this language models, you should I think it's one of the hot topics of our age. So at least some experience playing with then is useful and most topics apply to multimodo agents, but of course my my examples are Skewed towards code and text these are my areas of expertise. So, okay. Let's Ivy. What's generating AI? the name generative AI is already telling every time someone talks about you when they say generative Aid probably differentiating this type of AI from another type of AI so I think Station is very very useful. Why is it generative? It's not classification. It is not regression and it's not like multitask like multi multiple choice action selection. So what it is, then we're usually talking about machines or systems capable of producing human consumable. I think there's gorgeous important digital content. So usually text videos images or audio. There are more but I think these are usually the main ones. so it's generative not predictive I think sometimes it's used for it is used to differentiate against. artificial general intelligence as well. So this is not necessarily General, but it's producing content. And sometimes use it to differentiate against narrow AI this term is disputed. basically the idea is the outputs are unbounded in they usually involve digital content. I think the most obvious examples are chatbots assistants. That includes check GPT Gemini Claude and some open source alternatives. and it also includes text to image text to audio audio to audio all these types of systems, but I think like the most obvious examples are these chat Bots? This chatbot assistance and the text to image systems. And they are often associated with creative tasks. So it's predictive AI is usually class associated with like predictive statistical tasks and generative AI is usually associated with open-ended creative tasks. I don't love this term. I'm going to talk about more later, but it's it's something that people talk about. So how does one how does a modern AI assistant? work the very usually the most interesting in the hottest one to talk about but let's try to understand I think a little bit what happens when you go into gemini or when you go into chat GPT and you press enter like you type something in presenter. I think one of the key probably the key component of this systems is sequence modeling. and here's there's a note like not all generative AI Model but most generative AI includes a sequence modeling component even textual image systems that sometimes have another diffusion or variational Auto encoder component. It's usually the key one of the key ideas is sequence modeling And I think understanding sequence Extremely useful to understand this systems. what is a sequence model? sequence model is a predictive system that is capable of produce predicting the next element of a sequence. In text I think. Texas probably the easiest one to understand this phenomena You can often think about this as predicting the next word or predicting the next character. So in a sequence when you start with the you can try to predict what comes next predicting what comes next after the is really hard. But if I start adding words and try to predict what comes after the quick brown Is probably Fox the quick brown fox jumps over the lazy dog. This is a very common. sentence in English, so you I think the main idea behind it is like if you have something that is represented by a sequence. And you take a subsequence of that sequence. Can you predict what comes next? Think this is the key idea of sequence model. another interesting example is like if you go to mathematical sequences if I give you you can maybe try to guess maybe you're gonna count with numbers repeat it so it could be one one to two three three four four, but it can the Fibonacci sequence. And in this case, I think if I give you zero woman, two three five. It's probably the Fibonacci sequence if I gave you 8 and 13, it's very very likely the the Fibonacci sequence also if I gave you some lyrics like if I start with what we're halfway there. You probably leave sending to Living a Prayer like it's of course, there are some examples that we can probably not do the same thing. But maybe you're halfway there something else better. The wall is quite telling so I think this is the key idea behind the sequence model. Can I predict what comes next in a sequence given just what I've seen so far. So this is the key idea no mathematics so far, but you can ask like, how do you do this? And but those familiar with the answer like for those familiar with the field. we usually do it with a probabilistic model. I tend to think that's specially for language. This is not this is not an obvious leap like and I think it's probably one of the most powerful ideas of information Theory and information transmitting in general like I think Shannon made this leap of faith that if you can. rap sentences out of their meaning and just model them as probabilities you can actually do something some. like quite powerful things like and basically the the key idea behind this is that you can express. the probability of the next element in a sequence for example the probability of fox given the quick Brown as You can express this as probability distributions over the next word and you can probably maximize probability of fox given to a quick brown and you can maximize probability of 13 given the beginning of the supernet. She's sequence. You can also measure like probabilities probably the probabilities of bananas given the experiment suggests is very unlikely. but I think this idea of expressing. sequence the next element of a sequence as a probability over all possible elements vocabulary and given the conditionals of the previous elements of the sequence a very very powerful idea dear and I think like is a direct result from Shannon's theory of communication. So this is quite important. We're gonna go back to this sometime soon. And just a reminder like outputs are probability distributions over the symbols. model things in the world like if you have a bunch of sequences that you want to model. You probably want these distributions to reflect. the natural distributions of these words in in the world. So if you start with the word and you try to predict what comes next this is usually a very wide distributions, there are plenty of words that can come out of after the but if you start with a very targeted sequence like the World War Two ended in the year. And you try to predict the word that comes next. This is a very narrow distribution Like I think the it's very unlikely someone is gonna say 2024 there. Like it's probably 90. and 1945 so when you're trying to optimize for this distributions your objective needs to account for that like you have sequences that are very easy to predict and you have sequences that have a very broad distributions one way to represent this. And to formulate this problem I won't dive into detail here. But it's a direct result of information Theory as well. You can optimize for the cross entropy between you're measuring. and the distribution of the underlying model This is the detail. but if you take a machine learning course like you will see this result being brought quite frequently. Okay, so we can predict the words given what comes before? you can maybe try to go and build a big table of all the words in the internet and say like I want to predict what comes next. like if you try to do that with a big table, like all the words that come before in the word that comes next. It's possible. This is not impossible. But the number of entries in your table depends on the number of permutations of symbols not all symbols are occur in nature like language is has structure. but if you try to build that big table This is Not Practical for sequences over 10 cent symbols, basically because this grows with a factorial of of the number of symbols. so the big table is works up to three four symbols, but this is a tech that is used. and it's available on phones for example for text completion. This is changing. Some phones are using deep learning Technologies now, but up to sometime in the past every time you got suggestions when typing on on your phone. This was usually the tech that you were using like you had this big table of three or four symbols that come before the one that you're typing right now and you you have a table of occurrences of how how the next symbol what what's the most next thing will So we skipped to neural networks. And again, I won't be able to teach neural networks in a slide, but neural networks are usually an approximation of this table using dense vectors I think the the word neural network is has very little resemblance with how the brain works, but you They have been historically being called neural networks, and but it's basically a very smart projection of these sequences of words in in And if you can approximate singles to a vector combine symbols of vectors. To a sequence Vector efficiently and create a predictor. from the vector of singles to the next symbolt and you can model much longer sequences. And this idea has been around for quite some time. I think like at least 45 years. but there has been like huge qualitative leaps in the last few years. So this is not a new idea. You can take a sequence map to a dance Factor. and try to predict the next element in the sequence given this dense vector but has recently been transformed. So this is a plot of like time versus number of parameters going back to this. Like usually this projection is represented by a matrix multiplication and the number of elements in this Matrix multiplications is usually called like the weights or the parameters of the neural network. And if you take like this number of elements used to do the projection and time. It's almost an exponential like a few 2022. There are questions whether this exponential is holding or not a lot. Lots of like tech companies are not. disclosing the sizes of their models anymore but it's being almost like a clear exponential in the last few years. So this projections are becoming more powerful and Powerful the number of the length of the sequences and the accuracy of the representation of the sequences has been improving and I think one of the key components again, I won't dive into too many too many details here. is to a new Not new anymore. Five year old five six year old. no architecture in in like newer networks called the Transformer. This is the T in GPT. And I think the key idea of a transformer is that you can format sequences. and train At us like at a single inference step. So it's a technological leap is like a I think it's like going from like steam engines to combustion engines, but like if you take this this architecture is much much more efficient to train and to to model these sequences so you can actually grow the number of parameters. again for those unfamiliar with this topic. This made is maybe a little bit abstract. But remember we're taking input sequences and predicting the next word. And we do this through a projection. to dance representation and this projection is done. by Matrix Matrix multiplications to with very very large tables. So this is a hard concept to grasp. But at least I find it hard. but this is how text neural network works. So but then there is a question like what what this is actually doing is a very fancy auto complete like it's a very very fancy way of predicting the next word. do you go from that to an assistant? And you don't have to understand the underlying techniques from the fancy autocomplete to understand these next exports. So if I wasn't clear so far maybe reset that content and try to understand this part. So how does an AI system works you have this very fancy autocomplete that the sequence model. There's Auto regressive decoding. but this is only a part of the system There is something that I like to highlight that is formatting. So when you press enter on a chat board. Formatting is the first step. So what is formatting doing? So formatting then tokenization tokenization is basically a conversion to individual words. Currently they are not always words. Sometimes they are pieces of words, but formatting I think is the first step and one that is really important to understand if you want to work with applications. so formatting, is this process that takes conversation and transforms into a completion autocompletion problem and how you usually works is that you have a very specific format to represent conversations on these models. and they can often look like this like you have a preamble that you call a system instruction. Where you start you are very helpful AI assistant that is designed to help. People or humans on their daily tasks. Your name is Bob. like and then can add some examples to that this conversation you can have the user say Well, hey, what's your name? Then you have the agent say hi. I'm Bob. Are your your help for your assistant? and then you can input the the question that someone is asking like so how do you work? So if I ask the question, how do you work? should you a chatbot? That is only doing formatting? You sometimes you can assume that there. is this brand Preamble that looks like a conversation but that's hidden away from the user like so this was how we designed chatbots up to a certain point in the past like You would basically create this Preamble that looks like a conversation. but you always had a new line to this conversation in the end of the of the of the text and that is creating like this notion of But in practice like this is just a completion system, but it's a completion system that was tricked to believe it is in a conversation already and you just asking another question. So this is a very key idea of how do you turn? a completion system into a conversational AI And this happens to search to a certain degree on all the modern AI systems formatting differs a lot. I think between Agent to agent I think this particular formatting is the one that Gemma the open source Google models use but it's a real example like you can go to GMI and type this put this format and it will work. Then I think there is something that you also do on top of this models that is called instruction tuning. So it's how you update this. So after you train on the internet, so you have that sequence model. that was trained on all the sequences that you could Use so usually all the public web. or you can also go there and in the end of your process like Update the sequences to follow a certain pattern and this certain patterns are. usually Chosen and very curated and you can do this during force them responses. You can do this. You better adhere to the format that you use doing during during inference when you use the system. you can also use it to tune tone and form. But so what does these can look like I think if you're building a general assistant, you don't want to get into trouble buying doors in one candidate or or the other candidate in a political status setting so you can train the model to never answer your question like who is the best candidate? you can train the model to don't reply things that it doesn't know you can train them all that you have a name and this is now now can be removed from this Preamble and putting to the weights via this process called instruction tuning. These are usually like done after the fact. But it's important to keep in mind that all chatbots. They have these process. I think all the major ones at least. I'd like to think as these as editorial decisions like it's the same as the newspaper like the editor takes a look at what goes in the end and chooses how how the agent should reply but all chatbots have editorial decisions either implicit or explicit. This process is like I won't dive into the underlying techniques, but there are methods they get increasingly complex. You can do supervised fine-tuning. This is a method that looks a lot like Modeling the web but you can also do reinforcement learning. But anyways, this is just a footnote. But there are very important unanswered questions by this description of AI like how does fancy autocomplete get so good at random stuff. like and for those who use these chatbots you probably seen it doesn't feel like how fancy autocomplete like it feels like something else. this is a question. that I think this descriptive analysis doesn't. answer like it's we we actually don't know and I think we needed more teary around issue answer this question. And why even though it's really good at some stuff. Sometimes it makes like stupid mistakes. Like we you probably seen one or two examples in the web. Like if you follow the media, sometimes you will see chatbots making mistakes the these are some time fascinations. but we also don't have a good theory for why that happens sometimes. but I think the main question is like why why do why do they feel so good like and why do they actually are very good at some some of these tasks? So then I get to my second part interpretation philosophy in this course. So why should why talk about philosophy of AI I think the mainstream discourse is often very ideally a logical. and no matter what source do you you like I think It is ideological and this is not a bad thing. Everyone subscribes you an ideology. The risk is not being aware of the ideologies that are in communicated. And I think often things presented as pure fact are often under heavy dispute and I think specially if you're not like in the field it can be very hard to identify But yeah, I think hey guys permeated by several ideological assumptions that can actually get in the way of using them correctly. I think this is a this is probably the most important if you're trying to apply AI. these ideological assumptions can make you make mistakes. and so try to be careful and at least be aware of those when you're using them. So there are five open questions that I want you to talk a little bit about today. but there are no obvious answers. There's no right answer to these like again. these are philosophical discussions, but it's good to be aware of them and and to identify them in this course and when you see it. I think the first one is emergent. ism Is philosophical belief that phenomena cannot be explained by the son of parts? So it's usually be just used to describe Consciousness economy. Consciousness how it arrives from like matter from from brain matter and sometimes people use it in economy like transactions individual transactions are make economy arise but it's hard to describe economy by individual transactions. You can also think like a river as flowing but it's it's not an accurate description of a Reaver or it's very hard to just reduce it to that. so emergencies shows a lot on AI discords. I just used to describe it. it's a philosophical belief. it's not clear. So yeah, it's been showing a lot on AI discourse for some time now. there is this notion that if you make the autocomplete more powerful There are emergent behaviors that start to appear. So if you start to increase your match Matrix projection in your in your autocomplete. Component it becomes like more and more powerful. but then again some people argue that this is a mirage like it's just in incremental ways, but depending on how you measure performance and what types of metrics are you using? this scenes emergent? but if you there's often often, there is exists a metric that if you look It's incremental. So. It's a disputed topic. I think like we don't fully understand why this happens, but some people argue that it is not a real phenomena. But yes, I'll I think it's good to be aware. Emergency is used a lot to talk about AI. The second one is a more technical one, but is the notion of scaling loss. So this is this is a measured phenomena. So this has this has science backing it up. So if you look at If you increase so this plot is the validation loss. So the validation. This equation here. Actually the log of it, but if you look at the validation laws over the number of that the number of parameters or equivalent the compute that you spend to train the model. It almost certainly looks like a power law so. What's the interpretation of this? double the compute usually that our Fancy Auto Complete component gets twice as better. This is the interpretation of the scaling pericola. again, it's an empirical law. It's not a it's not a natural law. And we don't fully understand why it happens. But it's a major phenomena. But twice is better than what like this. It just twice is better at predicting. the next word and there's some research showing like okay, it gets better at predicting the next Ward. But doesn't necessarily get better at things we care about. So there's this notion of scaling laws. Do not scale. I think this is a very interesting paper showing that okay, you get better at predicting you get twice as better at predicting the next word, but what does this makes as impacting the real world and there's some research showing that sometimes it actually gets worse at things you care about. Even though it gets better predicting the next word. So the notion of scaling loss over the over the validation function is verified in previously, but the notion that making your model larger and larger you always get better. This is under dispute. And I think like it's important to keep in mind because sometimes people will say if I just have more compute Everything would be solved. This is not this is not entirely like a consensus Third thing let's talk about the Turing test. So you probably heard about the Turing test. and it's this notion of if a machine can fool who human that it's intelligence like it that it it looks like a human then it's probably intelligent like and this is not an accurate description of the Turing test, but I highly recommend learning about the Turing test if you don't But I assume most of you have heard about the tuning test be at this point. the Turing test Is an increased interesting for experiment? But I think this leads to if you subscribe to the Turing test. This leads you a trap called the trap. This is a term coined by this guy. but this basically like the Turing test makes us obsessed about replicating human intelligence. And sometimes we're not good at recognizing intelligence. That doesn't look human. a good example of days I think is like for systems that predict protein structures. This is something that humans cannot do. but AIS are incredibly powerful it and I think like it's probably one of in my opinion the biggest transformations in. the biggest opportunities in in the AI field But it's hard to recognize this as intelligence because it doesn't look like human intelligence. so I think the Turing trap invites us to think what types of intelligence We we can identify. That do not look human but are incredibly powerful and can help us. Improve our lives and live more fulfilling lives. Finally, I think there is one called entrepromorphizing it's the second to last. You must end to apply human-like values you inanimate and human no human entities. I think this has been highly documented. Throughout history. It doesn't only happen with ai's I think like this goes back to myths since I don't know the problem materials or that they're even an older like tellings of of this myth. but every time we entropomorphize we tend to lower our guard and the info size with with the thing that we are anthropomorphizing and anthropomorphizing is not bad per se like I don't think there is a problem with entrepromorphizing but it can be used as a design choice and I think that is where things get a little bit sketchy like you This can be used to deceive and make AIS look like they are more interesting and more intelligence that they look. and I think like often it's a design choice to include them. So you should always ask yourself Like is this AI trying to flirt with me? Like I think we've seen this happen before we always be always be doubt when when this happens like always take it with pinch of salt. There's also does notification like it's making a look like Disney characters. I think that's that's also phenomena that happens. Yeah. So be aware of entrepromorphizing and I think the final thing that I want to talk about is the shenon Divide. I think this is a term that I invite invented but if you go back to this light on Shannon, I think like by construction We removed any notion of meaning from symbols and we built our whole system around this property that so by construction we removed meaning so it's hard to reintroduce meaning like when you take the outputs of assistance without a theory a theory of semantics. So this doesn't say that these series of semantics don't exist, but you need one like otherwise, it's there is a There's a flaw in your like. epistemic step by step. reasoning on how how many arises so I think the Practical conclusion here is be very careful when you apply meaning and this usually showing metaphors to the outputs of these systems. So be careful with these metaphors like things like the AI understands or the a reasons or the AI thinks like these are very very powerful. That danger is Mentor Force. And these often are applied to negative metaphors as well things like hallucinate misinterpret lie. These are not like we don't have a theory of meaning for these AIS and I think we it's a huge opportunity to build theories of meaning to these AIS, but we don't have one. that is widely acceptable. So careful with metaphors specially, you know technical setup. finally my last part of the talk and no I'm a bit late on time, but I think I'll try to leave time for questions of try to be fast here. Formatting is incredibly powerful tool. I think I mentioned that before and I will try to give some examples about this. so formatting is sometimes called prompting engineering. I think this term gets a lot of hype so I try to be try to not use it. but If you do formatting right it's an incredibly powerful tool to like Take the best out of these powerful fancy autocomplete systems. there are two things that I want to talk about one is demonstrations or their strength Chain of Thought and I think there's a third one that includes light for editing. So editing, how do you take an account how to complete and you make a model that can edit things in the middle of a sentence? There is a trick that you apply to. to these language models where you with a certain probability like you reshuffle sentences, and you add this special words like pre-made and stuff. to learn how to predict things in the middle of a sentence and if you apply this to a very small percentage of your data set you're basically get this ability without damaging all the other code completion capabilities. and the fun fact is that this is something that you can use in your applications and most people don't know about it. but if you are in the middle of a document you can add edit things there just by using this trick. There's also happens. There are other representations that apply to the same thing think the second thing I want to talk about is is few short prompting. sometimes people try to describe a problem to these motors, but remember they are very good how to complete. So they're very good at finding the the patterns in examples. So I always suggest try to give 50 demonstrations. It's usually much better than explaining her problem and highly recommend trying few short prompting. There's also the opportunity to do instruction tuning. It's an effective but more expensive tools. So I always recommend people to try few short prompting first. And then try instruction tuning if that works and usually can do it very Cloud apis. You don't have to build instruction tuning yourself. And finally, I think this is another. Tip very engineering one don't roll. your don't train your own base models. Like it's a incredibly hard and expensive Endeavor like the same way You don't rewrite Linux. You don't build your own file system and you don't write your own database. Don't try to build your own auto complete try to leverage the ones that you exist in the market of the Shelf. Or maybe with some fine tuning and unless you're doing this for research like avoid building your own language models use one from cloud. You're usually better off this way if you're trying to apply them. Finally be aware of bias. The internet is not a neutral place. So representation in these models predicting the next word can contain bias. my main take away here is lots of these issues can be fixed and mitigated by these editorial decisions in the chatbot. But don't delegate decisions where accountability is important. We don't know how to fully mitigate those yet. Try to apply mitigations whenever you can. I think that's it. I think it's an incredible moment to be in this field, but also very loud one. So try to listen think and plan before you launch. I hope this this tips are useful as if you ever decide to apply AI. I will open the questions. I have some ideas here besides my >> Christian Esteve Rothenberg: Thanks. Thanks, Gabriela. >> Gabriela Surita: content if >> Christian Esteve Rothenberg: And well a lot lots of interesting stuff. I loved your animations the the and and the pictures that help also in getting in this election in the message and they're interesting topics that are thoughts and you mentioned so I was browsing here through the questions of the students as you know, they one of their activity previous to each seminaries a little bit of homework trying to identify which are the five most top relevant literature giving at the similar abstract. I will share it with you later. but and I was roasted through their questions and most of them they touch your last topic on on ethics. AI considerations Do you are you aware about techniques or efforts? Say this very relevant issue around the multiple dimensions. behind ethics racism sexism and can you shed some light? so that this biases can be overcome in the future or or there is no hope or what. what are your thoughts? >> Gabriela Surita: Yeah, sure. I think this I wish I had more time to cover this, but I also wanted to talk about this broader philosophical things, but Umbias, I think. I think one important thing is like this systems. They are practically reproducing probabilities. We see in the internet. And it's these two statements are very important. I think the internet is not a neutral place and it's not an accurate representation of society. So sometimes people are associate like all the probabilities that are in the internet should be reflected in the world. I think that's that's a very heavy statement because like not everyone is in the internet and not everyone has been in the internet for the same time. so but I I strongly believe this is the best we can get as a picture of this Society. What I think is there is a lot of like at least a written one if you can even if you introduce books and everything like that, it's very hard to go beyond that. what I think there are solutions is lots of these issues can be mitigated during that instruction tuning process. and there are some really interesting ways of like for example mitigating gender bias doing that step like you can basically make sure your priors for Male doctor and female doctor are well represented when you do translation from English to Portuguese for example where gender shows in inwards like like doctor so there's techniques that can be applied there. And they should be applied. And so there are lots of re interesting research there. Measuring these effects. I think it's incredibly important so you should be always taking these into account when you launch a system. You should try to actually measure these and I think the best way to accurately measure and know that if you're making that you should probably post on own your product launch is have like a diverse pool of people testing your system like if you don't have those, it's much harder to detect those but I I believe there is opportunities in research to do like more. To include fairness in your in and reduce bias in your responses in this like especially in this post training instruction tuning step. >> Christian Esteve Rothenberg: Right. I like this statement that don't or this recommendation. Don't delegate decisions where there is a second ability behind them. >> Gabriela Surita: Mm-hmm. >> Christian Esteve Rothenberg: That's a that's a very good one. Let me jump now over the YouTube channel to the chat. There is a technical question that you I really don't know. What are these vanilla Auto encoders the Ariel asks, do you consider vanilla auton coders as generative AIS since I can explore the Latin space? I don't know. what's late in space. I'm not doing good. >> Gabriela Surita: Yeah. >> Christian Esteve Rothenberg: Maybe you can Say something. >> Gabriela Surita: I think I think that this yeah, yeah, I think like not not a specialist in the field also, but like vanilla Auto encoders, I think are these models where you try to recreate content with a network? Via bottleneck so you have like very small number of parameters at some point in your network. that basically give you the the chance to do to tweak these parameters to the generation. I consider them generative because I think there is a more technical. definition of generative AI that I didn't use in my in my slides that are That is basically like something where the output space can be bigger than the input space. so I can it can be considered. I think generative AI technology, but I think it's more borderline. I don't think everyone agrees with this. But yeah, I would say I would say yes >> Christian Esteve Rothenberg: Okay good. And I don't know if you would like to comment Popped up. I was browsing here a number of students. They they probably saw the release of GPT for o and they want your opinion And also what is next or would you comment would you like to come and if you want to skip about gptu for RO that's also fun or if and if not, they >> Gabriela Surita: I mean I yeah, I can comment I think on a high level like I don't I think it's a very the field is very very active and there's lots lots of things going out by all the players. I sometimes I think there's a risk of over indexing to one particular release. It's everyone is releasing through eodically. I think it's when you look when you zoom out of it. you can see incremental steps in each one of these models. I think this really is in particular I think. Like this this week there has been like two important releases, I think. The one from openai and also the one doing Google I/O. I think these are both like interesting releases. I what I like about them. The most is that I think there is like this. huge push for Smaller models that I think I potentially use for in some ways. >> Christian Esteve Rothenberg: Okay. >> Gabriela Surita: But I want comment on details about about the model because I I think >> Christian Esteve Rothenberg: it's >> Gabriela Surita: it's a territory. >> Christian Esteve Rothenberg: it's fine, but I had to do my my work >> Gabriela Surita: Yeah. >> Christian Esteve Rothenberg: as well here by browsing through the questions, which I will share with with you as well and about this smaller scales, LA llms. And is there some space for also like Smalls scale applications in embedded devices in Salesforce that they don't have so much compute but for some specialized very much specialized tasks could be useful. That's one another question that popped up or Or we need to rely on very large-scale Computing >> Gabriela Surita: That's an interesting question. And I think that touches a topic that is very close in my heart. I didn't talk about this today, but like I mostly work on code generation. I think that's that's one of my my favorite topics and I think it's one of point of view one of the most successful any useful applications over here. And I think a lot lots of the recent models are embedded models that you can run in your computer with reasonable latency. So I I think there is tons of Not so narrow but narrower applications that are not like chatbots and they're highly crafted to one particular use case. I think code completion is a very successful story. Yeah, so I personally believe in that direction a lot. and that's not just say that they less powerful. Like don't get me wrong if you take have more model that works on a smartphone now. It's probably better than like not probably it's definitely better than the best model from two years ago. So they're quite I highly believe this this application of like more targeted in own device use cases. >> Christian Esteve Rothenberg: and Andy >> Gabriela Surita: I also like the idea of use cases that do one thing but one thing >> Christian Esteve Rothenberg: right >> Gabriela Surita: really really well this gets into products, but I >> Christian Esteve Rothenberg: like specialized LL and aims for a certain domain for a certain task >> Gabriela Surita: yes. >> Christian Esteve Rothenberg: that a lot of sense right because a like there's this statement if you want to do everything. You think you do everything? Well, you don't do nothing. Well or at all, right, that's >> Gabriela Surita: Yeah. >> Christian Esteve Rothenberg: interesting And as you know, we in the electrical and Computing engineering we will also look at Hardware a lot. There were a few questions here Hardware related. So today most of the Computing are exploding gpus right fpga's. there is also in the AI domain the tensorflow the it has a specific tensor Processing Unit. Do you see also Hardware about Harvard that is going to be designed for specific and type of or generative AI applications to accelerate it. Maybe it's already happening and I don't know. Sorry. >> Gabriela Surita: I think it's definitely happening. Like I think the latest generation of tpus and gpus are mostly designed for AI and I think they are extremely like generative. here and they're extremely like overfitted to this problem like I think they're huge huge opportunities there like I think. if you again something that is Up For Debate, but if you think AI is a revolution. Similar Au and and this is the view of this revolution, right and usually if you look back to like revolution industrial Revolutions in the past like you need few to so you need energy and you need like maybe it's not the but it's the iron like I think you have to think about the best comparison here, but so lots of companies that focus on Hardware are extremely well valued and I think like there's tones of opportunity research opportunities on doing better hardware for for large language models and generative AI Technologies if you work on this field, I'm very happy for you. There are loads of career opportunities for you. >> Christian Esteve Rothenberg: good at last question the same that we had we are already 2PM So what would you recommend to our students rather than undergrad if they want to to succeed and have and be employed like you in Google deepmind? What what is the key there? What would you what would you be your We're talking to to this undergrads undergrads and grats. >> Gabriela Surita: Be curious. I think don't be afraid at applying to these hard positions. Try to learn. a niche very well that like on the things that are hot try to find a niche that you do very very well and other people don't know much about it. >> Christian Esteve Rothenberg: feel they >> Gabriela Surita: And yeah curious overall. Yeah, that's that's why me advice I'd say >> Christian Esteve Rothenberg: great. >> Gabriela Surita: and don't be afraid to replying to these positions like it's hard to get in at Google but not that hard and it's it's a pretty interesting company to work with to. >> Christian Esteve Rothenberg: I loved it. So, And if you step by of course come over the faculty and say hello and keep the good work and Hope looking forward for the upcoming releases over there. >> Gabriela Surita: It was a pleasure to be here. >> Christian Esteve Rothenberg: Yeah. >> Gabriela Surita: Thank you so much. Bye now. >> Christian Esteve Rothenberg: thank you. See you all next Thursday with the next talk. 