Christian Esteve Rothenberg: Good afternoon, or good morning. wherever you are. Welcome to another seminar of topics in computer engineering say mentioned we are towards the end of the semester and as a advertisement that the next semester will have again. This is seminar series and the applications are open and fully remote participation. It's full in English another Edition that we will have. >> Filip De Turck: it >> Christian Esteve Rothenberg: in the second semester. And today we have a guest lecture. I like to believe comes Oak against lecture. by Philip The Turk and as usual Filipino spend time in introducing the bios of our speakers because there are large Rich easily found online. And but if you want to say a some words about yourself more than and then for the attendees prepare your questions we have around 15 minutes. towards the end for for interactive Filip and a topic that I say. I was telling you we are investing. time students research. What is a holographic type Communications how to do research that could be meaningful for for this area. So looking forward to to learn more about what you are doing what you see that our communities doing and >> Filip De Turck: you >> Christian Esteve Rothenberg: let's see if we can collaborate. and do some joint research, okay. Thanks, baby. the floor >> Filip De Turck: Okay. Thank you. Thank you Christian. and dear students. Welcome to this to this lecture. It's my pleasure to give this lecture of about 40 minutes. and then of course, I will be happy to interact with you and answer questions and maybe I will also ask questions to you maybe. So maybe just as a as a brief introduction to myself, so I'm based in Kent University and in Belgium, so here you see the partial map of Europe as you can see. Hopefully you can see the the pointer here. Yes, and Belgium is in yellow here. So we are close to England France Germany the Netherlands, Luxembourg. And I'm also part of iMac iMac is a research institutes in Flanders similar to front over and in Germany. And my research group is called the idealab research Group, which stands for internet's data. internet technology and data science lab We have many experts in our research group. I will also collaborate a lot with industry Partners. So that's really something that we put focus on and I know Christian is also a person well now researcher. global scale who also likes to work with with industry partners And the list with industry Partners here is not exhaustive. So it's just some examples of Industry partners that we that we work at. My main expertise is on network softwareization network automation Service delivery. I was also editor-in-chief overnight triplet transactions journals so Network and service management so feel free in your future careers. to submit to this journal or maybe during your Masters already and I was also a chair of the i3pld technical committee on network operations and management. and so this IEEE technical committee is one of the 28 technical committees. insights com communication Society. So here you see the entire list of technical committees and I highlighted the network operations and management technical committee and this technical committees are very interesting also for young researchers if you find topic interesting and you want to start building a network so getting to know people that work on a topic have personal conversations with people organize workshops special issues. and so on then these technical committees are very good starting point so you can contact Your published online you can contact them get in touch with them and then this is always a very good acceleration. to improve an extent your personal Network. So this is a small recommendation. So I mentioned softwareized Networks. so this means that we bring software to the network. So not only Hardware but also software and most important element is that we bring virtualization Concepts to the network. And this is similar to cloud computing. So I assume most of you if not all. Or familiar with cloud computing. So if I would ask you what is the main driver? For cloud computing to be successful. so it was launched about 20 years ago, and then the main driver to be successful. Was the availability of virtual machines so that on a physical computer physical machine you can start several virtual machines and you can start serving multiple users at the same time by making use of these multiple virtual machines. And the same reason is that if we can virtualize networks so that we own a single physical Network. We can run multiple virtual networks Then we can also start serving multiple customers providing service guarantees to all of them. thanks to this Concepts and this is really a game changer and the Technology. also an industry. So let me explain you very briefly Maybe you're already familiar, but I will explain it briefly. So even if you're not already familiar. it will be interesting to to learn. what is Network function virtualization because I will come back to this once I explained Holograms and volumetric media delivery then I will also Explain how virtual Network functions can be of interest so if we consider a home environments then there are multiple devices and in your home and typically there is a home Gateway. And on the home K3, we have different functionality. There is the HTTP server. There is an entity router firewall also quality of experience monitoring software for instance. if we have TV streams or video streams that we display on screens inside your home then we also monitor the quality of experience inside the home Gateway. And then typical Auto device is a setup box. so stb. Which contains? for a digital TV storage so that you can record movies and TV programs. Middleware that allows you to interact through remote control. And to start recording stop recording play a certain TV program. and so on for unless we need to middleware on the setup box. And also we have a streaming client. So this is the software that starts streaming the files and make sure that we can see the videos on the screen. so if you look at these functionalities on the home Gateway and Books, these are typically called Network functions. And when they are executed on these boxes Then they are executed inside your homes. but if we virtualize them so virtualized Network functions vnfs then They will be. executed not in your home, but in a remote Data Center. and the data center can be close by but can also be far away and another country, but then we software rise this this functions and and they run as software components inside remote Data Center. And this is a very important Evolution because then we don't need these devices in your home. You just need to connect the different screens. to the network sockets so that they have internet connectivity, but but all the other functions that are listed here. They are no longer executed inside your home. and there are many advantages first from point of view of a network operator. Because we can maximize resource utilization. and also optimize energy usage because if all the functions they run on separate devices in each home. They need to run permanently. but if if they are in a data center then we can run many of them on on the same machine. making use of virtual machines or containers inside the data center. and also in terms of Updates, this is a very important Advantage when we update Network functions, then we can very fast and easy deploy them. And and configure them. and it's much easier than changing software on a dedicated device at home in a data center. We can overnight quickly and efficiently updates Network functions. And also for the service provider there are many advantages. Which are listed here. so we can dynamically scale Network and Computing and storage resources. based on on service requirements. so we can make use of elasticity properties of cloud computing. and we have a reduced time to market for services. So if we have a new service. We can quickly introduce it and customers can make use it very fast because they don't need to buy the equipment. They don't need to connect manually. and it can be done automatically. Thanks to the software in the data centers. Okay, so this was a very important topic that I wanted to introduce first. And the second topic before we will delve into Holograms and volumetric meter delivery as the topic of adaptive Service delivery that I want to share with you first because it's important to understand this basic concept. is that if we have a video file That we want to convey over the network and show on the screen. Then in case of adaptive service delivery. We will. The the file into small chunks so each chunk for instance of 4 to 10 seconds. and footage Small Chunk we will provide. a low quality version a medium quality version and also a high quality version. So This is what you see with the different numbers here. the low quality version corresponds to number one medium quality number two and high quality number three. Okay, and the higher the quality the higher the bitrates that you need need over the network. I know the important element is that at a client sites? So here you see the client for instance who watch your movie on the smartphone or a tablet. and At this device at a client device we will make an estimation of the available bandwidth. and Okay good an estimation of the available bandwidth. And there is an algorithm here that calculates which video quality level can be transported efficiently over the network. So based on the estimation of the of the video quality as you can As you can see here on the screen Here's the estimation of the available bandwidths and then at the client side a decision is made. no we will. requests low quality one at this moment. We have a high bandwidth available Then we will request a higher bandwidth. And here we have a drop so then we will request medium quality. and so on. So this is the algorithm that YouTube is using so I believe this talk is also streams through YouTube. So the same algorithm is being used. and this lecture is delivered over the network Okay, we will use it for holographic type Communications. Okay. So next let's also now have a look at volumetric media. So I interviewed two important Concepts. I know I will focus on volumetric media and volumetric media delivery. so volumetric media delivery corresponds to the a technique that captures three-dimensional space and so we Are very used to two-dimensional video? in the past. So for instance here is a person. playing two-dimensional game and in 360 degree. video we are much more immersed into the situation. So there is a deep absorption and to the environment and into the activity. So for instance, this is for Formula 1 racing game. And you can pretend to be Max verstappen and you also see. with the virtual reality goggles and you see you have the same citation as if you wear in the race car. similar to maxford stopper. If you look at volumetric media. This is a technique to capture three-dimensional space. So then we also where Vargas and then objects can be displayed and we can look at your objects and it is as if your objects or really there and for instance we can then play a game and objects or shown. to us. And this is a technology like a virtual reality or Entituality and it really expected to play a key role in the future also from economical and societal point of view. so if you look at volumetric media delivery in the traditional 3D images there are two static views. So there is a view for the left eye and a view for the right eye and these need to be synchronized. But if we focus on Holograms then we add parallax. and Parallax as you as you can see here is the upper end apparent displacement of an object base. on a change in the observer's point of view. So if the Observer changes the point of view, then you have the feeling that you can look around the objects. So next to the object or on top of the object or even underneath your objects, so you create multiple views and these are the different movements. That you can make so if you make movements in these directions. And these are called Six Degrees of freedom. Then you have a different view of the of the objects. And for instance if you make use of Google meet or Zoom, then typically we have two. dimensions and if we make use of fully metric media, then we have six degrees of freedom and there are many important use cases for this. One of them is the holographic collaboration and conferencing we also call this daily presence because a person who is at a remote location. is then as a hologram displayed close to you and it's as if the person is close to you and you can interact with a person but in reality the person can be far away. And second important use case is daily surgery or in healthcare remote patient monitoring. Where you can also display in for instance in case of an operation a surgery. important. can also show the important parts and then surgeons can work together. or can monitor patients that are remotely And also very importantly there's use case of industrial monitoring and management. so industry for those here or even 5.0. where you can remotely inspect objects that are produced and a factory for instance. And instead of inspecting them locally, you can inspect them remotely and then you can decide which of the products are meeting the requirements. This can be. kept and which objects are not meeting. So thanks to this Holograms all these use cases or or possible and there are many more use cases and I I will explain some of them in the next Pages as well. So it's important to realize that the Holograms are not science fiction anymore for instance. There is a German circus. that's uses Holograms. instead of animals for instance a hologram of elephant here. There's also contributes to the Animal Welfare. and there's also the famous hologram of princess Leila and the Star Wars episode. and these two examples are local applications. So they are not transported over over the network. and if we would like to transport Holograms over the network then the bandwidth requirements are really very high. So in the order of gigabits per second. So then we really need compression to be able to transport all this data over the network. and of course as we are computer science Engineers these bandwidth requirements and also delay requirements or very interesting to us. And for instance, let's have a look at which bandwidth requirements do we need? for transporting video over the network? so if you have high definition television for instance as you can see here. For instance up to five megabits per second then if we have a higher quality and six 360 degree video corresponding to to 4K then up to 25 megabits per second and even 16k. Times 20 so 500 megabits per second. and if we then look at the Holograms then there are two important Technologies. There is the point clouds technology and also the Light Fields technology. so an endpoint clouds Let me explain it with the famous picture here. So this is the Stanford bunny. And if you want to represent this object through to a point clouds as the name suggests, then we will consider all possible points of the the objects. And three-dimensional space. so we will determine for each point on the surface and XY that's coordinates as you can see here for this point, and we will also add color information. So rats green blue information RGB value so six data points for each point and if we then have this information for all different points Then we we create a point cloud. objects and if if we transfer this information over the network, then we can create this Parallax effect. So then we can look next to the objects on top of the object underneath the objects and large. and the size of the object So based on this point where information We can really display it as as a hologram. and important optimization because if you have this point class the consume a lot of bandwidths. you really need a lot of data points. a lot of information and if you make use of meshes Then we will construct. triangles. So small triangles on top of the service and then we also keep the information about the location of the of the triangles together with the color information inside the triangle and then we can have small triangles and then we have a high quality representation. And if we have bigger rectangles or bigger triangles then we have a lower quality estimation of the of the object. So by changing the size of the of the triangles we can adapt the amount of data that we need and also the quality of experience. That corresponds to the objects. So now I explained the point clouds to you and so when we make use of this technology that we will focus on the object itself for instance. Here's a person's skiing down a mountain and then making a point cloud of this person is only the person itself. not the environment. But if you also need the environments so the mountains behind the person then we need image-based techniques. So it's a light Fields technique and then this is more computationally and intensive there. We have to reconstruct a planoptic function, which is a five-dimensional function representing the intensity of Lights. observed from every position and direction in the three-dimensional space. based on a set of example images so then we we can based on this plan of take function We can generate realistic images by indexing the appropriate light rays and then we can construct. and make use of Parallax to really as a hologram. Display the information. but an important observation that you you probably already made is that if you look at the required bandwidths For light Fields technology. They are much higher. Than for Point clouds. So this is the main reason why we typically use Point clouds. If we know give demonstrations with Holograms volumetric media, we typically use Point clouds, but I expect in the future because we can also model the environment and not only the objects in the future. light Fields technologies will be very important. So I also want to take this opportunity to introduce a European projects to you, which is the spirit projects. It started in 22, and it will run until the end of next year. here you see the different partners in the Consortium. So we have deutschutil Telecom on board. Also Erickson as an equipment provider. I know Christian has a very good relationship with Erickson. And also universities and research institutes are involved and iMac listed here. This is my affiliation and iMac is leading this European project. And the aim of spirits. as listed here is to build a scalable platform for Innovations. on real-time immersive telepresence. So the telepresence that I explained through Holograms that are shown close to you. And to allow for conference use cases, this is really the aim of the project to build demonstrators and Innovations on top of this. So it's important to understand that in order to be able. to provide daily presence through Holograms really as a service then we need Innovations on different levels. So we need Innovations at Network network layer. on the transport layer so different transport protocols. I assume you are familiar with some of them. and also at the application content and security layer. There is a need for several Innovations, and these are covered insights the spirit project with these partners. And let me show you a main diagram of the of the spirit project and here I make use of the virtual Network functions that I introduced in the beginning. So I explained you this is an important concepts for softwareized networks, and we also make use of them. These are the key elements and the European project Spirits where we have a lot of virtual Network functions. to enable volumetric media delivery over Networks. so we have virtual Network functions for compressing compression and encoding of the streams then also virtual Network function for transport buffering merging and synchronization of streams also for view prediction. So to predict very user will most likely view so that we can provide highest quality. there For decoding and also then for rendering and monitoring of quality of experience. So we have different virtual Network functions and then we have to deploy them on the available Network infrastructure and there is a edge Computing. So we have data centers close to the end users and if we need low latency then we should deploy some of these virtual Network functions. very close to the to the end users and others we can deploy inside data centers in A cloud computing environment. So it's important that we make a right decision where to deploy this vnfs in order to guarantee High throughput and low latency to the to the end users. and so we build a demonstrator where we have Android Telecom, which is based in Berlin and Germany and then University of Surrey is based in London. We build a demonstrator. So to demonstrate telepresence with people presence in Berlin and in London and to show Holograms of each other and interact with each other as if everyone is local and close to each other. and the the spirit use cases that I want to show you. or List it here and this corresponds to typical use cases when we talk about holographic type Communications. So we have the real time human to human interactions. So holographic conversations. when you want to have a conversation with the person that that's not locally there but at remote location. And also importantly real-time human machine interactions. Where we interact basically with for instance mobile robots autonomous mobile robots, and there are two options either. These are human initiated. So then a human requests and mobile robots. to take up a task for instance to supervise Factory plans. For instance. So you give the robots a specific task and then the robot takes care of this this task can also be machine initiated. This means that the mobile robots They are autonomous and they will alert humans when they observe something or they want to solve a contextual problem and they want to have guidance then they they contacts humans. They're also open calls in the Spanish projects. I just mentioned this very briefly. So other parties. Research institutes or companies they can also make use of the spirit platform and demonstrate their applications and the applications. that are covered and I will just go over this briefly is in the healthcare domain retail domain education training entertainments, manufacturing. so industry for those zero. 5.0 and also in the tourism domain so you see many targets application domains. So there is really a lot of application potential of holographic type Communications. okay, and a very important question that we should have in mind and that you typically also ask yourself when you consider the high bandwidth requirements. These are really huge and then the main question is That you ask yourself. Can the current infrastructure deal with this volumetric media delivery. And for this we did many experiments in our lab So based on point cloud scenes for instance of the objects that you see here. friends. If you have four of these objects that we require close to 20 gigabits per second. And then we did experiments. in overlap. where we make use of adaptive streaming. So the Adaptive streaming that I explained you in the beginning this one this technique we used for transporting the Holograms through Point Cloud scenes over the network and now we extend the the basic technique by considering different objects. So as you can see here, we consider four different objects and for each object. We have a low quality version. a medium quality version and then also a high quality version. And at the client sites, this is no. at the Googles that the user is wearing. There. We make a decision. based on the available bandwidths and for each of the different objects we make a decision. which quality levels can can fit. and for instance here as you can see of the available bandwidth is slow then we can also only use for the four objects the lower quality if we have more bandwidth available then typically we will select the objects where the user is watching at. So where users being attention to this? We will give the highest quality. And the others can have a lower quality and so on. so we can dynamically change the quality levels based on the And it's important if you do research on Holograms. holographic type communication as Christian mentioned in the beginning. It's important to not only focus on objective quality. measurements, but also do subjective studies so involve users. um and interact with Holograms and then also let them give their personal opinion on the quality levels. And so we did experiments and and overlap. You see the the settings here with four point clouds and different video sequences and different configurations of available bandwidths. and efficiency of the viewpoints prediction within total 30 participants of the subjective study and here you see a typical output. So the mean opinion score and as the The output that user's GIF. So if they really like the quality then they give five on five typically five stars and this corresponds to to 100 if they only give one star one on five. This is a score of of 20. So this is the cumulative distribution function that that you see here and depending on the scenario. So in the low bandwidth scenario medium bandwidth high bandwidth and even unlimited bandwidth scenario you see the mean opinions course of the users and as you can clearly see from this observation, the mean opinion scores or less than we expected. So we expected and then high bandwidth scenario or even unlimited bandwidth scenario much higher mean opinion scores. So it's it really means that there is something else that we should pay attention to because the mean opinion scores are lower than than we expected and the main reason is that the techniques that we use to the Adaptive streaming does not cater to the latency requirements. so we not only needs Innovations at the application layer as as in the example that I gave it adaptive streaming. This is basically an application layer optimization, but we also need Network layer optimizations and also across layer based end-to-end architecture for volumetric media delivery. And this is the platform that I want to introduce to you. So if you want to achieve immersive. so so that we can be deeply absorbs and into the contents. volumetric media delivery that we really need a cross layer approach. And here you see the the different layers, so we we have a holographic end user one and a holographic and user 2 The communicate with each other so here you see the the cameras that record a person and then here hologram is displayed. and so we need optimizations at the application layer. So we call these end-user optimization. So at the client side or the server side. also optimizations over the top and transport optimizations and then also optimizations with respects to network architecture. So let me briefly go over these three layers. I will keep it briefs because I want to meet the time requirements. and the latency requirements that Christian gave me. So in terms of end-user optimizations there are optimizations possible at the client sites. And at the server site so at the client sites, there are different feeds. as you can imagine visual feed or your feet and and tactile feeds. And we can make use of few Port prediction. This is a client side optimization. So we try to capture where the users are most likely watching. This is the viewport that we try to predict. We also need to synchronize the feeds and also importantly and this is a topic that in my opinion will be very important for the next years and research as a cyber sickness assessments. Because this is a feeling that users sometimes gets when they make use of. virtual reality applications. They can get Cyber sick because the expectant objects to be at a certain location at a certain time and if there's not a case then they can feel unwell, so it's important that we can predict and and this so that it's it's convenience for users to make use of holographic type Communications. at the server side we can also make use of tiles instead of entire objects. to divide them in different tiles and then the tiles can again have different quality levels. So then we have more freedom to decide which quality levels so that we can adapt to the available bandwidth. and then in terms of the over the top and transports optimizations. There it's important that we make use of of the right platform and Technologies. So if you make use of HTTP adaptive streaming the technique that I explained you then typically we focus on the quality optimization. We optimize for the best possible quality and then if you use other techniques like RTP UDP streaming or making use of the web RTC together with quick platform Then we really focus on latency optimization. and here you see the pro and cons of the difference techniques and for holographic type communication we in fact need the best of both Worlds. So we need a combination of this and that there are different techniques that are there that we can use for combining advantages of different transport layer protocols intelligent buffering techniques and also smarter return transmission techniques HTTP push that comes with HTTP 3.0 for instance. that allows us to obtain lower latencies. and then at a transport layer and control layer Innovations, we also have the virtual. Reliable quick also called PR quick. so quick as a transport protocol by my Google and these are extensions that investigated and recently published So here we make use of partially reliable quick where some of the tiles we can make them reliable because here most likely the user will watch this is a personal server, So most likely you will watch the person and serve words and not so much the environment. so then we can give These higher quality make them reliable and the delivery and the others can be unreliable. so they can be displayed in a lower quality. And then finally is Need for Innovation on novel Network architectures. and here optimizations with virtual Network functions that I explained in the beginning. So the service function chain optimizations or really important. so the efficient placements of these virtual Network functions and making sure that we meet the requirements in terms of latency and and throughputs. This is really an important. technology. So the main aim is that we can very conveniently very comfortably make use of holographic type Communications and communicate with people as if they were close to us. And in my team we designed a platform for this and we call this as our fog. So it's based on segment routing and also the concept of four computing. and we make use of microservices and the microservices then the corresponds to the virtual Network functions, and we optimally place them also by following the kubernetes architecture model. Basing them efficiently and sidebots inside containers. to be able to make sure that the content delivery. very efficient a very convenient to the to the end and users. So here you just see an example of VR user one VR user 2. that communicates with each other and in order to enable this we need several virtual Network functions that are shown here for scene merging quality scaling movement prediction and so on and then we decides which service will put in which which port and then where we will deploy the deploy it so and we have different options. This is a plot environment. This is an edge environment and this is a folk environment. This is a Raspberry Pi environment and then we decide where to deploy. these virtual Network functions and then we can host the service and really enable the holographic type communication between the the two users. So let me know go to the conclusions Christian. so there are many open challenges and and topics and I'm happy to learn that also Christian. is interested in holographic type Communications. So I look forward to a collaborations. believe there's a need for further research in terms of network optimizations So I lists several topics here. also in terms of predictive orchestration and distributed networking will also be very important also in 10-based networking will be important and also application optimizations that I explained. different techniques that I explained need to be further investigated optimized. And in my opinion, this is really a challenge that we have to tackle with a lot of researchers put an industry wooden and Academia. and then I believe we will have or we will be able to make progress and to obtain very interesting results for the future from an economical point of view. But also from a societal point of view as I explained. So it is also important to me. that we can also make some progress from a societal point of view. so with this Christian I would like to pass the floor back to you. >> Christian Esteve Rothenberg: Thank you. >> Filip De Turck: Thank you. >> Christian Esteve Rothenberg: Thank you Filip. very very interesting stuff And I was browsing through the spirit here deliverables looking into the the what you call the platform. I was a looking for some GitHub repositors where that students could start playing with with these volumetric media Concepts and that was one of the challenges that in our group where we >> Filip De Turck: if >> Christian Esteve Rothenberg: encountered like the barrier to entry of entry into this research because you don't have these cameras the end devices. so easily, what's up, high? How do you see the how to start and what open source components to put together Easter? Do you share my my feeling that it's not so easy right now to start or there's some Is there some light? yes, we're going to see more open data more open source to facilit. >> Filip De Turck: Yes. Yes. Yeah, excellent. Excellent question. So in my opinion, I I really hope to see a significant increase in the amount of Open Source software and definitely from from my team for instance the SR folk platform that I mentioned we made this open source, so it's mentioned in papers on this as it was published for instance and cnsm. um previous editions of cnsm conference and there we also have the hitup links to it. So I I definitely see a trend in research and Publications that a lot of researchers also are moving towards this direction. and also what you mentioned Christian in terms of barrier in terms of equipments. that you need so cameras and and so on. this is indeed true, but but even with with a single or with two cameras you can already do an interesting experiments. Um, and once a point cloud is created. So the first step in my opinion is always you have to create a point cloud. And when this is created you can publish it or you can share with all the researchers and then you can start doing your research and then you don't need a camera anymore because you have the point clouds that captures everything and then you can do the calculations. which oneization of it. so if we share point class and my team, we have Point clouds that we make a publicly available. So based on this you can do a lot of interesting research even if you don't have the camera or the expensive goggles. so it's of course good to have a few of these goggles. So that students researchers can have a feeling. And get acquainted to the to the environment holographic environment. but there is no need to have 20 or 25 of them. even made a limited number of of Google's. And cameras you can already do very interesting stuff in my opinion. >> Christian Esteve Rothenberg: Nice nice. >> Filip De Turck: look >> Christian Esteve Rothenberg: Yeah, I guess what some of my students have already used your point cloud videos. I can see. Alan is putting here a question. I'm very technical one or over the YouTube chat. He's asking about the partially reliable quick. >> Filip De Turck: Yes. >> Christian Esteve Rothenberg: if you I tracking on mounted head devices as Amanda Tory feature. >> Filip De Turck: Yeah, okay. Okay. Yeah. that that that's a good question did so your student has as clearly have a detailed look at the paper here. So the paper was published and I typically access I think a few months ago. And and there we indeed that the experiments where we included. viewport prediction and also also eye tracking and the main goal of the eye tracking was to make a prediction where the user is as watching. so Then we can provide a highest quality levels. where the user is watching and then the background surroundings can have lower quality version. but if you have very accurate viewport prediction, then you don't need to the the eye tracking. So eye tracking is mainly used if used is not moving a lot like myself now. I'm not moving a lot. So I I only make very few movements and then I tracking is useful But another applications then users are moving much more and then you have more data to make a prediction. where so if user is moving towards something then most likely during the next second or two seconds. The user will also move following the trend line. So if if you have trend lines available, then you don't need so much eye tracking. but for more Statics scenarios like myself today here then eye tracking is more useful if you want to optimize. Quality level, of course if you have sufficient bandwidth available. then you don't need it because then you can provide fairly good quality at all time, and you don't need to make a distinction. between high and lower quality. >> Christian Esteve Rothenberg: Okay. >> Filip De Turck: I I hope this answers to some extent the question of your students. Thank you. >> Christian Esteve Rothenberg: Thanks Filip. And so I learned I was last week close by I could to get but I was in advert in ucnc and I learned there is negative latency. That's a I hadn't learned this term. It's about prediction. So if you can predict the the next frame or the the next packet that will arrive you have you achieve negative latency just thinking about blowing my my mind because you mentioned prediction prediction a couple of >> Filip De Turck: Here. Yes. Yes. Yes. Yes. >> Christian Esteve Rothenberg: I guess then. AI ml artificial intelligence machine learning are are also at the heart of many of these topics that you have put here as research as a tool to solve many of them. Do you? You can you mention some AI or a mail to the church so many hates to see us that one to use. ml AI Beyond. Digging and movements. What else do you see applications for this? >> Filip De Turck: Yeah, okay, okay. very good question. Very good. So in in the past this machine learning artificial intelligence was typically used to predict movements of users. but also in terms of cyber security. or excuse me Cyber sickness detection and possibly prevention. There will also use machine learning techniques based on a lot of data that we that we acquire and also physiological data from users. So also heart rate maybe stress levels blood pressure. and and so on so physiological data That this generates a lot of data captured from a user's body. of course if they are willing to to participate to such a study and then based on this data you can also predict. when a user might feel unwell and experience cyber sickness and so on so I believe in that region there will also be a lot of application of AI and machine learning. and also as I explained so that the placement of virtual Network functions so the the software modules that has to be started in the edge or in a cloud environment and this has to be in a very Dynamic way and there we also make use of AI machine learning. To determine where we should put the components when we should move it to another location. and so on because this is something we cannot do manually. It's important. It's impossible to make a especially when it's very Dynamic and we have hundreds of thousands of simultaneous holographic type communication users. Then there's something where we will need ai-based network service management techniques. to make sure that all these software components run properly have sufficient bandwidths latency is not too high and so on so there definitely I see use of machine learning and very large scale environments. And they're my team is also focusing on edge clouds optimizations making use of machine learning techniques. >> Christian Esteve Rothenberg: Nice, and I guess there are also opportunities for Hardware acceleration right for for smart, needs fpgas this BFF. >> Filip De Turck: now >> Christian Esteve Rothenberg: the probably the the performance the latency. that's also another subject right as well. >> Filip De Turck: Yes. yes. It is. Definitely there's definitely another subject. then Hardware acceleration can be beneficial and and in the class. so in the data center so that you have a higher throughput. there and even lower latency. for instance if you have an encoding or decoding then with Hardware acceleration that the decoding or encoding time is reduced. a significantly But but you still have to take into account the latency of the of the network and careful placements of the still be important. But but definitely Hardware acceleration. also for security, this is topic that I didn't cover but security is also a topic in spirit project and there also Hardware acceleration is also considered To make sure that the security protocols can be run properly. >> Christian Esteve Rothenberg: Good, very interesting stuff. I like the connection the possibility of this physical world and to understand the quality of experience very user-centric where the right >> Filip De Turck: Nope. >> Christian Esteve Rothenberg: friend to I can imagine different types of persons of users will really evaluate their experience. So, I mean, it looks to me like it's a very hard problem. It's quite of experience. They'll be active metrics as far as I understood are not fully sufficient, right to >> Filip De Turck: Yeah, yeah. >> Christian Esteve Rothenberg: us. Yes, and then the users may change. a lot their their scores. What what is you the lesson learned around the qoe that you could highlight or what is the conversate of the art or what? What are less understood you of course, you put it here to increase queue, but What would you? highlight on this qie objectives objective studies >> Filip De Turck: Yeah, this is indeed a very very important area and there is also a lot of standardization and efforts by the impact. consortia. So the also focus on standardization of objective metrics techniques for subjective evaluations to standardize. So that they can serve as a kind of a benchmark. so that if companies say quality of experience is this High and other company has an older number so that we can compare them. so there is lots of efforts not only a research but also in these standardization bodies. And it also focus on the holographic type Communications already since not for so long already, but Two years or something. They did really start focusing on this and on query models for holographic type Communications. So this is an important topic to keep up with the standardization efforts and and so on and then my Approach is typically that's I start with an objective evaluation. Where where I measure the received qualities the obtained latency so things that you can really measure and then from this I try to derive whether there was a decent. delivery of the contents and try to put it in a percentage. And then next to this then. I I do subjective evaluations. To see if the objective evaluation is indeed, correct? And then there is really a feedback loop. Because based on subjective experiments. Sometimes they're new elements that you should take into account also when you do the objective evaluation. That user is pointing to something King and that is really devastating for for the quality and then you should take this into accounts. and and so it's really iterative approach in my opinion. and my goal is always to find the best objective metrics because if you want to make the system autonomous autonomous system. then Then it's very hard to do this with a subjective evaluations. so in my opinion subjective evaluations are Or key to improve the objective. >> Christian Esteve Rothenberg: right >> Filip De Turck: evaluations >> Christian Esteve Rothenberg: and the subjective other sufficiently were standardized and documented or do you need to find your own methods the methodology? >> Filip De Turck: Well, well, yeah, there are a few conferences for instance. >> Christian Esteve Rothenberg: Yes. >> Filip De Turck: There is the coolmex conference. And so it takes place next week in Sweden and this is typical community that publishes a lot of papers on this. And they have some guidelines for instance. You need at least 25 participants. And you need to ask them at least 10 different questions. and so on so they have some some typical approaches and they're published papers and I typically follow these this guidelines and I sometimes also publish in this Comics conference for instance last year. 23, it was organizing and I I organized together with my colleagues, of course. We organized comics in, Kansas. >> Christian Esteve Rothenberg: Nice nice will have a closer look at this and okay it really it's time. We need to to the to get the the timing right the students now have other classes. It was great talking to you about this very exciting topic as I mentioned in the smartness resource center that we have still nine years more to go early with the whole family. >> Filip De Turck: Oh. >> Christian Esteve Rothenberg: with Elixir and the best Sao Paulo funding agency. We are happy. We have a track on HTC on holographic type Communications and your group. >> Filip De Turck: I see. >> Christian Esteve Rothenberg: is a reference. The spirit project is also one of the inspiring ones that we are following and reading the deliverables looking for the open source code and data and >> Filip De Turck: Yes. >> Christian Esteve Rothenberg: let's let's catch up on the possibilities of collaboration of A student that exchanges that we have also good funding to to send you our PhD students. and stay and enjoy beautiful Belgium. Okay. Thanks you. Thank you. >> Filip De Turck: Okay. Thank you Christian for the invitation for your interests and all the best to do to the students. >> Christian Esteve Rothenberg: Thanks. >> Filip De Turck: So all the best. with the Academic Thank finally, okay. Thank you. >> Christian Esteve Rothenberg: Thank you. Take care. >> Filip De Turck: now see >> Christian Esteve Rothenberg: Bye. >> Filip De Turck: you soon. Thank you. 
>> Christian Esteve Rothenberg: Okay, so according. To the notification the streaming just started. >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: Let's wait a couple of minutes for students to join. Let's start. Thanks One more minute. We'll start. for some reason over YouTube your picture is not appearing your video Gianni just me. Okay? >> Gianni Antichi: Yeah, well, maybe because you are the host. I don't know. >> Christian Esteve Rothenberg: Yeah. Yeah, we are always This is streaming directly from meat. Let's >> Gianni Antichi: Yeah, and I never tried that. but >> Christian Esteve Rothenberg: yes look convenient, but it's a bit enough not having you and just oh wait. No you are appearing. >> Gianni Antichi: okay, maybe it's h >> Christian Esteve Rothenberg: nice, so but it one window >> Gianni Antichi: H okay >> Christian Esteve Rothenberg: after the other, okay, okay. Good. >> Gianni Antichi: That's nice. >> Christian Esteve Rothenberg: Okay, so we'll manage the interaction. >> Gianni Antichi: Yeah, no worries. >> Christian Esteve Rothenberg: Okay, so I think we can start. >> Gianni Antichi: Okay. >> Christian Esteve Rothenberg: Three minutes past one PM Brazil time. That's the and very glad to be here today the first seminar of this season this special season 100% remote is grad class in computer engineering at the faculty of computer and electrical engineering at the University of Campinas. I'm very happy to have Gianni Yani and Tiki. I'm not going to to read his bio for that you we put the information online and you can go to the personal Pages. Just say that I made Gianni long time ago. Well why we were doing our phds in some random conferences, I don't recall for some some reasons that were certainly worth. and and also very glad that we have been able to collaborate number of research efforts precise around the topics in networking that he will be presenting today. And and he will be presenting from somewhere because Gianni's jumping between UK and Italy at some point. I will try to understand how he manages and I learned that his nickname is the cloud because you never see him, but he's always available and that I can confirm. so Gianni with that introduction in formal as usual, I pass you the ball around 40 minutes. I will be back for 50 minute chat and >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: some interactions with our YouTube followers. We have 18 people plus the other students over 23, so they are probably half. And yeah, the florist is yours. So please sharing >> Gianni Antichi: Yep, let me start sharing right now. Here you go. Let me know if you can see. >> Christian Esteve Rothenberg: Yes, it's arriving. >> Gianni Antichi: Okay. Well, that's awesome. Um, okay then then he's I think here we can start. Okay. Yeah, first of all, thank you very much for the kind invitation. I'm super looking in our own Earth for me to be the first About that. Okay. so let me start with just jumping into the problem here, right? So the problem is basically we we can't longer expect the new general purpose processor to improve performance exponentially and this is what probably you have learned as a student to be like the mood low, right? So we are going to end to the more low. So basically, you know, the line speed of traffic is improve is increasing but the CPU count cope with that. And you know and I found this paper very interesting in this regards that this is a paper presented on and as the eye from Microsoft and basically if you see here like, you know, or you between the fact that the CPU count cope and networking stocks are becoming also increasingly complex and there are like a lot of features a running these tax on cpu-course takeaway processing powers from VMS. And this is a problem because increasing the cost of running cloud services, and I've been latency and variability to metal performance, and this is bad, right? So so that's why there is being a movement towards recently about trying to understand if we can find offload, right? So basically trying to reduce the the impact that we have on the application on the CPU cores and when we talk about offloads, basically, there are like a two type of low flows are really, you know, we can distinct and well, you know you want to do that for saving CPU cycles and the first type of category of a floods are really what will you call harder offloads right? So which means moving some of logic from the application down to Either a switch or or unique in terms of a programmable leak a programmable switch and this allows you to reduce the number of BCA Express transactions their the reduced the amount of allocated as Escape buffer, but in general like it reduced the pressure on the application and the CPU cores. This is one set and there has been like a loud really lots of work here that has been trying to see how you can leverage these new programmability to to make a better use of CPU cores. but this second set is really about instead instead of moving down here underneath or in the switch is really about moving down to the kernel because if you move to the current you you basically you have a less kernel user space transaction transaction. And then this means there you have a reviews amount of data returned by Cisco. And again you you are basically helping in savings if you Cycles and again in this context here, there is being really lots of work and Company really working on trying to see how you can programmatically change the current in a way to have a lighters for example a network stack and having like a better performance in your hand system. now take a step back. This is really really what what's the underlying your trade-offs and system right? So you might have a had a feeling already with that right? So in some sense every time we are talking about software, we have a lot of generality because you know, you're running on a CPU but the performance is definitely not great, Right. So but on the other side of the spectrum is, you know, you can have your features in hardware and the common sense is really hard there is generally less General, but it provides you performance. This is like a kind of a classic. type of trade-off that we have seen in systems and not surprisingly where I really wish to be and I think where also you wish to be is like a really be here, right? So basically find a system where we have high generality but also high performance. Now during this school during this talk. I will try to and do this lecture. We'll try to convince you that this is possible and with possible through to approaches moving the hardware towards better generality moving the software Tower better performance, and I'm gonna start with the first one and and the idea of having moving the harder towards generality means thinking about Primitives. and I'm gonna take as an example to explain what I mean with new Computing Primitives with a database application, right? So and let me provide a little bit of context here. Like I think about you are in a data center Network. This is a classic. Let's say figure of a data center networks. You have switches rocks with a with a servers. So usually when when you when you want to do a Telemetry system in a network, you have a basically switches that they report the data, so they send some data and then you have a collectors. They are specialized or software installed in dedicated server that they grasp the data and they understand what is going on in the network. Now here is the problem a data center can comprise of hundreds of thousands of switches. And at least I'm talking about a big scale Data Center and you know, there was an interesting paper. I see come a couple of years ago where Alibaba was showing that you can have it even few million Telemetry reports per second per switch. So you have lots of switches a lot of telemetry reports that the switch sends to those collector. so here is the problem. We we took like one of the state of the art and network called collector that there was a freely available and this is a paper and we try to understand how many really reports that that collector can inject, Right? So can ingesting can get of course. So if you need to reduce the if you get the more if you assign More chords or to these application you a better throughput and the reason why there is like a lot of a processing is because basically these collector has to First receive the report which is in a form of a pocket and this is ideal Riser then it has to parse the report understand understand. what type of report it is from where which is coming from and then it has to insert that these report. you know, that's a structure right? So in the memory in order that allows then afterwards to be queried it's clear that the more reports to process. The more cores are needed. And this is a problematic because has the the network size grows as you can see here on the x-axis the number of cores that you might need. It might be even more right? so and you can see like of course here you can see like a sort of a simulation where we try to understand specific reporting Be like the number of course that you will need when you increase are the of switches. Now we try to understand why this is happening. So and profile an application that into you before to try to understand we're really the time was a spent here. So we will there are like many tools if you are interested in these things. There are like many tools that really allows you to do that like the most the most common is a Linux perf that tries to get you where you run an application and you can basically understand from running curve where the CPU clock cycle spends in the cause of these application. So by running these basically what we figured out that you know, 13% of time was a spend in the io so, you know receiving the pocket. In a 13 of time was spanned in a parse in the report. So try to understand what's you know, which type of report it is from which which is coming from but really a lot of time and around the 7273 and of course depending on implementation of the stack you might have a slightly different, you know numbers, but basically really they take away is a lot of time really is spent into Inserting data in memory. So this that what we understand from those two figures here is a really that these these networks. These collector is a CPU bonded and we see that because we are the new course and we get a sort of a linear increase but also the most of the CPU Cycles are really spent now we want to get something better out of it what we can do. So first of all the takeaway that if we use a fancy IO system like having new I don't know Network stack that supports better. These application will not really help when we really help because anyway that that the most of the time or at least you might help a little bit but most of the time is really about inserting data into memory and this is a this is what an IO will not help. So then we can think about hey we can do something. What about if we try to have a some harder solution and that's where this type of start them from right so and our question was can switch insert queryable Telemetry data in collector's memory entirely by passing the CPU. So basically the question was if we have a switch that has to report data can this which access directly the memory of The Collector and just dump the data there without having to let's say send this data The Collector has to you know to parse the data and and understand where to put it. So it turns out that there is like a technique you might be aware or not, which is a pretty family pretty famous and used in HPC context and now even in data centers that allows you to basically write daytime memory by passing the CPU. This is what it is called RDMA, right? So RDMA stands for remote to them direct memory access. So basically the idea would be then huh if we can have a switch just do a RDMA call to an ink this is the collector, right? So this is a this is a server and here in the CPU. There is the logic of The Collector. But what if we have a switch that just trigger our demical on a Nick and this goes directly into the memory, then we can save the CPU cycles that we were saying before right? So this is great. Unfortunately, this is a little bit problematic and adjusts, you know do it because RDMA is limited to simple operations. So basically you can do just to read. Or write in a memory and some other type of operation. but also RDMA requires explicit addressing so as which has to know exactly in which location of memory they have to write and this makes sense, right? So I'm gonna try to show you with the thought example why these these limitation here they make it hard to do. These has we envisioned at the beginning. So, let's assume we have a you know a number of which three switches that they want to write in a data structure in a collector and this is like a sort of a ring buffer or an array, right? So oh. so and this is the case where for example you want to switch that needs to report some event data into a list in a collector. So, how do you do that while I should Rene coherence if you think about it that this is a classic problem of systems. so the first option you can think about is well, you can have the switch that they talked themself they coordinates and then they decide hey, I am gonna write here or hey or this one. Hey, I'm gonna write it here, but the problem is this impose too much over it and if you think about it, you don't want to wait right so Telemetry you want to have a switches that just send the data as much as they have because they will have a continuous amount of data. Well, we can think about the different option we can think about having a sort of a lock so you can basically the switch reading these acquiring a lock. Is there something we do like in multicore programming in a sense and then once the core the lock is acquired, you know this which can write this is all good. But again, this is delaying the process of what you really want to do is sending the data because which acquire a lock then after acquired the locker can write the message then I can you know can be zoned a lock and I think this is good. But in the meantime, there's Richard will have something else to report this is bad, right? So and also what about explicit addressing. how does which will know where to write you will have a problem to acquire the lock read the last address available? And then right and this is like kind of the laser process. This is bad. So then let's think about let's take a step back at this point, right? So what do we really wish to have so we wish to have something that is a lightweight protocol that that has a low hovering the switches because those witches needs to be used really for doing like a processing of pockets. not to yeah, they need to send Telemetry, but you know, this is not the main point. but we don't want to rely on The Collector CPU and this is what where we started right really, so we want to have a sort of er, DNA like protocol. But we want to have also coordination free updates. So what we want is we don't want the switches that they have to talk themselves to decide where to write. So we want just as which is to just push Telemetry report as soon as they have. And finally, we want to preserve the data organization and what I mean by that is that we want to directly organize us. So we want to push this data not in a random way, but we want to push this data in a way that then if I really did the data, I understand what's going on. In some sense. What we want is really memory layout aware RDM aware verbs so some sort of the RDMA verbs not just right and read the whole thing before but something that is aware on the type of data structure. that is that is in the collector. So that's what we come from and we decided to build after all this consideration and this is a work led by my students. I'm very proud of him and we present the last year a cecom and basically the idea is we do have And we do have a collector here. So a collector will be connected at some point to a switch there is gonna be a switch that is directly connected to our collector. and we are gonna have others which is that they will report the data to The Collector. Our idea is to build a protocol. We call it the direct Telemetry access where those witches they report to using our protocol then these these these data arrived to the last hop which that converts these into simple RDMA code. I'm gonna try to show you why this work and why this is a in our in our opinion and good solution, right? So here we have a bright only data structure. which makes sense. Right? So you just right the data that you want to report you have here the switches that report data using our custom protocol. Yeah what I call it before memory layout are dma verbs and then we do. Have these last Opus which that is in charge on translating standard are chemical and doing the explicit addressing that we were saying before. so intense is really about you know as we just send a report and the last stops, which says oh if you write it this way the report then this means that I need to write in memory into the collector these value at these address. Now, I'm gonna of course. We we have a limited amount of time and I will be happy to talk more but I'm gonna give you just an Insight an example on how this works just to give you a feeling what I mean by that with the more memory layout verbs and so forth, right? so and idea is let's have on the collector a sort of a key Value Store like a sort of a matrix where we store keys in value, right? So for all for each key there is associated the value the idea that we have is really to create a stateless mapping between memory and Telemetry key using hand hash function to Collision. Let me show you what I mean by that. Let's assume that there's a switch wants to report. you know a sort of a set of Statistics like a fluid and this is the number of pocket that I've seen for this specific flow ID. So the key in some sense is the flow ID in a sense. That is the classic five double and the value is accounted the number of pockets so that I have seen for this specific fluid. So this is the memory on The Collector which is organized like as a sort of metrics. So for the for the first you you have a flow ID one you have brought you you compute the three Ash function They will redirected to three different memory location on The Collector and you will write the count. So the value here here and here so you will trigger basically three RDMA calls a three different random location that they follow the ash Function One change. Another report will come over and we'll do the same will I have a different flow ID you and you will store here. The number of packets are the address that is Ash function of flow id2 and Ash function two or fluid too large function three of Liz read three and of course, you will write that other tree random location, of course as you can see from these you might end up to have these that was orange before now becomes green because you might have over written some data. The problem is like how to read this memory. Let's assume you have this data here. How how do we read this? So first of all, I need to know the flow idea. I want to query and the ashes that are used and this is good. This is okay. Right? So it's a reasonable type of things to request right? So because you feel like okay as an operator, I'm I want to know how many packets I have seen for this fluid so I know the flow ID, this is a system that I have in place in my network. So I know the hash is that they're being used so I can do that. the problem is really how can I know if these memory for example was not a very written right? So how do I know if there are like others with other flows that they collide with these memory in this case in this example that was over with written if you remember here there was orange and then after another flows arrived, you know, this becomes green. So what we did is that basically we add here the information not only the key see, right, so we are CRC a basically what you can do is to read here the information check if they CRC is correct. If it is not correct. This means that there was a collision so you can check another address current with the hash function if it is not then that's the value that you want. now the example of a primitive that we provide we call it the key, right and this gives you like also it helps you to implement the value stores, but we implemented different type of primitive. I'm not going really to to talk over those. But the point is the idea was we want want to make a set of Primitives that they help no matter the type of data that you want to report or the type of a monitoring system that you have on the switch You can make it compatible with our solution in a sensitives becomes a sort of a way for a serializing the information from a switch to the memory of The Collector. the creators out to be pretty We compare this wisdom that they basically use a CPU instead of just RDMA as we do and actually depending on the type of Primitives that you use depending on the type of data that you want to report you might have 4X or 14 One X Improvement. And of course this depends on many factor and again, I'm gonna I'm not gonna spend a lot of time on this because I want to just get the gift of a system right so But that's a point. It's also interesting that DTA. Our protocol is a lightweight we compare the results utilization of implementing on the switch. just a simple UDP, which is a certainly the most lightweight protocol that I can think of or implementing directing on this which are dma which is a pretty complex of DTA. It turns out to be instead the pretty light weight as a European, right so This is all good. Right? So if you think about it, and the question that you can think about is that do we really need to give up with software, right? So it's a can we do everything in Hardware. So and this tree gives me really to go on the other part of a talk is my argument is we don't need to we don't have to give up with software we can just build the better better compilers that will allow the movement of software. And I'm gonna try to make a case of for what I mean by that by taking an example of a pocket processing programs here. So if you think about how you run a generally software you have basically a programming framework or language then you compile it then you build you give it to a compiler and Optimizer and these builds likely let's say the set of table the data part of your program, right? So the idea that we had is the What if if I run time we provide some sort of information back to the compiler on the code Behavior a workload information. Then we produce around Time new shiny code. that is actually optimized for that specific code behavior and workload information. now I'm gonna give you like I'm gonna try to make a case on why this is useful, right? So, so first of all if you think about it, the network configuration is really unknown and compile time, right? So when you compile for example a software program you write your program that can support any type of things like Viola Villa and VXL and mpls, but that maybe after you run your switch your software switch you might decide to insert some rules that they have a villan or not because it is depends on the that's what configuration that is unknown you compile the software. we tried like what we did is that we took a load balancer from a meta which is a production layer grade the load balancer that that has open source code and we and what we did is okay. Let's assume we want to run a catron as a just an HTTP load balancer. So if if we analyze as an HTTP load balancer, basically we need to so it's about inserting rules that they are more layer for oriented, right? So so what if we if I remove any processing in the code that is already that is related to TCP because if I know that I'm running as HTTP load balancer it will receive only TCP drop Pockets. only TCP packets and not UD. Is actually turns out if just removing some branches that we knew they couldn't be used we could improve a 10% to the performance. so the system but simply because you help the CPU with the branch prediction and so forth. So the takeaway is really specializing networking code for slowly changing input like a network configuration improve the performance, but that's not really the end of it. Right? So let's really go beyond the little bit from code specialization for stable configuration. Let's see if we cannot musician at the packet level what I mean by that is think about UI processing program a packet enter you control you will check the pocket with the table and then you do an action if you think about it you are basically bounded the performance of your program by the fact that every packet that arise you need to check the table and then you need to do a memory access, right? So every packet one memory Acts now what if if for a specific flows that they are like a heavy flows and every time these I know that I'm gonna go do the same memory access instead of instead of doing these memory access. I will embed in into the code directly and say hey if this is a top flow and I know this is a top flow. I know already what is gonna be the answer. This is the code. This is what you need to do. It's a sort of a caching inside the code. So if you do that. Is actually this is of course in the presence of a skewed traffic because you need to have a heavy eaters it you know entries that they are hit a lot of time you get even a plus 23% of performance and if you consider plus the 12 before then it starts to be like something interesting. So the takeaway really here is the for maximum performance called must be specialized with respect the inbound traffic patterns. so the question when I when we realize that I might my first comment was This is not really new right. So if you have been here for longer you might have heard about what is called what they are cool profile guy that optimization a pogo type of tool that they use a results of profiling to optimize code, right? So then the question is, okay, so you are not really inventing anything. Can you just use those tool? So what we did is that we took a tool from Google how to say FDA yo and we took a code for a compiler from Matt a bolt that they basically they do this sort of things. Basically the idea what I do is they monitoring the the application are you works and then basically reoptimize which is exactly what I was saying. we apply that in the same similar circumstances that we tried before and actually we saw that the Improvement was not that great. So I was expecting to see plus 40% as I was seeing before, but I can't see that. so then that is really why because the problem is standard the Pogo tools are built for application but packet processing code needs a domain specific knowledge to reoptimize bucket processing code. You need to understand. What's the content in the into the the tables in what? what are the how many entries are heating? Which table while these are standard out of jio Bolt. They look more Akash misses. They look us more higher level type of properties that they know really helpful in the contest of optimizing code. So what we did is that hey, we should build a tool that does that. And basically the idea of the tool is you have some source code you produce llvm which is the intermediate representation. Then you go you analyze it you instrument to understand what's going on and then you you optimize it you get your optimized code that goes back in input the llvm and so forth and based on the results of the analysis. now this is gonna take too long to to explain everything to you and a time is running out. So I'm gonna just give you like an Insight on the most I think a juicy any interesting part, but if you are interested in the code, you can just just look it into the GitHub. Of course. so I'm gonna look into just the optimization, right so and we have this a lot of different optimization, but but I think I want to and some of them they depends on the input traffic some others, but I want to focus on just you right so I just want to give you a in hint of how this system work specifically I want to focus first on that coding limitation that removes branches that are not being used. So let's take us an example these code. This is a code that really taking from katron the load balancer from that right? So this is basically you think you can see it does a layer tree bucket headers the parts that you're for parsing then it does something right. So, let's assume that there are no quick services in configure in these systems. So in the in the table, so if there are no quick service is in our compiler realize our work our analysis the first we look at the table, we realize that there is no Is a quick services and then as you can see here, you will write you will override The Code by imposing 0 if it imposes zero here, then the classic compilers we realized that this is that this is a dead sort of a branch and then we kill the branch. which is great because then you basically you make the code lighter you make it easier for the CPU to understand what's going on for the branch prediction. And I'm going to show you another example related to one of the things that I was telling before are that we call it just in time compilation, which is basically inlining frequently hit the table entrance into the code. And this is I'm starting from a similar a code snippet where basically we do we we do a look up in a connection table, right? so here we do a look up in a table. and instrumentation basically instrumentation realize that there are like some flows that there are heavily heating to this table. So what is gonna do is basically instead of instead of doing the you know, just these that look up for every entry will do if the packet is is a topple then just go to this backhand. Otherwise go to this back and otherwise, yes, do they look up on the table? And this is like this is a right in the compiler we do and we run again the code. Does this work? Of course it works? Otherwise, probably you would not be here presenting it but but actually was pretty nicely right. So of course it is always depend on the amount of locality because you know, if you have a lot of editors you can play a lot with that and then you will get an improvement of 25 to 97% of the throughput if you have a less locality, then you will not have really a lot of improvement and those this is a trade off of the system we try this with different type of solution as which is a router the Catherine IP tables and so forth, but the takeaway is that this is like can improve really performance of system and why is that is basically because if you see here we try to understand what is happening that the micro architecture level. It is really about it to reduce. This is the percentage of the crease it reduce. basically the number of distraction it reduce the number of branches and as a result you reduce the number of Lord misses a layer last layer cache and this is why you get These improved so the lesson that I really learn and I'm going towards the end here. Is that the scanning application scaling application performance is possible right so we can do that. There are like we can have a better Hardware more generic Primitives for Hardware or better compilers. and now we have really we are in the right time. We have a fully programmable stack and a Common Language. We have a P4 for example that allows you to push something in the kernel something in the neck, but also in the switch. so the problem is really and this is more about more looking forward. here is a picking what functionality to offload desired and you know, because I again, let's think about we started from an application. We need to decide the front this application what we should put in the current in the nick or in the switch. And the problem is the programmer if you want to do that. The programming is to understand both application and offloading capabilities. So what the kernel and eakers, which can do that he needs to reason in some sense before implementing these about the improvements from a floating. implemented those separate components and you know if you think about ET can also means opportunities because you can reason and you can only test so many options right so And then you end up with the question which is the title of the talk really to offload or not to offload, right? So And that's why we need the smart compilers and compilers that basically given an application and doing some analysis a selection and generation can really understand where to push the right functionality and where either in the curler in the making the switch. We start going into this direction. We had a paper a lot less last year. We were mostly considering kernel application in kernel, but I think we can expose this to nickens which is as well. But then we need also layer seven. Let's say vertical Co design because in the moment that you start the pushing application in down into the Knick then you need to think about you handle transport protocols. how you handle this other things they usually the operating system does for you greatly. Um, since I promise Christian took me to make it like a really sharp in a 40, I'm gonna conclude by saying that to achieve that we need to have like a better integration between communities like system networking programming languages computer architecture, and I really want to have a special. Thanks for all the people. So I'm here presenting it because all those people contributed to this works. Otherwise, I wouldn't never be available to do that. I am grateful to my cats to a lot of cables and I'm super happy to take up question if you have thank you very much. >> Christian Esteve Rothenberg: Yeah, thank you Gianni. Yeah, the great overview congrats for the achievements. and and I have of course some questions that we don't know the answers. >> Gianni Antichi: Huh, of course. Otherwise, it would have been talking about those in the yes. >> Christian Esteve Rothenberg: writing another paper to get but I wanted to share with you on there and the audience and A few formations. Okay, so I will share. Here I was explaining Gianni how this seminar class work. Okay, so Lens, they did a leader to review based on the abstract that you provided for your seminar and here they identified. The papers and they need to reason why and of course I see your name here in in a couple of them. So the the reason of selecting some sort of I'm very satisfied that in a short exercise they were able to to identify. what are relevant pieces studies that they would bring to an island if they wanted to to learn more and we have over 30 30 32 responses. So some of you are late and And each of them they also elaborated at least one question. So I have here over 40 questions because some of them they're elaborated like three or four and some of them are really really good Unfortunately. We don't only have a little bit more than 15 minutes, but I will share this with you because there are some good ones. I identified that many of them going to the direction of the Moors law. >> Gianni Antichi: right >> Christian Esteve Rothenberg: So they because you you hated your dog. So the more law and the wall of the memory. is he so specialized Hardware comes to the rescue in this morning and I know it's not totally our field, but do you know something of Novel advances breakthroughs to bring this this more low in semiconductor or I decides or something. that is hitting. or >> Gianni Antichi: so I I know gonna straight away answer to your question, but I think I will answer to your question and actually and and I think one one things that excites me that I saw recently in an effort to bring more together. Let's say let's say accelerators and cores are the idea of the middle building better interconnect. And what what I mean by that thing building a connect I think is the realization that you really need the harder because otherwise you can't cope with a with a with the speeds of of the processing and since you need to have a harder then you know, the harder is there you need to have it how we can think the way we connect the CPU to the hardware, right? So and and that's like a couple of ways of that I can think of lately there has been like for example one way was a nano Pew type of approach. That was a paper. I was the I from a Stanford floor folks not only stand for folks, but most of them that the leading student was for Stanford, um showing how to redesign the meek interface for like two for sending data from the Nick directly to the to the CPU but also recently advancements like a cxl which I'm not sure you're aware of but basically Excel is a is a star stands for compute express link is a sort of a PCA Express, but the vision of cxl is really to have a sort of providing a coherency between the memory between ICS Alexa attached device in the knee and in the CPU and this is a way to I think in my mind the movement towards these like a sort of realization we need accelerator. The accelerator are here to stay and you know if they are there, but we also need the CPU. How can we re-archy text the way they do the the two of them? They collaborate in a better way. >> Christian Esteve Rothenberg: Yes, nice. >> Gianni Antichi: So I don't really answer your question, but I >> Christian Esteve Rothenberg: Yeah that the question is really hard. What is the future of more slow, right many fold directions? Yeah, I've interconnects. I've read something around Optical interconnects. And so the energy consumption aspect. that was another on my on my Questions. We are trying in our research group to try to account for the full energy consumption with uploading without uploading because we know at the end of the day and then of the month the >> Gianni Antichi: right >> Christian Esteve Rothenberg: bill comes and for many years. This bill can can pay off. Let me pluck, of course some on the world will be doing together. I'm presenting so in June in netsoft, we will be having this tutorial the smart Nicks the next leap in networking and me and Marcelo will be there. But we need to as you did recognize all the contributors. So we have slides being presented that we're contributed by giannii and also earlier in in May in our main Symposium of the Brasilia and of networking and distributed systems as BRC by the way, we need to invite you. We'll have also a short course that delivers not just the tutorial but also a forty forty five page and >> Gianni Antichi: one >> Christian Esteve Rothenberg: material and we are working on on these Francisco Marcelo and also designing the types of experimental Hands-On demos that we want. to also learn about so that's a plaque if you want to learn more about smartnix join us in sbrc or in netsov. But let's go back to the to the questions. Do you think the energy? And account with Martinique as a total system is is well understood so they performance extra performance and together with the overloading of potentially CPU cycles and reconstruction of the CPU down to the more efficient Hardware is well understood and based off or maybe >> Gianni Antichi: I I >> Christian Esteve Rothenberg: depends sometimes yes, sometimes not you have inside on that. >> Gianni Antichi: yeah, I I think he highly depends definitely like introducing new devices introduce more consumption in some sense, you know, but if you can make up with a with a performance that they can be like a sort of a trade-offs there, right? So but a trade-offs will depends a lot I think on the specific harder that you have and what I mean by that is you know, there are like You know gpus fpgas Asic. So asynch that they have as some form of programmability. you have a programmables, which is they all have a way different trade-offs, right? So and in terms of how you know the energy consumption in terms of how they behaving in terms of also how you can manage them from an Energy Efficiency point of view, right? So there has been like recently a lot of interesting work in the area of a trying for example to rethink a networking in the context of making let's say decision or routing decision to be more carbon aware There was an interesting paper Comics last year about that and you know, people are moving towards these >> Christian Esteve Rothenberg: Mm-hmm. Good good. Yeah here in the faculty that we do a lot of work on Energy Systems we should come up also with ways of measuring an understanding this this flows of Energies. and and so in the chat, we have 29 people and we have a couple of questions. So in addition to that more than 30 we have a couple very low level. >> Gianni Antichi: All right. Yeah. >> Christian Esteve Rothenberg: before I pick one and things for for the interactions, I want to to bring one of my own. So the the people language was really promising in potentially becoming a language for the networking Community, right? But then we saw that for different targets. some the compiler had to be redesigned the You used to see in in the sovere compilation the programming mean that they offloading choices by analyzing the software that in it will be in some language to you. See some That we will have another over the top language from their lower languages can be compiled like ebpa dpdk or even before I see before and and even lower or maybe before still assumptions is to become the de facto highest level language or should we stop with that? hope and get ready that there will be silos and ecosystems or I know that's another this one, but I had to >> Gianni Antichi: now that's that's an interesting question. Okay. so let's start from Let me remind you that I might my background is fpga design, right so for me before is even too high level, right so for being used to very long and so forth, but you know Jokes Aside like before was built like as a sort of an idea of a really really tighty coupled with the match action a type of a framework of r&t r&t, and he's in some sense a low level type of programming language. Raisson it is a software like but it's a pretty low level right? So and I think in an effort to move higher higher level probably like you know that this is I something that will help right. So one of the things why I am a little bit puzzled, and I don't know a good answer is for example. I understand that P4 is a is a is a he makes a total sense for programming accelerators. But when we when we think about endos networking you have let's say as you also mentioned a BPF, which is basically sort of a de facto way to program the Linux kernel and this is I'm still a little bit Unsure how those things that can be like a connected and nicely together in a way that that can be form like a career in the way to see from the application to their flows, right? So there has been a recent effort being pushing a P4 to delinos Kernel. There is a compiler called p4tc that allows you to express a P4 programs and attach them at the TC level of the of the Linus kernel. Um, and this is an interesting way to see like, how what can you do with before we did completely different? It with respect what he was born before that. It was a hardware, right? So now instead it's something like a more CPU Target, right? So and I'm curious to see how those things that can be connected together at some point. I am not sure to be honest, but I do see these as a valuable really research direction to think about the programming model. >> Christian Esteve Rothenberg: Yeah. Yeah, and I like your point this bringing together different communities the programming languages hardware and we see a really good pieces of work when you get the folks right together, and it's not easy because the language of which focus is different. We see with the machine learning when we try to bring a machine learning expert to our field of networking and it's not so easy, right? And he in the chat. We have questions by sunith who you also know. He asked about the the micro second scale the report stats to the collector in microsecum Scales. Could you give some examples or use cases where we are really facing this this problem is his question is micros the micro second scale for stats report. Is this the >> Gianni Antichi: um, so so let me see if I cannot plug back the presentation. I think it should be able to see it. Right. so I think the argument and going back here. Yeah, so the argument is really. million of telemetry reports per second the prez which is really about the type of queries, right? So is that you might have a lot of aquaries right? So you might have queries in relation to flow statistics queries in relation to path tracing flow queries in relation to Kio capacious and and so forth, right so and these all of them they might create a really lots of telemetry these at least this was what was claiming in the specific paper from Alibaba, right? So all I if we think about a single query and you ask me, is there a case of forever single query at microsecond scale? I think that there was an interesting work from for example Facebook and I am see 2017. We're basically we're advocating for having a microsecond counters in their And the reason was the following. they wear like using like a classic SNMP and so forth. They were seeing like a sort of a usage of the network that he was very low, but they were seeing like a packet drops and they couldn't understand why then they went they increased the resolution. They realize they were spikes in the traffic and with those spikes of the traffic. They basically wear microburst that they were creating for a short term like a queue like as high queue utilization and then packet drops. So in that specific case, I think that these can be an interesting more for a troubleshooting for example my micro or microburst the type of evidence. >> Christian Esteve Rothenberg: We have another from Rodrigo's. This is really low level and he is asking and the air dma with DTA approach similar to count means sketch what but what are the differences by adding this CRC? >> Gianni Antichi: um >> Christian Esteve Rothenberg: I honestly I'm not sure I fully understood. So the question hopefully you did. >> Gianni Antichi: So no the so okay. I think that this is the this is something that we didn't consider in a sense. I I agree with you that it might sound like in a sense account mean I think the difference is the way I do see that is if You would ever use account mean that you have a probabilistic approach, right? So you have a probabilistic approach in a sense that you take then the mean and then you get sort of an estimation right? So we wanted to have the sort of a similar, you know as much as possible the exact value and if we couldn't get the exact value then you know returning a failure for for the specific for this specific query and well and See give us is the innocence is a sort of a confidence if that entry was overwritten or not. If it is not of a written then is great. That's the value that we really looking forward to if it is was of a written it this means we need to go to calculate another hash and check on another member location. We're using one of the other hashes and of course if you all the three hashes in the case of the example the three ashes they return a fail then is a fail. But again, is that something that we show in the paper is a trade-off between how big you want to have the memory and then the harsh space and and the type of the amount of reports that you have. >> Christian Esteve Rothenberg: Okay, and now I got the question. So thank you >> Gianni Antichi: I mean, it was my interpretation of the question, right? >> Christian Esteve Rothenberg: I think it was I will take out with Rodrigo. He's around in people so on doing a CRC and hashes and doing it some interesting stuff within people. So last one. >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: and I like that one this comes from one of the class student reports. and how fast and often this issues whether to upload or not should we should be made? Okay this I understood it. You you what you propose this analyzing the application the code before it runs, but maybe around time something changes. >> Gianni Antichi: No. >> Christian Esteve Rothenberg: That's that's a reputation often and then the second one which I also like because we are in there machine learning here and AI good. And we apply machine learning to this problem or is it an Overkill and probably something more static and with guarantees? initially will be the approach and >> Gianni Antichi: yeah, so so talking about the the the decision that a compilation decision Okay, so, oh, I'm gonna answer this in two parts, right? So if we take these and if we take it just a pure software approach. Oh and we consider that the tool that was presenting. of course here you are bounded by the sort of your these this sort of a round trip time, right? So the time to go here and back here and and these this is a as a as a sort of effect on how good the analysis is they need to instrumentation and the optimization that you run and how much it takes to load the specific code. For example, a vpf code into the kernel. on now if your traffic patterns input traffic pattern do not change a lot. Then this is a good approach. Of course, if you have a traffic patterns that they change a lot then you might end up in a kind of a following like a sort of completely changing in circle without the gifting getting like a too much better benefits in performance. And again, this is a trade off of the system, right? So if we stand to talk about more generic on the should I your float something or not, which is what I was saying here. So the way that I was envisioning here is mostly something that is a should be in some sense a compile time before I running right? So I have my application I get the feeling on all the application I studying application and then automatically I decide what to offload currently because which are so not something I run time. He couldn't be like exciting doing our own time. To be honest with you. I Know if what would be like a really the mechanism why you want to do that in which case is can be convenient. I might Envision that maybe if you have a multiple application running then you might want to do these for some application at some fun time at some point. You want to have a kernel of Nick support for somebody then at some point the traffic pattern change and you might realize that you want to have kernel of mixed support for another application and this is where I can see the dinamia dynamicity of things. Um, I don't have I would be happy to talk more about that. I don't have a good understanding on how to do that around time because these requires reconfiguration of harder, which is at least for fpga is a kind of well done the student. There is a techniques that they are called partially configuration. For example for other type of Ariel. They are less understood so that that's something that you know, it could be like an interesting. Uh, let's say way to see the problem as a Next Step. >> Christian Esteve Rothenberg: and the Machine learning a part is if you have a data sets Pro having done this multiple times and I observed and then maybe you identified >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: patterns in the code that and then over time with data. The machine learning could be handed to to help you this decision. Like oh every time I saw this and this type of code flow structure or or whatever this work or that didn't >> Gianni Antichi: this is a this is interesting the we had some ideas in a relation to that. We didn't start because we couldn't find the really a person that was enthusiastic doing that. But that's something that we wanted to do at some point. >> Christian Esteve Rothenberg: or Gianni yes us check GPT putting the the code in the actors then. >> Gianni Antichi: There was they solve everything for you. >> Christian Esteve Rothenberg: yeah, everything that's how that's a lesson that we will take that it doesn't work like this. and I'm joking because I am playing with the judgity both in undergrad and ingrat even in ensuring the answers so that they are >> Gianni Antichi: Yeah. >> Christian Esteve Rothenberg: critically and they analyze and they will take them with very carefully. So Gianni it was great. So we are just in time other students just starting classes 2PM Brasil time. Thank you. >> Gianni Antichi: So much. Thank you very much for having. >> Christian Esteve Rothenberg: Enjoy the rest of if they forward to meeting you. >> Gianni Antichi: Yeah. Thank you. Bye. >> Christian Esteve Rothenberg: Thank you. See you. 
>> Christian Esteve Rothenberg: Okay, so good afternoon it after some technical challenges so much for technology, right? >> Katia Obraczka: exactly >> Christian Esteve Rothenberg: but we might hopefully so we are 10 minutes now past 1 pm Brazil time and very glad to my friend Katia obraczka directly from California, right I guess. >> Katia Obraczka: That's right. Yes. >> Christian Esteve Rothenberg: Yeah and Katia she will talk a little bit more about her biography as usual. I don't introduce Are speakers because otherwise it consumes too much time, but I let you to introduce yourself. So I will go I will pass the slide. so and you will. know okay when whenever you need me to. To go through okay. >> Katia Obraczka: Perfect. Thank you so much. Thanks welcome everyone very happy to be here and talking to you. So Christian and I, you know have known each other for a while now not nothing, but tell you how how long but yeah, and yeah, so um today I'm gonna talk to you a little bit about our research on iot for social good internet of things of course right for social good. Um, and I was told that I talk needs to be in English. So this is it Um, so I am a professor in the computer science and engineering department at UC Santa Cruz. I've been there since January 2001 so you can see how long I also have been recently appointed as the Citrus campus director for UC Santa Cruz Citrus stands for Center for information technology researching the interest of society which is and if you are curious you can take a look at that link. This is a multicampus Institute uniting UC Berkeley UC Davis. You see more said and you see Santa Cruz namely the campuses on in the northern part of, California. next please Christian um, so a little bit about me so before I joined UCSC in 2001, I was a research scientist at USC's information Sciences Institute isai. Maybe you've heard of it. It is considered one of the the birth and and then I was also a research faculty at USC's computer science department. And then before that I did my my PhD in computer science at USC then got a master's along the way and then before that I was at the federal University J as we call it. I did my masters in Computer Engineering and my BS in electro engineering at um So a little bit about Santa Cruz. some of you may have not heard of UC Santa Cruz before so just wanted to give you a little bit of context. Where is UC Santa Cruz as you can see it's on in the Cornea on the coast very beautiful location. And again, it's it's part of the northern, California a part of the UC system and the UC system is a system of campuses that have 10 campy spread in around California in the south in the center and also in the north and yeah, and and so you see Santa Cruz sometimes is not much known so next slide, please. um because it is relatively a Young campus. It was open in 1965 and we had only 650 students. So pretty pretty small. It is the one of the youngest campuses for of the UC system the you know, I think it's the third youngest, you know after Mercedes the youngest and then Riverside and then Santa Cruz just for you to have an idea of how much we've grown in the last few years in the fall quarter of 2019-2020. We had say almost 18,000 undergraduate students across 66 undergraduate majors and almost 2,000 graduate students again across 64 graduate programs. A lot of people don't know but we are actually the campus the UC campuses is the closest to the Silicon Valley So we have a very strategic geographical position and because of that proximity we we are able to collaborate a lot of with the local industry in the valley. And in fact, I live in the valley I don't live in Santa Cruz so, you know, it's it's easy to go back and forth. Next slide please. So these are some of the pictures from the campus. It's beautiful as you can see it overlooks the Monterey Bay you which you see here and here and then on a clear day you can actually see all the way across the bay and you can see Monterey on the south side of the bay and we are on the north side of the Monterey Bay and and you know, we're located at on a hill that overlooks the whole Bay. So it's really really very pretty in addition to that. You can see it's very interesting vegetation wise you have like, you know, the Open Fields as well as the famous red trees or redwood trees. I should say, right so in the central part of campus, there's a lot of the redwood trees like, you know, which the Sequoia are part of next slide, please. um You probably know that the universe is in the US have mascots right and the mascot for you for UC. Santa Cruz is the banana slug as you can see, so this little you know. Slug and because of its color right? It's yellow color and it got the name banana slug, right? So the reason why it's this mascot for you CSC, it's because it's a very common inhabitant of the Redwoods. So we see them especially on a rainy day. If you go out in the in the Redwoods you see them walking around. So next one. and I'm not sure if you've recognize this picture, but this is from a famous. movie and you can see that John Travolta is you is wearing the UC Santa Cruz t-shirt. I don't know. Did you reckon do you recognize this photo from which movie it is anyone? Maybe not. something that you've seen it's kind of a older movie exactly. It's Pulp Fiction. Oh next slide, please Christian. Yeah, so there you go. It's a 1994 Quentin 13 Tarantino movie and John Travolta as you can see there is wearing the Santa Cruz at t-shirt. Okay next. so now more seriously about my research. so I work in computer networks. My brother area of research the internet, of course and then more recently Wireless networking including multi-hop wireless networks wireless sensor networks disruption tolerant networks iot Edge networks Etc. And by the way, I forgot to say that if you have any questions, I know we have a Q&A at the end, but if you have any questions, especially Clarifying questions, please. Feel free to interrupt Okay, so my research lab is called the Internet working research group inrg and again if you're curious about the projects that we work on you can go to that link. We are of course, it's always you know, we're always revamping the web the website, but you know at this point you have a good idea of you know, the some of the projects that we've been working on and our projects span a wide range of topics that of course are related to network came, but some of them are actually more systems. applications of networking. Some of them are more networking specific and and looking at basic Network research like, you know protocols so but basically our is motivated by this. enough the future internet and as you can see I'm using the small eye internet because the big eye internet is the internet which is an example of an Internet and and by the way, internet the the term internet is the short form for internet work which you know by definition is a collection of networks that are interconnected together right and the internet the big eye internet is an example of an internet work, right? So please, you know, just I wanted to make that distinction because a lot of people don't so we in the lab we basically design build evaluate tests and deploy protocols and systems for the future internet. So you may be asking what are future internets or you know, some some of us call it the internet of everything now, right and and again, of course there are many many ways to Define that or explain what that is for for us. Christian please next slide so we can see that I mean again, you know, the many many different ways to define the the future internet, right and and of course you see the cloud, right? And then which kind of was the is the core or is located in the core of the internet, right? And then over time then you have things that are popping up next slide, please in at the edge right that in that connects to the cloud or interconnects through the cloud, right? And and that is kind of the you know, what we consider the future a future internet work or the internet of everything which is again, you know connecting all these different types of networks, smart environments like smart buildings smart industry autonomous vehic. holes Foams Smart Homes smart grids Oh, I thought there was something a question on the chat but but no So this this Edge Revolution as we call it right has been sparked by multiple factors, right? You know, one of them of course is the increasing availability of Wireless connectivity. that has become more ubiquitous and more pervasive and of course the increasing availability of computing power at and user devices our phones and laptops Etc have become more and more powerful and the price has been going down right? So you have this explosion of these end user devices and the same time explosion of these new applications that are running at the edge of the network. So just a quick historic perspective, right the you know, the you know, when did the Internet meet Wireless, right? So that's kind of what I'm referring here as I internet 1G to internet. 4G right. So and then you can see here in the x-axis time and here some sort of the evolution of the technology in terms of the generations, right? So here in 1970s, as you know was around the time that the internet was born and it was called the arpanet. Because it was actually funded by arpa. Arpa, you may know is the research arm of the Department of the US Department of Department of Defense. And of course, they fund a lot of research a lot of basic research and the internet actually started with funding from them way back when and then in around 1990s then you we started having this explosion of the internet they basically because of the applications like email and the web right? That's when the web was taking off in the early 1990s right and some of us app as the killer app that drove the internet to start them right to to fame. Then in 20 in the 2010s smartphones came to be right and then and just to have an idea in 2007 the iPhone introduced the mobile web application, right and then around the 2020s. That's when we started talking about the internet of things or iot or machine to machine, right? That's kind of you know, that that got kind of give you it gives you an idea of Shanda was happening. Next slide please. Um, and I don't want to I know we had we started late so I don't want to spend too much time here, but this is just to give you context on when the Internet of Things became a thing which you can see from here this graph here right that in around between 20 at 2003 and 2010. there was more there were more connected devices than people in that though were connected to the internet. Okay, so that's kind of when you know, people started thinking about you know, the internet of things and you can see here the the evolution of the world population and here the evolution of the number of devices that people were using Next yeah. And this is again another view of this Evolution right the internet of places the internet of people the internet of things and then in the future, it's this is what people are predicting is the internet of wearable things, right? So you you're you're gonna have more and more devices that you're going to be wearing like these VR glasses, right and and Etc So again back to that. Figure that. We saw in the beginning of the internet of everything right including the edge. And and the edge is where the action actually is, right? The cloud is kind of a little more ossified, right? It doesn't change very much but the edge is actually where where things have been changing in the recent past. Next slide please. And this is another way to look at it, right? you know the cloud right and like a pyramid the cloud right? You know, the the second tier let's say where you have these devices that connect the edge to the cloud right? And then you have the edge. Okay. So this these are access networks, right radio access networks or your home your campus Network, etc, etc. Right and this is your the end user devices that can connect directly to the this layer or can be connected to other devices for me what we call an ad hoc network. challenges of course many many challenges because of scale right as you can see here. The prediction is that by 2030 will have between 50 to a hundred billion Internet connected devices. This is a lot right? It's a number that you know, we can we can even can even imagine right not just the scale but the heterogeneity of these devices like and and not just devices but the networks and of course these services and the applications that run on them and of course the autonomy and administrative decentralization that these networks are going to bring about because I mean, you don't want anyone to be administering your home network, right or your building Network or your smart grid Network Etc. And of course the challenges also bring opportunities, you know like scale means redundancy heterogeneity diversity autonomy and administrative administrative decentralization resiliency and and the ability and robustness, right? Okay. so I'm rushing through because I want to get to the actual projects. Where is the action again? The action is in the edge or at the edge right? and and that's kind of where these projects that I'm gonna talk to you about are happening. yeah, so we can skip those Christian and and yeah, I I so just be the one before I don't know. Wait, yeah. I don't know if you're familiar with the term Edge Computing. So I just wanted to intro kind of introduce it right, which is a paradigm introduced by Satya in 2017. where which says that substantial Computing storage resources are placed at the internet's Edge in close proximity to mobile devices or sensors, right? right? So again a lot of these a lot of these paradigms are happening because more and more. These are not just the devices themselves, but the applications that run on them need very strict quality of service requirements, right? So so you need to be to have the control of the network be close to where the action is. Yeah. So again, this is you know, why proximity matters so I'm gonna skip through that all and and that's another another Paradigm that that was sort of the term was coined recently the term agent intelligence, which is the convergence of edge Computing and AI. So, okay great. So now we can talk about the projects and then still have time for a Q&A at the end. So the first project that I wanted to talk to you about is the an Internet of Things. system that we are developing for climate resilience as you know, like in general but more specifically for environmental monitoring in particular for Wildfire assess a risk assessment. This is Joint work with a lot of people so they're all here. And in partnership with the with our campus fire department and and also Cal Fire which is the the department in California that takes care of our forests. And of course, you know is responsible for Wildfire monitoring. um I'm pretty sure that you already have heard of wildfires. A lot of people are surprised to know or to find out that they are actually unnatural phenomena. They occur naturally nature. The problem is that you know because of everything we know about climate change and and what's happening with the environment. They have become more frequent more intense more devastating and and unfortunately more tragic. and the the problem is that this is going to be this is likely to be an ongoing phenomenon, especially as us humans. encroached into these, you know areas, which are called wild or but interface or we as we know them. Um, so um, these these are pictures from the some recent fires in California. I'm sure you've heard of them. This was the 20 Oh, yeah this it's it's really hard for me to even look at this but this was 2020 a 2020 wildfire that burned these you know this number of Acres, which is again unprecedented and cost over two billion dollars in in damage and and suppression efforts. You can see from the the skies, right the the burned area that you know what this Wildfire cause this was actually a fire that happened during covid and it actually came one mile north of our campus and and it was horrible luckily because of covid the campus didn't have a lot of people a lot of because it was, you know people were actually working from home and and attending classes from home. So it was not two problematic, but it was very very scary. and and the campus had to be evacuated some parts of Santa Cruz had to be evacuated as well. So next slide, please. So talking to a fire managers and First Responders, you know what they kept telling us. is that early detection is critical right? Because it enables timely response enables them to allocate resources appropriately and enough time for planning and preparedness the problem next slide, please so why is it hard to predict these wildfires in a white in a timely manner the problem? is that what we have now doesn't have the necessary resolution the necessary granularity to alert First Responders when conditions are becoming more conducive to wildfires, right? So this is a just a picture of an automated weather station that is located in the state. But because these were I mean this is actually not too bad. It's pretty you know, Compact and light in comparison to more traditional weather stations, but still they're not easy to deploy and they're you know fairly expensive right? So it's not easy to dep. them in the quantities that we need in order to provide the adequate granularity that we need in order to be able to detect these wildfires in a timely manner next slide, please. so we so what we decided to do is to try to design a system an iot system. That will be low cost. um also compact with the necessary sensors that we can deploy in these remote regions. cheaply so that you can we can provide the necessary granularity for the risk assessment the Wildfire risk assessment models to be able to ingest that information and be able to alert First Responders when they need to pay attention to a specific region and and maybe start either deploying people there or throwing water or or retardant in that region in order to prevent a wildfire from starting. This week this this system. Oh, sorry Christian just a little bit just one slide back. Yeah. So again, this is challenging from an engineering point of view because given that the system needs to operate autonomously in these remote regions. We cannot guarantee that you're gonna be able to maintain them by a human right? So they must be operating what we call off. grid right off the power grid and off the communication grid and they need to be deployed in us in a scale that if for example a wildfire would would happen and some of them would be burned by the Wildfire or eaten by an animal, right? There is enough redundancy in the system that that doesn't matter right and they are low cost enough that they are pretty much negative negligible, right? So that that were those were one of our challenges in building a system like this next slide, please. So on that's Eureka is the name of our system. It's a low-cost scalable robust of the grid energy efficient system that can operate autonomously and continuously to monitor large outdoor areas. these the the current system. Actually you can Actually you can equip it. to do different types of monitoring for wildfires. We equip them with the sensors that you know, the fire managers and the fire on the fire emergency crews told us that they need for example temperature humidity soil moisture wind are the essential ingredients to a system like this. next so this is just a like. sort of a functional view of the system. you have the iot here that is deployed in the field right then you have it. It feeds the risk assessment model that is running in the cloud. Right? And then there is a decision support tool that also runs in the cloud. But also, of course he has a user interface to the man emergence the fire managers to give them information about what is the risk of wildfires in different the different regions at the system is deployed, right? It's an end-to-end pipeline that connects the ground iot sensor Network the Wildfire risk assessment tool and the decision support tool again. This is running at the edge. This could be running at the cloud and at the edge and the same thing here and then you can see the this feedback loop that the the actual the decision support tool can actually actuate in the iot in order to try to make the iot work the way it needs to work. For example, if you need the the sensors to sense more frequently or less frequently. Maybe you want to sense in a region rather than in another region. So you are able to to also actuate and also have a sense of how the system is working like, you know having Diagnostics about the Tea as well. I'm we're running out of time, and I do want to talk about the other systems at least a little bit Christian. So maybe fast forward here oh, yeah, so this is you know, yeah, so No 40. Yeah that one 43. So this is um, just a quick view of the current prototype as you can see there is a there is a solar panel and then in an enclosure you have we have the actual node with with its sensors right some of the sensors need to be outside of course, right and like the wind sensor and the temperature sensor. So again, they have to be weatherproof which which is another another challenge of deploying a system like that. next slide Okay, I think I would go forward. Oh, yeah, so one one back Christian one more. Yeah, just to show you this is where we are going to deploy the system initially. This is a an area on the UCSC campus as you can see. it's pretty you know, it's very um, the vegetation is pretty dense, right? So these are the Redwoods that I was talking to you about. So there is a natural reserve inside the campus and that's where we're deploy the system initially. and then test it over there with the help of the our fire department on campus. And and this is our hypothesis. Oh, sorry This is our hypothesis. Right? our hypothesis. Is that with finer grain environmental data, we can improve the accuracy of wildfire risk assessment and that's kind of what we need to test, right? Okay next. This is just a picture of you know the test bed in the lab as you can see we have three nodes and we're testing them in the lab right now in terms of Park consumption and also reliability the ability to send data reliably between these notes, of course, you know, this is easy, right because it's in the lab, but you know, we are also testing them outside. Okay, great. So, this is a what another one of our iot projects? This is an iot for healthcare and this is a again joint work. So the cool thing about these projects is that I get to work with all these amazing scientists in different disciplines, right? So for example on in this project I'm able to collaborate with a medical doctor you see San Francisco also research nurse at Davis so it is it is super interesting in that sense. And of course my colleagues a colleague Colleen Joseph Sanai unzao at UCSC with who are you know working in different in different parts of the system. The name of the system is purple purple for a stands for pressure pressure ulcer and prevention platform. I'm not sure if you've heard of pressure ulcers before. anyone I don't want to be redundant. No. Okay. Great. Thank you for the feedback. So pressure ulcers are also known as bad source. And they these are ulcers that form on the skin because of low profusion meaning low blood flow to that area of the skin of the body and this is usually because there is a lot of pressure like physical pressure on that part of the body caused by normally immobility so patients that are immobile for very very long periods of time and they have the tendency to form these ulcers these cars in in the in the on the skin. These are very problematic because not only they are very hard to treat and See when they appear. It's already too late to prevent them because they form from the inside out and also they may cause more other problems like, you know infections and Etc. So and and so this is a big problem for Hospitals and Clinics and long long term for care facilities that treat sick very sick people that are not able to move right? And this is just a like a quick fact that you know, it affects 2.5 million individuals in the US and costs the Healthcare System 11 billion dollars annually, so it's a big problem for hospitals because they are liable when these ulcers appear in their patients right currently on the interventions and and the techniques for preventing. These ulcers are very very antiquated. And don't really work very well. In addition. They are very labor-intensive and and because of that that they don't they're not performed consistently and and are not based on science Christian. I see you there. It's because of time right? Yeah, well, so let's let's run over these slides quickly and again, I has access to them so if you have any questions, and then you've also have the paper some of the papers that talk about them. So this is again a quick diagram that illustrates how the system works. This is one part of the system. This is actually the softer platform that will enable censor data from being acquired from the patient, right stored. And then you have the purple predict module which where the analytics run that will analyze that data and then you have a visualization module that will let the clinicians of healthcare team visualize the information raw sensor data as well as the analyze data. This is a picture of our current prototype and you see that it is connected to a bandage that was developed at UCSF. Where you have like built-in little sensor prep sensor pressure sensors that are embedded into this bandage, right? And then, you know, we are connected it to Pi map, which is the that platform that I described to you part of purple. Yeah, yeah, we can go to the next one and and here are the different ways to deploy the system right? You can deploy it, you know completely in the cloud. You can deploy it partly in the cloud and partly close to the patient. So the way the system is designed allows allows the system to be deployed that you know in different ways depending on the health care and resources. Next and then this is the last one and again quickly. This is again iot for resilient food production. So climate right as a whole and we call it Greener Greenhouse. So this is a project again with some colleagues of mine. You see a c and also you see more said and involves of course faculty and students and basically the idea is to design the Next Generation Greenhouse. that is energy efficient and resource efficient in general. So let's just move quickly. Yeah, we can we can go quickly here. Oh and then this is the paper that where it's like a white paper at this point. We are working on a more technical paper now, but this is a white paper that describes the motivation the system Etc. Again, this is online you can take a look at that, but A cool thing about the system. Is that instead of RF radio frequency. We are using visible light communication. So we are actually reusing the LEDs that are used to illuminate the plants in order to do two more functions on power the solar panels that are powering the iot tags as well as provide the downlink the communication down link to communicate with the iot tags. So so that is kind of the Innovation about this system. This is showing you a greenhouse on on campus actually another part of Campus that is close to the coast. This is these are greenhouses that we have access to and then next slide, please. So this is the greenhouse inside right? We have the the control part of the greenhouse and then you have the the traditional lights that we are installing new led a new LED light system there to be able to test our virtual visual like communication System next slide. oh, this is again a we are in order to prove our hypothesis right that you know, we're we're doing better than a traditional iot. So we have a we built on a traditional iot tag using Laura and this is the VLC base iot and you can see here on this side of the of the room the LED A system that we we acquired that we are going to use in the greenhouse. Okay the end sorry about rushing and going through the slides very quickly. But again, this is my contact information. This is the pointer to my lab if you want to see more of about these projects or other projects that we've been doing. you're more than welcome to do that and and let me know if you have any questions. >> Christian Esteve Rothenberg: Thank you Katia. Thank you. I am sure there will be many questions where one that I will throw to you is how do you manage to to put together this multidisciplinary teams? Because I I can't see Engineers scientists very diverse backgrounds And yes, how do you manage? how who drives these? Is there a funding is the other incentives? Let us know. How do you do this? >> Katia Obraczka: Yeah, I mean again, I I love these into interdisciplinary projects. I mean as Engineers, right? We are problem solvers, right? So in order to be able to solve real problems, we have to be able to contact the real people that are trying to you know that work with these problems on a daily basis. So, um one of the cool things about UC Santa Cruz I think is the because the the campus is not too big. And and also not too small. so we are able to connect with other with our Because again of these again there are funding opportunities that come up that actually require interdisciplinary work. the help the iot for healthcare one was actually brought To Us by the the doctors themselves, right? They they came to us and said, you know, we have this big problem. We cannot solve there's no current technology that help us. Could you help us and and come up with a system to try to solve this for us, right? So we're lucky that you know, because the UC system is so diverse, right and and but at the same time connected in we are able to collaborate not only inside you see Santa Cruz but across the campuses that Institute that I mentioned to you that now I'm I'm gonna director for it you see Santa Cruz also, incentivizes these into disciplinary collaborations through for example seed funding, right? So that is also something that really helps. >> Christian Esteve Rothenberg: Nice. Yeah talking about campus. You can see the picture over here is the campus of unicam and there is a a big projects called have for sustainability. So they want to build a district for living Labs. So it resonates a lot with what you presented. So building a putting bringing together real programs interdisciplinary research in in the world. Okay. >> Katia Obraczka: know >> Christian Esteve Rothenberg: so Katia, I'm sorry, but it's already our students have a duties at 2PM these classes right in the in the lunch time and I send you my best regards here from I'm right now at the foundation of Unicom. >> Katia Obraczka: wow. >> Christian Esteve Rothenberg: I'm sitting with Matt Daniels here. It says hello. We are having a meeting of our first year of This smartness research engineering Research Center that I look forward to to you more. >> Katia Obraczka: Of course. >> Christian Esteve Rothenberg: Okay, and let's see if we can collaborate again. >> Katia Obraczka: Yeah, absolutely. I'll be happy to Christian and and you know, we are still we are all right still working with. Ramon, right? >> Christian Esteve Rothenberg: Yeah, >> Katia Obraczka: Yeah, >> Christian Esteve Rothenberg: yes and also within rear in >> Katia Obraczka: Yeah, Ramon and with India. Yeah, and then actually I'm I'm actually going there probably mid-may. >> Christian Esteve Rothenberg: nice >> Katia Obraczka: Yeah, and and yeah, so, but hopefully I'll see you soon in you know 3D. >> Christian Esteve Rothenberg: in real in real not your reality. >> Katia Obraczka: Not not not VR real >> Christian Esteve Rothenberg: Okay. >> Katia Obraczka: not VR. Yeah, um and >> Christian Esteve Rothenberg: So students are asking. How do you how do they go can get into this team? So maybe just dropping emails and and sending any interest I guess if they want someone wants to know more or potentially engaged reach by email, right? >> Katia Obraczka: Yeah, yeah, I don't know. I mean, I don't know if the brasilian government has been sponsoring these. Exchanges that they used to in the past right and like mattels, you know when he came here, right? You know that was sponsored by the Brazilian government, you know, there used to be you know, the science Without Borders right program. I also had a few undergrads that came >> Christian Esteve Rothenberg: explanation >> Katia Obraczka: so but but now we can also do things remotely. So yeah, I think that the best ways to maybe look at these projects more carefully and you know, maybe do a class project right? I don't know if you if I probably you >> Christian Esteve Rothenberg: nice >> Katia Obraczka: probably require Capstone project for >> Christian Esteve Rothenberg: Yes, we do. >> Katia Obraczka: Undergraduates or a master's right? So that's another way to you know engage. >> Christian Esteve Rothenberg: Okay, great, then excellent. Thank you very much. >> Katia Obraczka: Sorry for the the technology glitches. >> Christian Esteve Rothenberg: That happens. >> Katia Obraczka: Yeah. >> Christian Esteve Rothenberg: Okay. Take care. >> Katia Obraczka: Take care. >> Christian Esteve Rothenberg: Thank you. >> Katia Obraczka: You too. 
>> Christian Esteve Rothenberg: great another Thursday seminar life here with roots of Sofia long time that we know each other from different types of interactions. I've visited her in Portugal in the labs. Now. I know she's Germany and we hormone collaborators soon working in the lab and we'll see each other also in US CNC in Belgium next month Looking forward to catch up. And talking about catching up a root is bringing today a very interesting seminar and that merges a couple of Hot Topics right Internet of Things Edge Cloud Hear a lot about this. What is this continuing of each and cloud? and of course Energy Efficiency and resiliency, so relevant and of any networked system. As usual route. I am very informal so I'm not going to read your extensive bios our students and and Watchers can read and also follow your links your Publications for you connect with you over LinkedIn. And so with that I will pass you. So that you can present usually around 40 minutes and then we will chat and I will pick some. questions from from the class and also from our life. participants over over YouTube Okay. >> Rute Sofia: Okay. Thank you. >> Christian Esteve Rothenberg: So, thank you. >> Rute Sofia: So hi everybody and Christian. thank you again for the invitation. It is really also a pleasure to speak again with you and I'm really happy that we are going to to meet each other again in or have this chance in network because after covid as we all know. A lot a lot Rights happen. So as as Christian explains and let me see if I can move my Going to today's speak about this so-called. that's to glad orchestration. But before that I have a quick I hope introduction. So Christian didn't introduced me. I would like more to introduce what what I'm doing in Germany right now, So you see here my background and I'm not going over that what is important? I think for the seminar is for you to know that my background technical background is on packet based networking. So in the past I've worked on scenes like quality of service IPv6, but I've also worked on Mac layer design on high-speed networking and then also on wireless and since 2019 I moved to Institute in Germany called forties. We are in Munich. So in the southern part of Germany to develop a new team focused on Industrial iot so technically where we focus is on next Generation industrial iot applications, and our research is focused on the development of networking architectures and protocols because that's where my background and I think my best contribution can be and also on computational architectures So in a sense this is a mixed team and in this so intelliciplinary team, it means that my team has as researchers people that come with a background from networking like me. It's also people that have a background more focus in iot for instance with some knowledge on semantic Technologies and Distance in AI not on developing new AI models, but actually on in particular forage Computing and so the Institute where I am sort is you have here information. Okay. So is the software oriented Institute? So this means software engineering oriented Institute. We have a number of teams. So we call those teams competence fields and each team. Let's say with the teams are aggregated into three clusters one focus is on software engineering the other focus on Industrial or intelligent infrastructures and the third focus on robust Ai, and so this means that I said, there's also these interdisciplinary notion among me and Michael Leeds and let me give you also a quick view on how we what we do and how we perceive iot because that is important to understand our Notions of edge Cloud continue, right? So everybody speaking about these so that so in the industrial iot team Mauricio. Is that in order to enable Next Generation iot and these next generation iot will is comprising that will comprise mobile devices. So heavily will compress mobile devices, but also as we know very very small embedded devices, but it also involves people and it involves AI so cyber physical objects with cognitive capability and in order to so imagine all these devices and cyber physical systems interconnected and also people and interconnected by a system of networks, and that system needs to be a bit more clever than what we have today on the internet and so the vision of my team is that we think we should rethink and redesign to some point. of course some of the infrastructure that we have for iot and that redesign is based on considering a joint design. working and computation All the way we work is that we do research of course similar to what possibly you do with Christian's group and then our outcome is normally vital scientific Publications, but also and I think this is relevant open source software. So there's a link here. Okay that you can check with some of our most relevant open source software. Normally we have some demonstrators for the edge. So that's where we are for. now really going into the seminar So today what I bring you is some information about what is this Edge to Cloud Continuum, and then I'm going to bring an example based on a European project called quote echo on how we perceive these orchestration. What does this mean? What are we using in terms of Technologies? And then I have created also a few other parts on the seminar. Okay. So one relates to give you some information about use cases, so where do we need this? Where are these environments that are a bit more variable than the ones we have today on the internet and I bring also some information about how you can experiment with this project. So maybe this is interesting because we are developing a bunch of opens our software with a number of companies and academic Partners in Europe, and maybe this is interesting also for you as students. So but let's start by the beginning. So in order to explain to you On edge 12 Continuum we need to go back and to think about Internet of things that also to think about that now we call it internet things, but in fact, it's the internet right So the internet of things it's basically the internet since it's beginning. so since darpanet, so it's just that it has evolved and evolved and evolved. It is a complex system, but now it interconnects a lot more things most of the traffic is on the edge on on the fringes of the network. So the internet as it was designed there's a principle on which it is based called the end-to-end principle, which basically says that the intelligence is on the edge. So the intelligence is really on the system. There's nothing intelligent in the middle. We just have a bunch of Dentistry supporters. And that is the network. That's what normally what we call infrastructure or the network. But the issue is that with these new interconnections that we have now to the internet. Okay. so with our smartphones or with sensors also with more clever sensors that can already think and react. Also with satellites the notion events. What is The Edge on the internet is not anymore on the fringes. We cannot stay anymore that we really have applications on on the fringes and or on the extremes that they have the internet. And so this means that there is now the needs and a lot of people across the world. Are thinking about how to adapt to the needs that we have in your applications. So a fact is that isn't when principle is any way changing because we have also other things on the network we have programmable devices we have new ways to do to do computations or not necessarily just on the extremes of the network, but actually in routers already we can do caching in routers. And so these all changes the way we possess information but also that we compute the applications. And so where are we to the in terms of Internet of things so that's sort of how the internet is evolving. So you have here to figures that show a little bit the confusion that we have today on the internet. So on the left hand side you see that we think about the internet as this complex system. So we all know of course that there are differences in popularity. And we still have let's say areas that are so rural very rural that are very isolated with lack of Internet access we have some like a telecommunications still so not necessarily on rural areas. So I'm here in Munich, but if I go to a lake 80 kilometers away close it with the Alps then my my fourth reactsis disappears. So this is basically in urban areas. We know we have been support and outside. It's still a problem and we have something new which is spontaneous communication. So today on the internet. We also have the possibility to create infrastructures on the fly to with a specific purpose for instance emergency communications for health or after a natural disaster. So today these are functions that that we have and that we can we can the internet. So on the right hand side, that's the example that you have. so very stable televised infrastructures and then mobile. okay, integrating satellites uavs and so on but also spontaneous Communications and the mobile devices that we have today, they are also changing so they can do storage so our smartphones they can do this as you know, they can do Computing they can reason up to some point so we can have reasons there. And we have a bunch of different networking technologies that are supported. So this is becoming increasing complex. And so the idea of explaining this is to tell you it's not manageable anymore. Just based on human configuration. So it's not actually just a matter of for instance ensuring that all are at this points are well placed Etc or that our node Vis or our ran areas are well configured in fact the internet as it is evolving requires some facilitation have intelligence. that's it can easily adapt to new situations. So that is let's say the perspective of the internet right and then on the Internet of Things We have of course the networking part, but we have the systems. We have the systems that process data and that gives us level results about data. so Based on data sources and so on this slide what I bring you is Of for a principles or aspects that any iot system has and the issue is that today we have over 600 proprietary iot systems. So it is very hard to handle the interconnection of thousands or hundreds of sensors already for instance in manufacturing in factories, or even processing neural analytics, which is another example of industrial iot applying has already hundreds of sensors. And normally they would be connected via Ethernet of is the mother fixed form and that is being changed and and people are using more and more Wireless and cellular Network Technologies. So that's that's just the parts. That's what you see here represented as connectivity. What we also have on iot system are the interfaces to the user. So the human machine interfaces and we have the data sources, which can be our sensors that can also be people and can also be already as you know objects or cyber physical systems are really just softwareized agents such as an AI agent. So about a chatbot can be a data source or iot. And then of course, we have parts or systems or applications that process the data because that's the important part in iot and that store that data in a way. That is secure. We hope that it users privacy and also one thing that is becoming more more important is to ensure super energy. which means that the data can only be used by let's say their own shareholders or stakeholders and the data is let's say adequately certified or regulate within a specific region worldwide. So that's all is iot. Also, so when we think we speak about internet things except the thing itself is a semantic obstruction, so it's not an object. It's actually a description that we do today. That's describes the object. So the thing about is let's say descriptions. So embass is the description of an object. Okay, but it's also it can also embody the description of human behavior. For instance. I'm going outside and jogging and therefore I run at some speed okay, or I stop now. I am stop so that information can be seen as being sent from a little source and it is relevant for some applications. So this is internet of things but we spoke about Edge to powder extraction. So we are speaking about the internet the transport part the edges. So what the what is this Edge to plot Continuum and why are people calling it? That's the cloud continue. We're not just internet. So when we speak about the internet we focus more really on the infrastructure aspects. So on the transport of information and when we speak about urgent Cloud we focus on the data processing and on the data start storage. So on the Improvement of processes on the improvements of the performance of applications that we are using in some specific use cases or domains processing health or in manufacturing and so today. Okay, so there are different of course different terminologies for Edge. You will hear about depech farage and microwaves Nano ads. Okay. So today what I bring you is sort of a perspective. Okay, there are others that the perspective that we are pursuing heavily in Europe. Okay on the deepest is let's say what is far Edge and here and I'll explain also why we need to have that. difference okay, but the edge to plot Continuum is basically a representation. okay of all the computation and networking management and networking control that we need to do to ensure that the data is adequately processed and that we have the performance that we need for some application from match to Cloud. And wife rematch the plot and where is The Edge? So the edge is closer to the end user it was from Edge to Cloud because today most of the applications as you will see in some examples that I bring they are not anymore stored or computed just on the cloud and the reason for that maybe may have to do it costs but has also it's sort of a process has also to do with the technological Evolution that we have today where virtualization has gained the new space on internet and on the development of applications and where new application architecture so applications that are more modular. not not anymore. Monoblocking start that we merge and also finally Hardware that we have and the capability to have let's say more open Hardware more programmable Hardware So the representation that you have here is a potential representation okay of what is far engineer Edge and basically you see here the human and iot interfaces close to the real world. So close to the people. We are data sources other data sources are cars or other vehicles. So the so-called Vehicles where everything cameras them sensors like temperature sensors and other types of sensors. The forests today is connected to all the sensors that we as in users use close or in the real world, so they are not normally controlled by an operator because the operator does not have the rich to control them, but they are extremely important to the internet because that's where the data sources are. Okay, so to give an example, imagine your home. Okay, no notation. And then you have your network Terminator Okay, and you're access point collocated with a local router and then you have a bunch of smarts things in your home measuring a lot of things Alexa and controlling the light. So all that is the farage. So your operator, which is the network Terminator that he places close to your door, but beyond that there's no control and in fact all the data is there so We call that the far inch. In a factory we call the far Edge for instance involves all the equipment that brings data to the system and normally relies what with what we call a shop floor or could be a warehouse again. There is no control of the operator there. So in this design, that's where you see the far Edge. okay, simplified design and the mirror edge is actually the area that you operator controls. So a very known model. okay that you possibly know of for A near Edge is Etsy Mac. Okay. And so this means that what we do today and let me show you another picture so that I can give another example you have here a similar representation just a little bit more detail on the left hand side. You have the far Edge. So here imagine. This is for industrial iot. Okay, so imagine a factory and then there's a shop floor where we have conveyor belts robotic arms Etc and maybe we also have other locations like Warehouse where we have automated guided vehicles and all these so we call these the far Edge So normally this areas they are Private so in the sense that data can flow to the cloud but normally does not enter. Okay, so this is not because just of a firewall, in fact, there are protocols that are used such as opcua that don't allow this without specific configuration. So in manufacturing the the manufacturers don't want a redirectional communication as early. Okay, so and this is an example where Edge is important. So if all the diet can be processed and control and start that the edge that's something that is important for manufacturing and it's happening actually ready to die on this example. What I bring you is the blue box is they represent parts of applications. Okay. So for instance if I have an application that is monitoring the temperature in rooms then that application as you know, normally is possibly no is containerized. Okay, so that's what we do today. We we have maybe learner a Docker container to support the protocol that does the communication. Okay. We see way and PTT. We have a container that can give basic analytics. Maybe we have a container that handles with tensorflow some aggregation of data something like that and the deployments of those containers today is orchestrated by Liz's system. Okay. That The notes where we can place or containers that belong to an application and as a decision on where to deploy that those containers based on the need for CPU and memory. That kubernetes is not care about the network. Okay, so where that's is not know if it's deploying so to elements that need to be close together and have a minimum latency of 10 milliseconds to run together. Okay, for instance, so but basically what I'm also showing you is that this microservices are not anymore just computed at the clouds. They are actually being placed on different areas of these Continuum, So Edge to Cloud continue and this notion of edge to help continue has to do it computation. Okay, so we have two worlds the networking world and the computational world sort of colliding. So here you have an example for instance for the same aspect with for Consumer reality. So this would be forces in a city where we have some buses with some Edge notes. Okay. So these are as you know could be simple knobs embedded like a Raspberry Pi that's what is happening today could be something more clever like A regular laptop. which runs already some information for instance. Imagine on this bus. There are a number of users that since information take pictures see the status of the road. Not right notes on about delays and then that information is normally would be sent all to the cloud and what is then today already is that data is aggregated on on this yellow boxes. So on the on the edge, and then it's only process to the cloud whenever required. So this means that we reduce latency or at least intuitively we can reduce latency and we also reduce potential costs because we don't need to pass all the information to the class. Okay, and so one thing that is happening on this Edge love continuum. so devices is our internet is actually that this is continuing to change. Okay. So right now we still have let's say this notion of Internet of things but still static. Okay, so the routers are static so in the core most of the things are still static, but now there are new things becoming it's grated for instance. As you know, we have satellites satellites gives us great access to remote and large regions as you guys have in They are mobile they can route information. So today we have already the possibility to do routing. Okay on Leo constellations. So this means that these are edges and we will have to deal with this integration on the internet. Okay, so this Edge to Cloud Continuum will soon start to be very mobile and is already also in space. Okay. So the major major confusion, let's say so in order to ensure that we can let's say manage all these deployment of applications in this Edge to Cloud continuum. We have three Vogue a little bit the systems that we have and we're created more for clouds. Okay, so all the systems that we have and where we place our applications today. So Docker based applications, they were developed for the cloud and now they have to be adapted to these Edge to Cloud. where the micro Services of applications can be deployed anywhere to serve specific requirements at an application has like lentacy or bandwidth or cost but also to serve preferences that the user the user may say, I don't want my data versus imagine in health. It's really go to the cloud. I wanted to stay on my Edge my home for instance. And so in order to articulate these we need let's so let's say a knowledge plane or some tool that can do better articulation and on this slide you have a summary which you can check. after a About what is what we need to make this. Let's say Edge to cloud computing really work and I would say that from my side and in addition to AI that we are all speaking about right and the role are generative AI etc. Etc. But I think that is extremely important is to ensure that we can find a way to develop architectures that do joint computation and networking and so John Gage. Okay sets a long time ago. That's the network is the computer and that is actually I would say that the internet now is the computer. Okay, so it's happening more and more and more with a lot of efforts based on human configuration. And so in order to to invest these okay, so there are a number of challenges and the next slides so I'm going to go a bit faster over this because I would like to to give you an example we disconnect about how these orchestration can be then what does it mean? What do we need? So there are a few challenges. Okay. So if we want to advance to this Edge Cloud Continuum that will have large-scale very dense areas a very small in them sensors and then also have to handle that already clever sets of satellites and devices on the air plus our personal devices that are increasing their intelligence. There are a number of challenges that we have. address in computer science and from our perspective. okay here in my team and my view is that there are these there are much more challenges but these are the ones that we are focusing upon focusing upon. so joined computation networking to us is crucial ensuring that whatever you do if any AI if we need an AI system it can run on top of mobile systems not necessarily seller, but I'm saying we're routers maybe mobile. We're not just our our interviews the device is our mobile and context awareness. So brings them information about the user about the application and about to surroundings where the application is running. and so for that we have a number of topics that were we are working and so today I'm going to collect all this project that I'm going to speak about gives you an additional more Hands-On inside into the first topic which is this flexible wedge Cloud Continuum where we are trying to create a framework that's not only is more flexible. So allows to support mobile environments, but actually brings resilience and one of the one of the let's say on bizarre expression on these management one of the goals that we have is to reduce the energy that is required to do the art expression. Okay. So another is of course improve latest latency or increased throughput that we are also very interested in reducing latency. so this project as I explained is a large project that so we have a few of these you possibly also I believe they're also common projects between Brazil and Europe. I also have some students there. So but basically this is a large project involving 16 Partners now across Europe you have the logos that the main ones there and where we decided So based on this Vision that you see and that we have in parties we decided together. Okay to create an open source framework while they call means cognitive centralized Edge to glad orchestration. that's has the difference of trying to inject into these archustration data metrics compute Matrix, which means observation a CPU and memory and network metrics. which means observation of perimeters such as latency, but also bandwidth congestion Etc and the reasons why we decided to do that, so the challenges are the ones that I just explain. And so what is the status of the piece? This project has already one year of living in June actually will be 18 months and we are so we have already some code released which you can play with I will explain also in an adventure slides, but more importantly we will release the full framework all components in June. So in this project we are trying to work straight The Edge to Cloud Continuum that of course because this has to be a bit more practical. We selected a number of use cases. So today, let's see if I manage to speak about one, but you have here examples. So you have the the use cases that we are doing. Okay, and you also have information and there's a report that explains all these so but let me then go to the notion of orchestration still from a high level perspective but to give you so basically of course when we go from idea, right, so these that's called Continuum. We need to manage it. How can we do it? How can we work the computation layers? Okay plus the networking layers all together because these are normally separate worlds in computer science and then what to use in practice. Okay. So what we decided to use was to create this framework around Hoover dance, and why? as I explained most of the iot applications in this really iot applications. They are containerized today. We all use Docker even for fun. And so this means that kubernetes is the manager that handles how these microservices will be deployed. And so what we decided to do was to create a framework, that's so framework means sets a software components that can work with kubernetes. So they are modular some are independent. Is for instance we have a scheduler It can be used instead of the kubernet scheduler and we have an orchestrator based on AI you can be used together with kubernetes must not necessarily with our scheduler, but it is also intended to be worked as a whole and the the key aspects you have here. So this framework. Why does it run? So here you have a representation for instance two edges, and I'm just giving you a high level example on how these woodwork. Okay. So this means Francis emerging that's on edge one. I'm running some application. Okay, so this could be again to make it simple observation of people so meaning the number of people in a building or in a room. okay, or could be environmental observation and let's suppose that that service is running on edge one and now there's a bunch of users moving more wedge to okay and basically for their who can be placed in the edge can be placed also on the cloud. So here's the representation. There's not mean that go there for necessarily has to run on the well and basically what it can do is based on some external injection of perimeters external here means for instance user moves or 10 users moved. So in fact, we should activate the service. that was observing the room in everyone in Edge to because now we have not many people there or maybe it is empty or maybe it's it is night. So these are external. Notions of context. Okay context awareness and and we want to activate that service on on the second Edge. Okay, and then the service starts running and all the let's say all the networking interconnections and the computational aspects have to be adapted and that's what connect with us that adaptation smoothly or we hope it will do it smoothly. So so then what does what does this project has? So in order to create these notion of our situation we decided as I said to create modular components and you have here some examples. So for instance we start you see here the user so the user is for instance a person that wants to deploy an application. So imagine you want to run some application with a mobile robots Okay. So with some friends and that mobile robots uses slam. okay, which is extremely heavy and so the first the first aspect is normally when we work these today, we upload the applications somewhere with kubernetes. Okay. So first we do it's Docker have or we use some other tool and then we just say Okay. I need the CPU. I need this memory and I want to run this for a hundred days. and kubernetes does wonders. And basically places things in specific notes, but but in terms of the so in terms of the of the network as they explained there is no realization of what is going on. So maybe the node files are the lead files between the nodes and therefore our application cannot run kubernetes does not see that so what we do with codec with that so we have this interface to the user and as you see here there is here something called create as control plane. So this means here we will link overheads and then we have a number of components that two different things for instance. MDM can injects data observability to the system for instance. It can say that the database has one style database of the application. So imagine there was a problem with the database with the data sources and it went style or actually, that's it grew too much. Okay, and we need more another note to run the application or or to the database. We have also the brain let's say the Arts writer that relies on AI to analyze how stable is the system over time. So the the here this these components that we call pdlc it does not the decide where to place the application it simply tells the scheduler of which is the entity responsible to place the application or its components on different notes. Okay, so imagine a graph and nodes and the schedule of decides on how to do that. So pdlc tells the scheduler. Hey, I think that the best option to you to run this application based on prior learning. Okay is actually to deploy it this way. That's a scheduler makes the final decision on how to do that. We also have as you see here. We inject some Network information. Okay, so we bring some information for instance bandwidth congestion, but also other interesting things for instance energy. It's expense on the connections that are on the transmission on a specific note. So that's the orchestration. And so what is the status of this now? I'm going to jump you have a much much more slides. So as I said you have if you want to read more, okay about houses this work and what exactly this is we have to large reports and also some papers. Okay. so we collect everything on Augusto which is an international database open and here what you have is the architecture of called Echo. So meaning the software part what exists already has open source is in green. What is still not uploaded? So once again, I did not mission. is that code is in an eclipse repository and what is still not available is in red, but until June there will be a version available. And so I'm going to jump over a few slides. Okay, because I would like to go to these guys but on these Slides you have more information about each of these components if you want to understand more. What we do, okay. you also have some examples of Matrix. So I'm speaking about metrics and data. But what exactly do you use and you have here? Okay. So for instance that observability we collect information about the databases the freshness compliance. So whether Francis in Europe, we are all into this gdpr and data protection and therefore sometimes we cannot run applications outside Europe and therefore this is an option of compliance for instance and but we also collect some other information. Okay, which you can see here and I think the most interesting and we which is the one that we are still not yet completely integrating is actually something from Mission about user preferences. Okay, so this could be very simple examples for instance of location not personal location. Okay. So this would be also skated market location. So just points and nothing personal here. It's more about preferences. I said it's different if a user is running an application. Have a number of use cases. So in the use cases, what what we do is that we try to create write an environment that we've thought was meaningful for code Eco to show how code they could perform and why we need something like today and we have six use cases very different. So take a look because they are interesting and there's a lot of information already about them the start of the development of these is that right now most of the use cases have already equipment set up and installed at a lamp. Okay, but now some of them are starting the second place which is integration in operational environments and for instance one of them that I bring to you is from University of getting and being then with the city of getting and where the colleagues and are deploying using Edge Computing and here forces you see, okay, so there are some locations in the city and what they want to do so they already have an edge note that says system information. They also use leaders to understand traffic patterns and they hear they give a very very simple example okay of what happens for instance they have questions around the city each of them includes two to three nodes. We call this the cluster. Okay, and they are processing data and maybe in some cases it would be best to redistribute that processing and for instance an example. very simple that they give here is for instance at night where we have a direction of traffic with a lot of heavy traffic and What we have maybe on the other side some Edge note collocated with cameras processing barely nothing and so in the in that case some of the processing should be passed to the edge notes that are almost empty and that's where what that go enters Okay. So podeco does not play with the application. There's not even no the application. Okay. So the application is like a black box, but it can pass part of the application and information so database status two other nodes to redistribute the load to reduce the energy that we need to reduce the latency that we need for the specific application. so I'm now going to jump across the others, but as you see you have some information about it, and also there are contacts around if you want to speak with the people. and I would like to call your attention so that last slides to several things. So the first is that you can already if you are interested on these experiments with code Echo and we would be very glad if you do so because at different levels so in the gitlab repository of codeco you have not only the components that you also have information about how to Randy so you can just run it for the purpose of experiments. Okay, really using the images that we have created or you can really download the code contributes. Okay, and this is open source, so that's Really a great world open so you have all the components of codec already with with some parts and then you also have something that we find very interesting which is an experimentation framework that we are developing. So actually is led by our colleagues Latina in Greece and they have developed really a pipeline for experimentation that you can do with code Echo. So this means you want to do some experiments, but you don't understand kubernetes and you don't know exactly where do I get to that and how am I going to install for their own and they have created a framework that you can just download and do some tests and devops everything from from start when he has examples of applications. We have a data generator. Of course I said notice that is our first place of work. So not everything is perfect, but you can already experiment we are doing the same and it works well and so meaning The experimentation format you have it here and you can see better. So there are also a few Pipers in the nodu about this about what what we can do and what you can do yet at least you have already the new scheduler integrated. So this means that you can play with some applications and try to do deployment to comparisons with coordinates and experimentation framework does everything so it deploys everything and that's the collection of measurement parameters of all code that whole components. So it's very useful. To play and to do some experiments and the rights and some papers. so you can also final slides promise to slide so you can also engage with us. So as other projects large projects, we have a program that is actually starting now. so started in April led by our partner Inova and that basically sort of an engagement program. So we are doing multiple events. You see there. Okay, and we're going to give some awards. We also give the possibility to participate in some events. Not necessarily just project events, but for instance scientific workshops or conferences where the project is involved and so as you see there, okay, and so this is another way to get engaged. So this is my last slide. You have all the links here. And I thank you for your time. And let's hear your questions. Thank you. >> Christian Esteve Rothenberg: Thank you Ruth. Very interesting. Stuff. I was already here a browsing through your website the open source repositories as you know, I am a big fan of Open Source open research and I I think I will try to borrow some of your nice ideas in the project. I really like the this packaging approach that you are trying out and sharing here a little bit just about the you the project website this Pro this program is also very interesting. This is a UI type of initiative so that they encourage all you projects to to do this ircp program to engage. It's very nice to see the maturity of how projects now are embracing open source in a much more. Let's call it effective and professional way. Yeah. >> Rute Sofia: Right, I think. there's as you know, Christian a strong push. Let's say in Europe to this. So do this and across a lot of things right? And I'm very that we managed to convince even the large players in in the projects open everything. Okay. So what sort of wasn't interesting process I would say but very happy for this. Okay, so everything is really open. >> Christian Esteve Rothenberg: thank you a great great example and I'm sure we will. have opportunities to to collaborate. Let's let me take one question from my side, which is related to one question that appeared over YouTube. And and first and I must confess. I was not aware about this fear. So this near and far Cloud there and and I know you were working user-centric. So for me you used to Centric near was like close to the user and far far from the user but your perspectives are from the >> Rute Sofia: exactly >> Christian Esteve Rothenberg: centralized operator Cloud view so near from that and far from from that right? just a matter of perspective, right? >> Rute Sofia: Well, I mean, no far is really considered in user as close to the end user recipes possible. you are right. This is the Telco perspective, but in fact is what is being used across all industry here. Okay, so meaning in Europe and even by so let's say the European commission. Did you connect? okay for instance farage is really on the fringes, Okay, so it is indeed there is an adoption of of the Telco Centric. Let's say Perspective. Okay, so not the American folks perspective, but Europeans help perspective. >> Christian Esteve Rothenberg: I will need to ask my colleagues that work here in Brazil. What is our perspective? Maybe I can bring it to you. What is para near because in Brazil as you know are near and >> Rute Sofia: Yeah, for instance. It's also forces Huawei calls it here deeply Edge. I think if I'm not mistaken or I >> Christian Esteve Rothenberg: yes. >> Rute Sofia: think IBM called micros always the same. Okay, so we're very civil. So fog is so there's also okay, but let's this is indeed the approach being used. >> Christian Esteve Rothenberg: Good good to know we need to to we're always up to date and one of the questions from ADI Ariel was is it the same as Folk for computing was a term that has already sometime. How do you relate folk would be the near Edge or or >> Rute Sofia: Yeah now actually Focus both right so you you don't have it here, but fog the the concept of fog is more American very interesting. Okay from I think Roberts right and Cisco or not long time ago, and it's actually so, you know, we have these different views and Europe is really more focused on boxes usually but basically fog is more variable. Okay, so it's actually covers for Edge to me a edge, but it's more Flex. Go because it doesn't Focus so much just on focus more on the functionality on the functions that we have. So I would say that for computing is actually the same thing more or less. okay with a bit more flexibility than far engineer Edge. So I think that the key difference here, is that when we say farage, there are a number of things that we cannot compute yet close to the resources. Okay, and so people in Europe are trying to go so to push functionality even from the network. So the aggregation right as far as possible sources and this is farage. Okay, so if you go to fall, so let's say let's tell call Mark competitional View. And fog is more flexible. Okay. There are also so on the paper that this is a white paper, but you have information about that about the different models and also about origins of four Computing. There was something before Computing Okay, so they're old ideas that anyway come to life and what is happening now or happens if you're 20021 >> Christian Esteve Rothenberg: Good good. Thanks. so I'm brushing here through the questions of of this students that I say explain to you. They do some homework on on the topic of the seminar. So and I spotted here a common. a type of questions around the security aspects here. They one was very explicit here asking. what are examples of security a challenges what are key challenges with the and you talk also exemplified. I know it's very much related to Industrial use cases. So the devices are out there. So anyone in theory could have physical access, I guess that's a concern or someone who pick up this device and then try to break it or what would you? >> Rute Sofia: Okay. >> Christian Esteve Rothenberg: share about the security >> Rute Sofia: so >> Christian Esteve Rothenberg: challenges >> Rute Sofia: so right as you know, what happens now is that basically everything is very statically configured. Okay, ACLS and service level agreements and then also really hard. Let's say access to devices. So you mentioned manufacturing. No, there is no access to the manufacturing environments. They are closed. Okay, they can send information. Nothing can answer. That's how it is. words today so they can even disconnect if they want from the club because okay. There's also I possibly you you all know. Okay, so there's this notion of industrial Edge a big push from Siemens about equipment. that's is let's say a little scale lower than clouds in terms of computation and runs in the edge is very, well. There's all the processing now, let's suppose on this new environment as you said, okay where there is more flexibility, and now let's qualify. you can show you and so for some this kind of environments on the right hand side. where now we have some manager, right and we are orchestrating these vegetables. What are the issues that we have? So of course as you know, we send some it's not that for instance an old enters suffers with kubernetes today. You have a list of suitable notes, right and those nodes whereas if you use AWS or whatever. okay online Cloud space as you know, all all the notes are just worthy for them at least. Okay, but there are the problems. So one is being interest worthy ensuring that's not the malicious user. That's how it is done today, but another problem is I said is compliance. Okay. Can you run your data there? Can you run your application there? Okay, and your country is it's regulated to the carving believe to the rules of of your country. Okay. And so that's another aspect now if we move to something where we need code there. Okay, where? more variable and now when you know the answers like your smartphone, how can we know? it's just worthy. Okay, so there are several ways one thing that we are looking into. okay, which I did not explain a lot more technically. Is that like we just browse there so I don't want to take a lot of time but let me see here. Is that on this on these are expression and using Ai and on this election of notes. We use something called or we are we have proposing to you something called swarm Computing where the notes where we are going to so the suits of all notes for this graph. Okay into deploy the applications they will we'll set first some contracts. Okay based on DLC. Okay. Okay, so based on a ledger technology So to be more clear and so this means that before starting something we are using smart contracts. Okay, but of course there must be some levels so that there's one why one is okay on my graph. I just use these nodes because I trust them that's a normal way. Okay that we all do to we know once really to use this more. flexible stretches. What can we use? Okay, and one thing that has been used in the past, but we are not doing it. Okay is actually Trust management as we do with humans. Okay, so it was quite used in opportunistic routing for instance. where we negotiate trust or we have some rewards okay or some incentives for good behavior. so that this does not undermine attacks that we may have but actually create systems that are more just worthy. Okay. So for this decentralized systems more flexible decentralized Stress Management is something that would be relevant to consider. So >> Christian Esteve Rothenberg: yeah, so >> Rute Sofia: for the second part which is attacks Okay, so somebody injects malicious information may happen. Okay. So your application runs like crazy becomes greedy. so codec was prepared for that. Okay, so we have sort of a governance and resilience framework that we are building. Okay, but there's some rules so that we can test and prepare the system so that no application can can do that. Okay, but let's see successful. >> Christian Esteve Rothenberg: the other being big thing before we've finalized that the time passes so fast, I was spotting here is as you put all those in the in the Titleist energy efficiences. We know sustainability is a big topic there are so to say easier targets, so a device level a compute level to do energy efficient aspects. What would you say? It's the code they call Main. goal or Innovative approach towards Energy Efficiency or new insight or some insight that that you are. Working on what could you share around it? >> Rute Sofia: so I I saw in this slide that you have. So what we are doing is so notice it for as Energy Efficiency is not about the machines and making them work better. So imagine a graph where this applications are deployed. We are looking at the networking level and computational level. Okay. So what we are trying to say, is that okay. I'm going to select the graph. Okay the nodes and there is a connections so that it reduces the energy consumed and we call that greenness. Okay, and basically let's say users says I want to operate my application and they sure that I don't know. Okay, I never reach 80% or of the full capacity of the system or that I want to keep energy consumption on these level. Okay. So in our case right now is really energy consumption so battery of the notes. And energy through used during transmission. Okay, but could be other things would be CO2. We are not doing that yet. So and what we do is we created some functions. Okay, so something that we use on the Let's say on dark straighter on the AI based Ox writer which give a cost to an old. Okay where that costs the greenness cost of the node relates with let's say the number of application it is running and so on. Okay the battery test and so with the Transmissions that it's doing. okay, so we create we have functions that's you have here an example. Okay, so there's Global so if you want to test still very I would say initial at this part of the functions that we use, but you can play with it. And so basically the idea is that okay, so that's where we play with energy. So we are saying we select the graph where the notes have a minimum energy cost or where the graph is the minimum energy cost and taking into consideration other parameters as well. Okay, and that's so one thing we have to do is to prove that this is actually the best option. Okay. So right now this is life, let's say we are doing this because we believe it will improve over all the system. That's We did not completely validate it yet. Okay in large scale or something like that. Okay, but that's the idea. >> Christian Esteve Rothenberg: Yeah, the energy domain is is huge. It's a must say it's to embrace it as a as another research Contin. consideration. >> Rute Sofia: Yeah. >> Christian Esteve Rothenberg: It's it's >> Rute Sofia: You have a case of a use case. Sorry to just wear about really energy where colleagues from University of polytechnica de Madrid >> Christian Esteve Rothenberg: Oh. >> Rute Sofia: are using codec proposing to use code Eco to improve the smart grid operation. Okay, so you can check this. >> Christian Esteve Rothenberg: Good. Yeah, I have some colleagues and background over there. So let me wrap up first, of course. Thank you rute. It was great having your your seminar. They open source out there. I will spread the word and let's catch up more and talking about catching up. Let me just share with everyone what is coming next next week? This is a topic you you will certainly like because it's related to Industry time sensitive networking over Wireless. So that will be next Thursday seminar by Dave from Intel and then we'll have an alumni from our faculty at Google working on. on Gemini this generative AI work and then we have others still for this first semester in our queue. So router. Thank you again. See you soon. >> Rute Sofia: Thank you, Chris. >> Christian Esteve Rothenberg: Keep safe. >> Rute Sofia: See you soon. See you >> Christian Esteve Rothenberg: Thank you. 
>> Christian Esteve Rothenberg: great another Thursday seminar life here with roots of Sofia long time that we know each other from different types of interactions. I've visited her in Portugal in the labs. Now. I know she's Germany and we hormone collaborators soon working in the lab and we'll see each other also in US CNC in Belgium next month Looking forward to catch up. And talking about catching up a root is bringing today a very interesting seminar and that merges a couple of Hot Topics right Internet of Things Edge Cloud Hear a lot about this. What is this continuing of each and cloud? and of course Energy Efficiency and resiliency, so relevant and of any networked system. As usual route. I am very informal so I'm not going to read your extensive bios our students and and Watchers can read and also follow your links your Publications for you connect with you over LinkedIn. And so with that I will pass you. So that you can present usually around 40 minutes and then we will chat and I will pick some. questions from from the class and also from our life. participants over over YouTube Okay. >> Rute Sofia: Okay. Thank you. >> Christian Esteve Rothenberg: So, thank you. >> Rute Sofia: So hi everybody and Christian. thank you again for the invitation. It is really also a pleasure to speak again with you and I'm really happy that we are going to to meet each other again in or have this chance in network because after covid as we all know. A lot a lot Rights happen. So as as Christian explains and let me see if I can move my Going to today's speak about this so-called. that's to glad orchestration. But before that I have a quick I hope introduction. So Christian didn't introduced me. I would like more to introduce what what I'm doing in Germany right now, So you see here my background and I'm not going over that what is important? I think for the seminar is for you to know that my background technical background is on packet based networking. So in the past I've worked on scenes like quality of service IPv6, but I've also worked on Mac layer design on high-speed networking and then also on wireless and since 2019 I moved to Institute in Germany called forties. We are in Munich. So in the southern part of Germany to develop a new team focused on Industrial iot so technically where we focus is on next Generation industrial iot applications, and our research is focused on the development of networking architectures and protocols because that's where my background and I think my best contribution can be and also on computational architectures So in a sense this is a mixed team and in this so intelliciplinary team, it means that my team has as researchers people that come with a background from networking like me. It's also people that have a background more focus in iot for instance with some knowledge on semantic Technologies and Distance in AI not on developing new AI models, but actually on in particular forage Computing and so the Institute where I am sort is you have here information. Okay. So is the software oriented Institute? So this means software engineering oriented Institute. We have a number of teams. So we call those teams competence fields and each team. Let's say with the teams are aggregated into three clusters one focus is on software engineering the other focus on Industrial or intelligent infrastructures and the third focus on robust Ai, and so this means that I said, there's also these interdisciplinary notion among me and Michael Leeds and let me give you also a quick view on how we what we do and how we perceive iot because that is important to understand our Notions of edge Cloud continue, right? So everybody speaking about these so that so in the industrial iot team Mauricio. Is that in order to enable Next Generation iot and these next generation iot will is comprising that will comprise mobile devices. So heavily will compress mobile devices, but also as we know very very small embedded devices, but it also involves people and it involves AI so cyber physical objects with cognitive capability and in order to so imagine all these devices and cyber physical systems interconnected and also people and interconnected by a system of networks, and that system needs to be a bit more clever than what we have today on the internet and so the vision of my team is that we think we should rethink and redesign to some point. of course some of the infrastructure that we have for iot and that redesign is based on considering a joint design. working and computation All the way we work is that we do research of course similar to what possibly you do with Christian's group and then our outcome is normally vital scientific Publications, but also and I think this is relevant open source software. So there's a link here. Okay that you can check with some of our most relevant open source software. Normally we have some demonstrators for the edge. So that's where we are for. now really going into the seminar So today what I bring you is some information about what is this Edge to Cloud Continuum, and then I'm going to bring an example based on a European project called quote echo on how we perceive these orchestration. What does this mean? What are we using in terms of Technologies? And then I have created also a few other parts on the seminar. Okay. So one relates to give you some information about use cases, so where do we need this? Where are these environments that are a bit more variable than the ones we have today on the internet and I bring also some information about how you can experiment with this project. So maybe this is interesting because we are developing a bunch of opens our software with a number of companies and academic Partners in Europe, and maybe this is interesting also for you as students. So but let's start by the beginning. So in order to explain to you On edge 12 Continuum we need to go back and to think about Internet of things that also to think about that now we call it internet things, but in fact, it's the internet right So the internet of things it's basically the internet since it's beginning. so since darpanet, so it's just that it has evolved and evolved and evolved. It is a complex system, but now it interconnects a lot more things most of the traffic is on the edge on on the fringes of the network. So the internet as it was designed there's a principle on which it is based called the end-to-end principle, which basically says that the intelligence is on the edge. So the intelligence is really on the system. There's nothing intelligent in the middle. We just have a bunch of Dentistry supporters. And that is the network. That's what normally what we call infrastructure or the network. But the issue is that with these new interconnections that we have now to the internet. Okay. so with our smartphones or with sensors also with more clever sensors that can already think and react. Also with satellites the notion events. What is The Edge on the internet is not anymore on the fringes. We cannot stay anymore that we really have applications on on the fringes and or on the extremes that they have the internet. And so this means that there is now the needs and a lot of people across the world. Are thinking about how to adapt to the needs that we have in your applications. So a fact is that isn't when principle is any way changing because we have also other things on the network we have programmable devices we have new ways to do to do computations or not necessarily just on the extremes of the network, but actually in routers already we can do caching in routers. And so these all changes the way we possess information but also that we compute the applications. And so where are we to the in terms of Internet of things so that's sort of how the internet is evolving. So you have here to figures that show a little bit the confusion that we have today on the internet. So on the left hand side you see that we think about the internet as this complex system. So we all know of course that there are differences in popularity. And we still have let's say areas that are so rural very rural that are very isolated with lack of Internet access we have some like a telecommunications still so not necessarily on rural areas. So I'm here in Munich, but if I go to a lake 80 kilometers away close it with the Alps then my my fourth reactsis disappears. So this is basically in urban areas. We know we have been support and outside. It's still a problem and we have something new which is spontaneous communication. So today on the internet. We also have the possibility to create infrastructures on the fly to with a specific purpose for instance emergency communications for health or after a natural disaster. So today these are functions that that we have and that we can we can the internet. So on the right hand side, that's the example that you have. so very stable televised infrastructures and then mobile. okay, integrating satellites uavs and so on but also spontaneous Communications and the mobile devices that we have today, they are also changing so they can do storage so our smartphones they can do this as you know, they can do Computing they can reason up to some point so we can have reasons there. And we have a bunch of different networking technologies that are supported. So this is becoming increasing complex. And so the idea of explaining this is to tell you it's not manageable anymore. Just based on human configuration. So it's not actually just a matter of for instance ensuring that all are at this points are well placed Etc or that our node Vis or our ran areas are well configured in fact the internet as it is evolving requires some facilitation have intelligence. that's it can easily adapt to new situations. So that is let's say the perspective of the internet right and then on the Internet of Things We have of course the networking part, but we have the systems. We have the systems that process data and that gives us level results about data. so Based on data sources and so on this slide what I bring you is Of for a principles or aspects that any iot system has and the issue is that today we have over 600 proprietary iot systems. So it is very hard to handle the interconnection of thousands or hundreds of sensors already for instance in manufacturing in factories, or even processing neural analytics, which is another example of industrial iot applying has already hundreds of sensors. And normally they would be connected via Ethernet of is the mother fixed form and that is being changed and and people are using more and more Wireless and cellular Network Technologies. So that's that's just the parts. That's what you see here represented as connectivity. What we also have on iot system are the interfaces to the user. So the human machine interfaces and we have the data sources, which can be our sensors that can also be people and can also be already as you know objects or cyber physical systems are really just softwareized agents such as an AI agent. So about a chatbot can be a data source or iot. And then of course, we have parts or systems or applications that process the data because that's the important part in iot and that store that data in a way. That is secure. We hope that it users privacy and also one thing that is becoming more more important is to ensure super energy. which means that the data can only be used by let's say their own shareholders or stakeholders and the data is let's say adequately certified or regulate within a specific region worldwide. So that's all is iot. Also, so when we think we speak about internet things except the thing itself is a semantic obstruction, so it's not an object. It's actually a description that we do today. That's describes the object. So the thing about is let's say descriptions. So embass is the description of an object. Okay, but it's also it can also embody the description of human behavior. For instance. I'm going outside and jogging and therefore I run at some speed okay, or I stop now. I am stop so that information can be seen as being sent from a little source and it is relevant for some applications. So this is internet of things but we spoke about Edge to powder extraction. So we are speaking about the internet the transport part the edges. So what the what is this Edge to plot Continuum and why are people calling it? That's the cloud continue. We're not just internet. So when we speak about the internet we focus more really on the infrastructure aspects. So on the transport of information and when we speak about urgent Cloud we focus on the data processing and on the data start storage. So on the Improvement of processes on the improvements of the performance of applications that we are using in some specific use cases or domains processing health or in manufacturing and so today. Okay, so there are different of course different terminologies for Edge. You will hear about depech farage and microwaves Nano ads. Okay. So today what I bring you is sort of a perspective. Okay, there are others that the perspective that we are pursuing heavily in Europe. Okay on the deepest is let's say what is far Edge and here and I'll explain also why we need to have that. difference okay, but the edge to plot Continuum is basically a representation. okay of all the computation and networking management and networking control that we need to do to ensure that the data is adequately processed and that we have the performance that we need for some application from match to Cloud. And wife rematch the plot and where is The Edge? So the edge is closer to the end user it was from Edge to Cloud because today most of the applications as you will see in some examples that I bring they are not anymore stored or computed just on the cloud and the reason for that maybe may have to do it costs but has also it's sort of a process has also to do with the technological Evolution that we have today where virtualization has gained the new space on internet and on the development of applications and where new application architecture so applications that are more modular. not not anymore. Monoblocking start that we merge and also finally Hardware that we have and the capability to have let's say more open Hardware more programmable Hardware So the representation that you have here is a potential representation okay of what is far engineer Edge and basically you see here the human and iot interfaces close to the real world. So close to the people. We are data sources other data sources are cars or other vehicles. So the so-called Vehicles where everything cameras them sensors like temperature sensors and other types of sensors. The forests today is connected to all the sensors that we as in users use close or in the real world, so they are not normally controlled by an operator because the operator does not have the rich to control them, but they are extremely important to the internet because that's where the data sources are. Okay, so to give an example, imagine your home. Okay, no notation. And then you have your network Terminator Okay, and you're access point collocated with a local router and then you have a bunch of smarts things in your home measuring a lot of things Alexa and controlling the light. So all that is the farage. So your operator, which is the network Terminator that he places close to your door, but beyond that there's no control and in fact all the data is there so We call that the far inch. In a factory we call the far Edge for instance involves all the equipment that brings data to the system and normally relies what with what we call a shop floor or could be a warehouse again. There is no control of the operator there. So in this design, that's where you see the far Edge. okay, simplified design and the mirror edge is actually the area that you operator controls. So a very known model. okay that you possibly know of for A near Edge is Etsy Mac. Okay. And so this means that what we do today and let me show you another picture so that I can give another example you have here a similar representation just a little bit more detail on the left hand side. You have the far Edge. So here imagine. This is for industrial iot. Okay, so imagine a factory and then there's a shop floor where we have conveyor belts robotic arms Etc and maybe we also have other locations like Warehouse where we have automated guided vehicles and all these so we call these the far Edge So normally this areas they are Private so in the sense that data can flow to the cloud but normally does not enter. Okay, so this is not because just of a firewall, in fact, there are protocols that are used such as opcua that don't allow this without specific configuration. So in manufacturing the the manufacturers don't want a redirectional communication as early. Okay, so and this is an example where Edge is important. So if all the diet can be processed and control and start that the edge that's something that is important for manufacturing and it's happening actually ready to die on this example. What I bring you is the blue box is they represent parts of applications. Okay. So for instance if I have an application that is monitoring the temperature in rooms then that application as you know, normally is possibly no is containerized. Okay, so that's what we do today. We we have maybe learner a Docker container to support the protocol that does the communication. Okay. We see way and PTT. We have a container that can give basic analytics. Maybe we have a container that handles with tensorflow some aggregation of data something like that and the deployments of those containers today is orchestrated by Liz's system. Okay. That The notes where we can place or containers that belong to an application and as a decision on where to deploy that those containers based on the need for CPU and memory. That kubernetes is not care about the network. Okay, so where that's is not know if it's deploying so to elements that need to be close together and have a minimum latency of 10 milliseconds to run together. Okay, for instance, so but basically what I'm also showing you is that this microservices are not anymore just computed at the clouds. They are actually being placed on different areas of these Continuum, So Edge to Cloud continue and this notion of edge to help continue has to do it computation. Okay, so we have two worlds the networking world and the computational world sort of colliding. So here you have an example for instance for the same aspect with for Consumer reality. So this would be forces in a city where we have some buses with some Edge notes. Okay. So these are as you know could be simple knobs embedded like a Raspberry Pi that's what is happening today could be something more clever like A regular laptop. which runs already some information for instance. Imagine on this bus. There are a number of users that since information take pictures see the status of the road. Not right notes on about delays and then that information is normally would be sent all to the cloud and what is then today already is that data is aggregated on on this yellow boxes. So on the on the edge, and then it's only process to the cloud whenever required. So this means that we reduce latency or at least intuitively we can reduce latency and we also reduce potential costs because we don't need to pass all the information to the class. Okay, and so one thing that is happening on this Edge love continuum. so devices is our internet is actually that this is continuing to change. Okay. So right now we still have let's say this notion of Internet of things but still static. Okay, so the routers are static so in the core most of the things are still static, but now there are new things becoming it's grated for instance. As you know, we have satellites satellites gives us great access to remote and large regions as you guys have in They are mobile they can route information. So today we have already the possibility to do routing. Okay on Leo constellations. So this means that these are edges and we will have to deal with this integration on the internet. Okay, so this Edge to Cloud Continuum will soon start to be very mobile and is already also in space. Okay. So the major major confusion, let's say so in order to ensure that we can let's say manage all these deployment of applications in this Edge to Cloud continuum. We have three Vogue a little bit the systems that we have and we're created more for clouds. Okay, so all the systems that we have and where we place our applications today. So Docker based applications, they were developed for the cloud and now they have to be adapted to these Edge to Cloud. where the micro Services of applications can be deployed anywhere to serve specific requirements at an application has like lentacy or bandwidth or cost but also to serve preferences that the user the user may say, I don't want my data versus imagine in health. It's really go to the cloud. I wanted to stay on my Edge my home for instance. And so in order to articulate these we need let's so let's say a knowledge plane or some tool that can do better articulation and on this slide you have a summary which you can check. after a About what is what we need to make this. Let's say Edge to cloud computing really work and I would say that from my side and in addition to AI that we are all speaking about right and the role are generative AI etc. Etc. But I think that is extremely important is to ensure that we can find a way to develop architectures that do joint computation and networking and so John Gage. Okay sets a long time ago. That's the network is the computer and that is actually I would say that the internet now is the computer. Okay, so it's happening more and more and more with a lot of efforts based on human configuration. And so in order to to invest these okay, so there are a number of challenges and the next slides so I'm going to go a bit faster over this because I would like to to give you an example we disconnect about how these orchestration can be then what does it mean? What do we need? So there are a few challenges. Okay. So if we want to advance to this Edge Cloud Continuum that will have large-scale very dense areas a very small in them sensors and then also have to handle that already clever sets of satellites and devices on the air plus our personal devices that are increasing their intelligence. There are a number of challenges that we have. address in computer science and from our perspective. okay here in my team and my view is that there are these there are much more challenges but these are the ones that we are focusing upon focusing upon. so joined computation networking to us is crucial ensuring that whatever you do if any AI if we need an AI system it can run on top of mobile systems not necessarily seller, but I'm saying we're routers maybe mobile. We're not just our our interviews the device is our mobile and context awareness. So brings them information about the user about the application and about to surroundings where the application is running. and so for that we have a number of topics that were we are working and so today I'm going to collect all this project that I'm going to speak about gives you an additional more Hands-On inside into the first topic which is this flexible wedge Cloud Continuum where we are trying to create a framework that's not only is more flexible. So allows to support mobile environments, but actually brings resilience and one of the one of the let's say on bizarre expression on these management one of the goals that we have is to reduce the energy that is required to do the art expression. Okay. So another is of course improve latest latency or increased throughput that we are also very interested in reducing latency. so this project as I explained is a large project that so we have a few of these you possibly also I believe they're also common projects between Brazil and Europe. I also have some students there. So but basically this is a large project involving 16 Partners now across Europe you have the logos that the main ones there and where we decided So based on this Vision that you see and that we have in parties we decided together. Okay to create an open source framework while they call means cognitive centralized Edge to glad orchestration. that's has the difference of trying to inject into these archustration data metrics compute Matrix, which means observation a CPU and memory and network metrics. which means observation of perimeters such as latency, but also bandwidth congestion Etc and the reasons why we decided to do that, so the challenges are the ones that I just explain. And so what is the status of the piece? This project has already one year of living in June actually will be 18 months and we are so we have already some code released which you can play with I will explain also in an adventure slides, but more importantly we will release the full framework all components in June. So in this project we are trying to work straight The Edge to Cloud Continuum that of course because this has to be a bit more practical. We selected a number of use cases. So today, let's see if I manage to speak about one, but you have here examples. So you have the the use cases that we are doing. Okay, and you also have information and there's a report that explains all these so but let me then go to the notion of orchestration still from a high level perspective but to give you so basically of course when we go from idea, right, so these that's called Continuum. We need to manage it. How can we do it? How can we work the computation layers? Okay plus the networking layers all together because these are normally separate worlds in computer science and then what to use in practice. Okay. So what we decided to use was to create this framework around Hoover dance, and why? as I explained most of the iot applications in this really iot applications. They are containerized today. We all use Docker even for fun. And so this means that kubernetes is the manager that handles how these microservices will be deployed. And so what we decided to do was to create a framework, that's so framework means sets a software components that can work with kubernetes. So they are modular some are independent. Is for instance we have a scheduler It can be used instead of the kubernet scheduler and we have an orchestrator based on AI you can be used together with kubernetes must not necessarily with our scheduler, but it is also intended to be worked as a whole and the the key aspects you have here. So this framework. Why does it run? So here you have a representation for instance two edges, and I'm just giving you a high level example on how these woodwork. Okay. So this means Francis emerging that's on edge one. I'm running some application. Okay, so this could be again to make it simple observation of people so meaning the number of people in a building or in a room. okay, or could be environmental observation and let's suppose that that service is running on edge one and now there's a bunch of users moving more wedge to okay and basically for their who can be placed in the edge can be placed also on the cloud. So here's the representation. There's not mean that go there for necessarily has to run on the well and basically what it can do is based on some external injection of perimeters external here means for instance user moves or 10 users moved. So in fact, we should activate the service. that was observing the room in everyone in Edge to because now we have not many people there or maybe it is empty or maybe it's it is night. So these are external. Notions of context. Okay context awareness and and we want to activate that service on on the second Edge. Okay, and then the service starts running and all the let's say all the networking interconnections and the computational aspects have to be adapted and that's what connect with us that adaptation smoothly or we hope it will do it smoothly. So so then what does what does this project has? So in order to create these notion of our situation we decided as I said to create modular components and you have here some examples. So for instance we start you see here the user so the user is for instance a person that wants to deploy an application. So imagine you want to run some application with a mobile robots Okay. So with some friends and that mobile robots uses slam. okay, which is extremely heavy and so the first the first aspect is normally when we work these today, we upload the applications somewhere with kubernetes. Okay. So first we do it's Docker have or we use some other tool and then we just say Okay. I need the CPU. I need this memory and I want to run this for a hundred days. and kubernetes does wonders. And basically places things in specific notes, but but in terms of the so in terms of the of the network as they explained there is no realization of what is going on. So maybe the node files are the lead files between the nodes and therefore our application cannot run kubernetes does not see that so what we do with codec with that so we have this interface to the user and as you see here there is here something called create as control plane. So this means here we will link overheads and then we have a number of components that two different things for instance. MDM can injects data observability to the system for instance. It can say that the database has one style database of the application. So imagine there was a problem with the database with the data sources and it went style or actually, that's it grew too much. Okay, and we need more another note to run the application or or to the database. We have also the brain let's say the Arts writer that relies on AI to analyze how stable is the system over time. So the the here this these components that we call pdlc it does not the decide where to place the application it simply tells the scheduler of which is the entity responsible to place the application or its components on different notes. Okay, so imagine a graph and nodes and the schedule of decides on how to do that. So pdlc tells the scheduler. Hey, I think that the best option to you to run this application based on prior learning. Okay is actually to deploy it this way. That's a scheduler makes the final decision on how to do that. We also have as you see here. We inject some Network information. Okay, so we bring some information for instance bandwidth congestion, but also other interesting things for instance energy. It's expense on the connections that are on the transmission on a specific note. So that's the orchestration. And so what is the status of this now? I'm going to jump you have a much much more slides. So as I said you have if you want to read more, okay about houses this work and what exactly this is we have to large reports and also some papers. Okay. so we collect everything on Augusto which is an international database open and here what you have is the architecture of called Echo. So meaning the software part what exists already has open source is in green. What is still not uploaded? So once again, I did not mission. is that code is in an eclipse repository and what is still not available is in red, but until June there will be a version available. And so I'm going to jump over a few slides. Okay, because I would like to go to these guys but on these Slides you have more information about each of these components if you want to understand more. What we do, okay. you also have some examples of Matrix. So I'm speaking about metrics and data. But what exactly do you use and you have here? Okay. So for instance that observability we collect information about the databases the freshness compliance. So whether Francis in Europe, we are all into this gdpr and data protection and therefore sometimes we cannot run applications outside Europe and therefore this is an option of compliance for instance and but we also collect some other information. Okay, which you can see here and I think the most interesting and we which is the one that we are still not yet completely integrating is actually something from Mission about user preferences. Okay, so this could be very simple examples for instance of location not personal location. Okay. So this would be also skated market location. So just points and nothing personal here. It's more about preferences. I said it's different if a user is running an application. Have a number of use cases. So in the use cases, what what we do is that we try to create write an environment that we've thought was meaningful for code Eco to show how code they could perform and why we need something like today and we have six use cases very different. So take a look because they are interesting and there's a lot of information already about them the start of the development of these is that right now most of the use cases have already equipment set up and installed at a lamp. Okay, but now some of them are starting the second place which is integration in operational environments and for instance one of them that I bring to you is from University of getting and being then with the city of getting and where the colleagues and are deploying using Edge Computing and here forces you see, okay, so there are some locations in the city and what they want to do so they already have an edge note that says system information. They also use leaders to understand traffic patterns and they hear they give a very very simple example okay of what happens for instance they have questions around the city each of them includes two to three nodes. We call this the cluster. Okay, and they are processing data and maybe in some cases it would be best to redistribute that processing and for instance an example. very simple that they give here is for instance at night where we have a direction of traffic with a lot of heavy traffic and What we have maybe on the other side some Edge note collocated with cameras processing barely nothing and so in the in that case some of the processing should be passed to the edge notes that are almost empty and that's where what that go enters Okay. So podeco does not play with the application. There's not even no the application. Okay. So the application is like a black box, but it can pass part of the application and information so database status two other nodes to redistribute the load to reduce the energy that we need to reduce the latency that we need for the specific application. so I'm now going to jump across the others, but as you see you have some information about it, and also there are contacts around if you want to speak with the people. and I would like to call your attention so that last slides to several things. So the first is that you can already if you are interested on these experiments with code Echo and we would be very glad if you do so because at different levels so in the gitlab repository of codeco you have not only the components that you also have information about how to Randy so you can just run it for the purpose of experiments. Okay, really using the images that we have created or you can really download the code contributes. Okay, and this is open source, so that's Really a great world open so you have all the components of codec already with with some parts and then you also have something that we find very interesting which is an experimentation framework that we are developing. So actually is led by our colleagues Latina in Greece and they have developed really a pipeline for experimentation that you can do with code Echo. So this means you want to do some experiments, but you don't understand kubernetes and you don't know exactly where do I get to that and how am I going to install for their own and they have created a framework that you can just download and do some tests and devops everything from from start when he has examples of applications. We have a data generator. Of course I said notice that is our first place of work. So not everything is perfect, but you can already experiment we are doing the same and it works well and so meaning The experimentation format you have it here and you can see better. So there are also a few Pipers in the nodu about this about what what we can do and what you can do yet at least you have already the new scheduler integrated. So this means that you can play with some applications and try to do deployment to comparisons with coordinates and experimentation framework does everything so it deploys everything and that's the collection of measurement parameters of all code that whole components. So it's very useful. To play and to do some experiments and the rights and some papers. so you can also final slides promise to slide so you can also engage with us. So as other projects large projects, we have a program that is actually starting now. so started in April led by our partner Inova and that basically sort of an engagement program. So we are doing multiple events. You see there. Okay, and we're going to give some awards. We also give the possibility to participate in some events. Not necessarily just project events, but for instance scientific workshops or conferences where the project is involved and so as you see there, okay, and so this is another way to get engaged. So this is my last slide. You have all the links here. And I thank you for your time. And let's hear your questions. Thank you. >> Christian Esteve Rothenberg: Thank you Ruth. Very interesting. Stuff. I was already here a browsing through your website the open source repositories as you know, I am a big fan of Open Source open research and I I think I will try to borrow some of your nice ideas in the project. I really like the this packaging approach that you are trying out and sharing here a little bit just about the you the project website this Pro this program is also very interesting. This is a UI type of initiative so that they encourage all you projects to to do this ircp program to engage. It's very nice to see the maturity of how projects now are embracing open source in a much more. Let's call it effective and professional way. Yeah. >> Rute Sofia: Right, I think. there's as you know, Christian a strong push. Let's say in Europe to this. So do this and across a lot of things right? And I'm very that we managed to convince even the large players in in the projects open everything. Okay. So what sort of wasn't interesting process I would say but very happy for this. Okay, so everything is really open. >> Christian Esteve Rothenberg: thank you a great great example and I'm sure we will. have opportunities to to collaborate. Let's let me take one question from my side, which is related to one question that appeared over YouTube. And and first and I must confess. I was not aware about this fear. So this near and far Cloud there and and I know you were working user-centric. So for me you used to Centric near was like close to the user and far far from the user but your perspectives are from the >> Rute Sofia: exactly >> Christian Esteve Rothenberg: centralized operator Cloud view so near from that and far from from that right? just a matter of perspective, right? >> Rute Sofia: Well, I mean, no far is really considered in user as close to the end user recipes possible. you are right. This is the Telco perspective, but in fact is what is being used across all industry here. Okay, so meaning in Europe and even by so let's say the European commission. Did you connect? okay for instance farage is really on the fringes, Okay, so it is indeed there is an adoption of of the Telco Centric. Let's say Perspective. Okay, so not the American folks perspective, but Europeans help perspective. >> Christian Esteve Rothenberg: I will need to ask my colleagues that work here in Brazil. What is our perspective? Maybe I can bring it to you. What is para near because in Brazil as you know are near and >> Rute Sofia: Yeah, for instance. It's also forces Huawei calls it here deeply Edge. I think if I'm not mistaken or I >> Christian Esteve Rothenberg: yes. >> Rute Sofia: think IBM called micros always the same. Okay, so we're very civil. So fog is so there's also okay, but let's this is indeed the approach being used. >> Christian Esteve Rothenberg: Good good to know we need to to we're always up to date and one of the questions from ADI Ariel was is it the same as Folk for computing was a term that has already sometime. How do you relate folk would be the near Edge or or >> Rute Sofia: Yeah now actually Focus both right so you you don't have it here, but fog the the concept of fog is more American very interesting. Okay from I think Roberts right and Cisco or not long time ago, and it's actually so, you know, we have these different views and Europe is really more focused on boxes usually but basically fog is more variable. Okay, so it's actually covers for Edge to me a edge, but it's more Flex. Go because it doesn't Focus so much just on focus more on the functionality on the functions that we have. So I would say that for computing is actually the same thing more or less. okay with a bit more flexibility than far engineer Edge. So I think that the key difference here, is that when we say farage, there are a number of things that we cannot compute yet close to the resources. Okay, and so people in Europe are trying to go so to push functionality even from the network. So the aggregation right as far as possible sources and this is farage. Okay, so if you go to fall, so let's say let's tell call Mark competitional View. And fog is more flexible. Okay. There are also so on the paper that this is a white paper, but you have information about that about the different models and also about origins of four Computing. There was something before Computing Okay, so they're old ideas that anyway come to life and what is happening now or happens if you're 20021 >> Christian Esteve Rothenberg: Good good. Thanks. so I'm brushing here through the questions of of this students that I say explain to you. They do some homework on on the topic of the seminar. So and I spotted here a common. a type of questions around the security aspects here. They one was very explicit here asking. what are examples of security a challenges what are key challenges with the and you talk also exemplified. I know it's very much related to Industrial use cases. So the devices are out there. So anyone in theory could have physical access, I guess that's a concern or someone who pick up this device and then try to break it or what would you? >> Rute Sofia: Okay. >> Christian Esteve Rothenberg: share about the security >> Rute Sofia: so >> Christian Esteve Rothenberg: challenges >> Rute Sofia: so right as you know, what happens now is that basically everything is very statically configured. Okay, ACLS and service level agreements and then also really hard. Let's say access to devices. So you mentioned manufacturing. No, there is no access to the manufacturing environments. They are closed. Okay, they can send information. Nothing can answer. That's how it is. words today so they can even disconnect if they want from the club because okay. There's also I possibly you you all know. Okay, so there's this notion of industrial Edge a big push from Siemens about equipment. that's is let's say a little scale lower than clouds in terms of computation and runs in the edge is very, well. There's all the processing now, let's suppose on this new environment as you said, okay where there is more flexibility, and now let's qualify. you can show you and so for some this kind of environments on the right hand side. where now we have some manager, right and we are orchestrating these vegetables. What are the issues that we have? So of course as you know, we send some it's not that for instance an old enters suffers with kubernetes today. You have a list of suitable notes, right and those nodes whereas if you use AWS or whatever. okay online Cloud space as you know, all all the notes are just worthy for them at least. Okay, but there are the problems. So one is being interest worthy ensuring that's not the malicious user. That's how it is done today, but another problem is I said is compliance. Okay. Can you run your data there? Can you run your application there? Okay, and your country is it's regulated to the carving believe to the rules of of your country. Okay. And so that's another aspect now if we move to something where we need code there. Okay, where? more variable and now when you know the answers like your smartphone, how can we know? it's just worthy. Okay, so there are several ways one thing that we are looking into. okay, which I did not explain a lot more technically. Is that like we just browse there so I don't want to take a lot of time but let me see here. Is that on this on these are expression and using Ai and on this election of notes. We use something called or we are we have proposing to you something called swarm Computing where the notes where we are going to so the suits of all notes for this graph. Okay into deploy the applications they will we'll set first some contracts. Okay based on DLC. Okay. Okay, so based on a ledger technology So to be more clear and so this means that before starting something we are using smart contracts. Okay, but of course there must be some levels so that there's one why one is okay on my graph. I just use these nodes because I trust them that's a normal way. Okay that we all do to we know once really to use this more. flexible stretches. What can we use? Okay, and one thing that has been used in the past, but we are not doing it. Okay is actually Trust management as we do with humans. Okay, so it was quite used in opportunistic routing for instance. where we negotiate trust or we have some rewards okay or some incentives for good behavior. so that this does not undermine attacks that we may have but actually create systems that are more just worthy. Okay. So for this decentralized systems more flexible decentralized Stress Management is something that would be relevant to consider. So >> Christian Esteve Rothenberg: yeah, so >> Rute Sofia: for the second part which is attacks Okay, so somebody injects malicious information may happen. Okay. So your application runs like crazy becomes greedy. so codec was prepared for that. Okay, so we have sort of a governance and resilience framework that we are building. Okay, but there's some rules so that we can test and prepare the system so that no application can can do that. Okay, but let's see successful. >> Christian Esteve Rothenberg: the other being big thing before we've finalized that the time passes so fast, I was spotting here is as you put all those in the in the Titleist energy efficiences. We know sustainability is a big topic there are so to say easier targets, so a device level a compute level to do energy efficient aspects. What would you say? It's the code they call Main. goal or Innovative approach towards Energy Efficiency or new insight or some insight that that you are. Working on what could you share around it? >> Rute Sofia: so I I saw in this slide that you have. So what we are doing is so notice it for as Energy Efficiency is not about the machines and making them work better. So imagine a graph where this applications are deployed. We are looking at the networking level and computational level. Okay. So what we are trying to say, is that okay. I'm going to select the graph. Okay the nodes and there is a connections so that it reduces the energy consumed and we call that greenness. Okay, and basically let's say users says I want to operate my application and they sure that I don't know. Okay, I never reach 80% or of the full capacity of the system or that I want to keep energy consumption on these level. Okay. So in our case right now is really energy consumption so battery of the notes. And energy through used during transmission. Okay, but could be other things would be CO2. We are not doing that yet. So and what we do is we created some functions. Okay, so something that we use on the Let's say on dark straighter on the AI based Ox writer which give a cost to an old. Okay where that costs the greenness cost of the node relates with let's say the number of application it is running and so on. Okay the battery test and so with the Transmissions that it's doing. okay, so we create we have functions that's you have here an example. Okay, so there's Global so if you want to test still very I would say initial at this part of the functions that we use, but you can play with it. And so basically the idea is that okay, so that's where we play with energy. So we are saying we select the graph where the notes have a minimum energy cost or where the graph is the minimum energy cost and taking into consideration other parameters as well. Okay, and that's so one thing we have to do is to prove that this is actually the best option. Okay. So right now this is life, let's say we are doing this because we believe it will improve over all the system. That's We did not completely validate it yet. Okay in large scale or something like that. Okay, but that's the idea. >> Christian Esteve Rothenberg: Yeah, the energy domain is is huge. It's a must say it's to embrace it as a as another research Contin. consideration. >> Rute Sofia: Yeah. >> Christian Esteve Rothenberg: It's it's >> Rute Sofia: You have a case of a use case. Sorry to just wear about really energy where colleagues from University of polytechnica de Madrid >> Christian Esteve Rothenberg: Oh. >> Rute Sofia: are using codec proposing to use code Eco to improve the smart grid operation. Okay, so you can check this. >> Christian Esteve Rothenberg: Good. Yeah, I have some colleagues and background over there. So let me wrap up first, of course. Thank you rute. It was great having your your seminar. They open source out there. I will spread the word and let's catch up more and talking about catching up. Let me just share with everyone what is coming next next week? This is a topic you you will certainly like because it's related to Industry time sensitive networking over Wireless. So that will be next Thursday seminar by Dave from Intel and then we'll have an alumni from our faculty at Google working on. on Gemini this generative AI work and then we have others still for this first semester in our queue. So router. Thank you again. See you soon. >> Rute Sofia: Thank you, Chris. >> Christian Esteve Rothenberg: Keep safe. >> Rute Sofia: See you soon. See you >> Christian Esteve Rothenberg: Thank you. 
>> Christian Esteve Rothenberg: It so good afternoon another Thursday seminar Thursday 1pm Brazil time. very glad to have today Dave cavalcanti is a principal engineer from Intel. He is originally from Brazil. And the topic as I posted it over LinkedIn, it's very timely. It's really about time sensitive. aspects of networking very challenging aspects in wired networks and even more in Wireless and Wi-Fi 5G and the evolution to our six G. type of deterministic communications for use cases in multiple areas in robotics in entertainment. a very interesting topic where the industry I would confess my feelings. that is Advanced compared to Academia in a couple of Senses. That's something we we can discuss later with with Dave and what their academic could do to to contribute more to this field. This is my own challenge in the research group in our research activities to to become more and more competent in this topic. As usual they I don't spend time. It's about time making most of out of our time in introducing our guest speakers always with a very interesting And curricula so everyone can check out the extensive achievements of Dave and with that. I pass you the floor. is you yours. So about 40 minutes of presentation so that we have then 15 minutes for for a chat Q&A from from our viewers life. and from the officially a roast students that have done their homework, and they have plenty of questions for you. Let's see Much the time allows us for this chat after your presentation and thanks again for for accepting reputation. >> Dave Cavalcanti: All right. Thank you Christian really glad to be here and and For the invitation always good to be sharing this topic which you know, I love to talk about with the team in Brazil. And as you mentioned, you know, I am originally from Brazil. I'm from the state of pardonable called I graduated there in undergrad and then did my masters also there and then came to the US to do my PhD so I've been in the US for around 20 years and that's always good to keep the connections to my colleagues in Brazil. So with that, let me give a brief outline for this discussion today I separated into Parts first. We'll talk about the need of time critical Communications right from any applications and these are really exciting times and our share some of our experience and some of the applications that are driving these need for from networking with Precision Time and and I'm what we call time sensitive networking as well. And then we'll go a little bit into why Wireless is important. What are the special needs for wireless and challenges as well? and then on the next second part, I'll try to cover more the advances in the technology what is happening in Wireless? We call it a wireless DSN in general. For you know, representing Wireless technology that can provide more determinist Communications and time will cover a little bit of a Wi-Fi and a little bit of 5G and talk about some of the activities we are doing to drive the deployment of these the industry and ecosystem and yeah, I'll try to close with some feature directions and there are a lot of that interesting research challenges in this space. So I hope that will be interesting for you as well. So let's go ahead and start. You know, we only see increasing need for a low. latency High reliability strict time Precision tasks in many domains right many markets. One of the marks that driving a lot of these requirements in the last few years is industrial robotics. You know automation Factory automation. That's one area where people trying to be more efficient than trying to move to more softer defined systems. And that puts a lot of requirements on the network actually and I'll talk more exactly about how you can model that problem. But you know there is a need for for that kind of strict time and timeliness right the ability to deliver data within some bounded latencies in this market also other areas that is coming up recently is the immersive experience right the interactions with world. And the reward and this has to happen real time. And latency and Time Performance is very important as well. So those are just a few examples, but there you can find. time critical applications across many Industries power energy agreed within Vehicles autonomous vehicles. Those are all good examples of that, right? Just to illustrate one more one specific case right as I mentioned earlier. the industrial industry automation industry is really driving some of these requirements and one of the Transformations happening that Industries really moving from dedicated devices or controllers or sensors, right? that will run a control Loop. in the field distributed that a device only those one thing that's kind of traditional automation model the reason movement to trying to leverage a lot more software defined capabilities like in it right a lot of things move to the cloud. And this is Transformations also happening days domain, right? where a lot of these controls functionalities are trying to be moved, you know from dedicated Hardware. Which are very expensive to make and a very specialized to more general purpose compute right on the on the cloud or depending on the latest requirement. What we call Edge compute is a where you'll have some of this compute capability and infrastructure that's closer to. To the factory floor. Let's say right so that is the cloud Edge automation model. So there is a transition happening this industry. and that means the you know, if you think about controlling a robot right remotely from an edge serve or from a cloud you can think about. Okay. how do I get this sensor data and how I get the automation data back? And if the robot has to be controlled every 2 millisecond to tell the position of the arm, you can imagine this will be a very strict requirement for the network and that's what's happening in the industry. So we are trying to build networks that you know in the past was just trying to be optimizing throughput. And you know what we call the real time in the past was something like well browsing and you know interactions. Which you know, it's not at the same level as this new applications requiring right in terms of latency. Now, you're talking about single digit millisecond, you know, not 100 milliseconds. So these are really Time critical applications, and this is putting a lot more. requirements on the networks to deliver this data with determinism. Another area I mentioned briefly is the inversive experience, right? So again, extend the reality mixed reality of virtual reality where people interact with real-time systems. that's another good example. and in a way this kind of interaction has shared a problem with would the Industrial Automation industry as well. where you actually have to get data or sensing data out of the the real devices and then compute that in the cloud and then come back on the edge right and then come back with an actuation and show that in some kind of visualization, right? So that's another example of these Loop. You know, there is a control Loop here right in terms of sending time critical sensing or data. It can be used a tracking or can be tracking a robot and getting the actuation back which could be displaying information could be an action to a robot. So this is a fundamental requirement so that the network delivers this dating with boundary latency, but also they compute capability also is important to be done in real time, Right? And so that is that's what's writing their need for the advice of the networks and also they compute side right so that then we're going to you know what we call time sensitive networking is how you make general purpose networking. time critical right of meeting this time critical requirements and you know, the requirements can be divided in Humane categories one is is time synchronization. actually some applications do need access to Accurate or precise time references if you think about sensing and sense of fusion applications, there's an example where you'll need that. Another major requirements what we call timeliness is the ability to produce a result Data within a deadline, right? So that's very important for many applications, especially when you're doing that in software in the edge or in the cloud. You depend on the data being transmitted the compute action being executed within that that latest bomb so Think about this your things latency bounds or deadlines and then times synchronization, which means executing something at the right time, right? So those are the the fundamental requirements that we call it generically Time Performance. And here a few examples. examples. I already mentioned some right. You see the industrial control Loop where this tasks happened in a tightly isoctronous control. So keeping track of time. Keeping track of these. You know a latency bounds which are you know times when you have to transmit the sensing data or you have to receive the activation and if you miss that window a robot made me set task right or maybe miss a package in the conveyor belt. you know, these are some examples. Or in a external reality if you miss this actuation or the sensing you may have bad performance in the bad user experience, right? One of the big challenges to make this happen today. Is because we need all of these. To share the same network and the same computer resource, right? and like if you dig a little bit deeper into the industrial segment right you'll see actually Many factors already have Solutions right to control robots these Control Systems being running for decades already. So that's not really new technology. But the way they solve that problem is by creating dedicated networks actually dedicated field buses. Like there is a dedicated wire. To come, you know to connect a sensor to a controller, right? and that works fine, but you know only scale and specially that doesn't doesn't take advantage of standard networking Computing resources that are widely available, you know technology like ethernet and Wi-Fi or even cellular right? They had their own proprietary Solutions and that's a transition I mentioned. All these industry are trying to move to a model where you have a generic Network, you know, it could be your Wi-Fi network or your internet for structure your schedule Network. and then you need to run all these different types of applications in the same network. You know could be some applications are delayed tolerant, and that's fine. Some may need some real-time interaction users, but some really need hard real time guarantees, right and some are safety critical. And that's really the problem. How you You can run these applications in dedicated networks today, but you cannot really run all of them together in a same network. where you guarantee some of these. data right will be delivered, you know within certain that lines. so that's the challenge and you know to have these distributor Computing networking with the time it is and and there are two parts of the problem here at least from you know our side also at Intel the two ways. we looked at this is we need to have determinist within the system and by the system here in this slide. I mean like within the compute platforms right within our our city use our servers or client devices, right? And that's something we call time coordinary Computing. So you need to compute with Precision Time. And with the timing is execution right wise. but also now since we're Distributing Computing across the network The Edge and the cloud. We need the time it needs between this systems, right And that's where the time sensitive networking comes in where you need technology that can guarantee the data is delivered with the communism across. Across multiple systems, right? So so this is the overall problem. We need to solve. so now I'll dig a little bit deeper into the time sensitive networking side. So that's really the the networking capability. And the other thing we try to use here is all based on standard right we want to have open. platform that we use industry standards so that we can leverage, you know widely available silicon and devices that are not specifically built for a certain application right and the way we do that is by defining standards for this technology and they're already very well known successful standard connectivity like internet Wi-Fi 5G, right 6go come. And what? the DSN test group in nitripovi 802 group did always doing is to Define tools that can enhance. This underlying communication or connectivity standards to make them more deterministic right and to make them available in you know with with these guarantees and so that they can be used to enable those time critical applications. So this DSN tools are defined to build on top of the existing ethernet. Wi-Fi 5G. So they don't really replace those systems. They are kind of a link layer hence means that you put on top of the existing systems, right? So that's the way we look at TSN. And what are the core elements of this technology, right? There are many of them, but we can summarize them at least. What the industries actively working in implementing now, are these three things right at this point? One is time synchronization, right? How you synchronize time across devices in the network? And and the other is how you schedule your traffic or how you coordinate the resources in the network to make sure. That the time critical data is always protected against congestion, right? So one of the problems in the networking right when you're trying to share the network without these these types of applications that have mixed criticality. Is how you you know you at some point if the network is free. That's okay. Everything is very fast. The problem happens when there is congestion right? You have keys building up on this which is or the access points or the base stations. And then how a Time critical packet comes right that you need to send to a robot for example, and they find the congestion right? Then it gets dropped. Oh it gets delayed. So how you avoid that one of the approaches? Is to reserve resources ahead of time. to make sure that when the concretical traffic comes they're always going to be a resource to to handle it at the right time. So that's under these area of traffic scheduling your time over schedule you talk a little bit more and I'll give some examples always we apply this into Wireless domain for example. and the third pillar which is very important is how we manage these systems right because as You know, I I indicate with the scheduling case. If you have to reserve resource for something to ensure that I mean isn't You need to know ahead of time right? When is that gonna come because otherwise if it's random anytime that because very hard so there is a need to actually manage the the POS of the traffic in the network with some capables to detect those streams set them up and then configure the network resources so there's a lot of capabilities there. So here I won't go into the details of these standard. So that's a snapshot and there is a link here. You can actually explore. These are the different TSA and related projects in nitripoe in these different areas of synchronization latency reliability or configuration and management. And there are many ongoing projects. You know, there are. projects that are done and I show some examples of the implementations of them, but you know, this is ongoing work, right? that's still a lot more to be done in this space and on top you see there are profiles that means some markets may need just a subset of this feature. So what we do is the industries we get to get and Define a profile meaning what the options or subset of tools is important for industrial or for Aerospace or for automotive. You'll see that there are different profiles that are also defined. And you know from a protocol stack standpoint. You can look at as I mentioned briefly earlier right here same tools as something at the data link layer that builds on top of the the trees go in Mac layers. and then you can still run TCP IP on top and other protocols on top as well, right, but the key is the foundation is based on standard Hardware like standard Wi-Fi chips or internet chips of cellular 5G, you know chips. Okay, so let me go through moving through the some examples here. But again as I mentioned earlier there are profiles in different Industries. You can check this out later on. but I'll try to move more now into the Wi-Fi and the 5G part right and you know. specially because while all these started with wired communication, even there you have a problem of congestion, right? So for this type of time critical application, so you can imagine in Wireless this problem is even harder to solve, you know, but Wireless bring many Advantage right because you don't have wires you need Mobility. You need you can get flexibility of moving things around in a factory. You can have more reconfigurability. So it's really a good tool to have and that's what's driving the interest for also enabling these time sensitive features over Wireless Solutions. and just to give you our next stint of the how big the problem is, right? If you look at the and this figure on the left. the current consumer Wireless solutions that you have your standard Wi-Fi, you know or cellular 5G right the standard consumer grade. Today, you know your latency if you measure latency error rate, they are around these ballpark right things of milliseconds the best case you get right but 100 millisecond mostly. And internet with this TSN features is pushing down to microsecond, right? ditch the goal is really to get wireless from this consumer grade to much closer to what you can get on the wire basically adding more determinism had better reliability. and I think we are now in this ballpark of a millisecond, you know, that's where I'll show some results on that but you know With still a little bit far from the actual wire, but that's this scope of the problem right of the potential here, right? We're trying to get to and wireless different because there are different effects on the wireless channel right stochastic in nature. There are variations in frequency in the interface and all the things you have to deal with which is not the case for wire, which is more stable right with some isolation. But you know, that is a hope here right? Because we have new capabilities and wireless is always evolving. A few things to highlight are they latest. Wi-Fi 660 and 5G standards, which have more capabilities and I'll just highlight one figure you'll see in this to to size. here is really both of them are based on on a concept called ifdma where you enable more granular scheduling of the time in the frequency resources in the Wireless Systems. So that gives a a more capability for you to allocate resources and to control the the access to the channel and improve the time it is so those are some of the key features in these systems and you can see Even though these are completely different Wireless Technologies. They do share some of these fundamental features like fdma for example in Wi-Fi 6, you know and 5G. Okay, so let's continue now moving a little more into examples of how you're using this TS and features into into Wireless, right? First I'll start with Wi-Fi. So basically, there are two TSN features that we actually have already. defined standards for and actually have implemented it in some of the Wi-Fi devices that we have. One is time synchronization. So this is basically the ability to to transmit time from a clock what we call Grandmaster is a reference time that you can transmit that time across the network. And make sure that all devices have tightly synchronized blocks across the whole network. and you can use that clock to in your application. Let's say to to align the control cycle of the you know, an industrial automation system, right? where the sensors need to send data the right time you can use that for it, but you can also use that clock to coordinate activities on the network side. And that's the second feature here what we call time our scheduling. and this is based on the I triple standard called a2.1 qbz. And the time synchronization is also based on the standard called a total.oneas. with some Wi-Fi support for it through ftn or timey measurement FTM is the latest, you know, fine time measurement is We use okay. So first you synchronize every device and then you will Define a schedule where within Wi-Fi you have set of keys in each device. And the basic concept is you define windows that are protected. Some of these skills are protected. meaning like during certain times only one of those windows are open and everything else is close in that way you actually Make sure that you have a protected window over the air. and then when that time critical traffic comes it will be able to to get through the channel without you know completion. So that's one of the capabilities, you know that we have over over Wi-Fi with this feature. There are more Tools in wifi 6 that you can improve this even further I want going to the details, but you know some things like a trigger-based access. It's useful for that. of now again, this is just some data illustrating how these trim our shaping Works in Wi-Fi. you know basically as I mentioned you actually have to use and you'll reserve a time for the Tran critical queue what we call the tsmq and then you can use the rest of the time for other traffic and you repeat this cycle. And that's one way that you get. This performance on the right side you'll see. Without this feature you have latency that shift, you know that they vary a lot all the way up to 60 millisecond. but when you have this enabled and you create a cycle where you give this time Windows, you can actually bound the oscillators for the time critical flow, right? And because the time critical always gonna have a window certain times. So that's just an example to illustrate how this works. Briefly I mentioned I won't go in details here because we don't have time but you can check some reference on this. Wi-Fi with trigger base or fdma access allows you to switch from the traditional contention base. mechanism Wi-Fi where everyone's competing. for the channel And they may have collisions to a mode where the access point. schedules the transmission so basically the access point controls the access of the channel say I tell who is going to transmit at what time and then I can also have multiple stations transmit at the same time. That's what the fdma mode gives you so that actually reduce latency. And crazy reliability with WiFi as well. These are some. simulation results. We did some time back to illustrate. if you compare traditional Wi-Fi with this trigger base, you know you can get bound to the latest with a lot more reliability in more capacity. Basically, you can support let's say one millisecond you could support nine users compared to four in the traditional system, you know, and so on so that I mean the main point on this is this gives you a lot more control of the the Channel with Wi-Fi. that improves the feminism. another feature with Wi-Fi that we have implemented that comes from the TSN toolbox is called a redundancy. It's very simple concept right is you if you have something that's really time critical safety critical like controlling a robot. For example. you can already done this to it because if one link fails you have another one and you replicate the data across different paths. And you can apply this feature on a wired link as well. Some things are so critical as saying in airplanes or in vehicles right that they already have multiple wires so we can do the same over Wireless and it's even more important in Wireless because you are susceptible to more interference right and things you can't control. So what? Law is you basically use two radius within the device two channels at the same time. So basically to redundant links on Networks. and that show you know, these figures shows that if you have you know that redundant capability you can improve your reliability right for a given latency bound. You know by almost half, you know 50% high, right? Oh double the reliable here, right? So from 46 to 99% is some experience experiments we did right? So that's very simple concept basically just redundant links and set it in both links and the probability of the interference to happen in both things at the same time is lower and you if you need more redundancy, you have to keep adding more links, but eventually there'll be a point where you'll be, you know, not cost-effective right because you're really need to be real systems. Let me see if this works. Okay, this is to illustrate an example where we use some of these capabilities to control our robot over a Wi-Fi 6 Channel. And do find manipulation and and this graph shows. The latencies we are observing in this the histogram for the latency in this experiment right around you know, wanted to Two and a half three millisecond at the most right? these robot was operating at 2 millisecond cycle so every two millisecond we had to to send a command. So that that was the traffic. We're trying to secure you. Okay. Yeah, this is another figure. I will skip this one because of the time but basically illustrates that when you have the latency variation, oh when you have in these experiment right you can have the latest around 2 milliseconds me into it. More or less 1% there. Okay, so let's continue. Now. Look at what is next right in in the Wi-Fi with the ascent and then I'll get a little bit into the 5G. So I showed you some results that we had with Wi-Fi 5 Wi-Fi 6 now, we actually in wifi 7 so Wi-Fi 7 is just getting ready this month. Actually. They should be done with the spec and they're already products based on Wi-Fi 7 in in the market. So so that is the latest Wi-Fi 7 base on the 800.11b spec. One I'll talk about two main features that comes in Wi-Fi 7 one is called multilink operation. You remember I showed you the redundancy results those results were using two different chip sets right two radios for the same. compute device in Wi-Fi 7 actually a single Wi-Fi chip can actually have two links in it already by having these multilink feature. So you can do that. same concept now in a cheaper way, let's put that way right so you can operate within a single Wi-Fi device. Of course. It depends how many bands your support. We have Wi-Fi 2.4586. So at most three, I think that's that's a feasible thing. So this can help you improve throughput but Can help with latency and reliability. in another feature that we have included in Wi-Fi 7 is this concept of protected window that I mentioned earlier for scheduling? where the in the wireless Channel itself you have you can create this Protective Service periods called TWD service period where you can restrict the time. For only certain traffic and in that way match what we had with this cubic solution. So these are some of the the capabilities in the Wi-Fi 7 okay, so let me shift now a little bit on the 5G side. I I trust everyone heard about 5G I won't go into the details of 5G, but there is one component of 5G. that's very useful to achieve these that I mean is we're trying to get you right and this is called urlc Auto reliable all agency where the 5G system can guarantee some of these capables within the system. And then what we did in as an industry right in Wi-Fi like in wifi in 5G as well with defined. and 3gpp worked on this to Define standards to coordinate or connect the 5G Network to a TSA Network and synchronized time across the 5G system. So this is something that you know has been standardized as part of 5G release 16. Is how you transport time across the 5G system. There is some concepts of gateways called. device side PSN translator Network site here Center later that defies the mechanist to do this. And in that way you can have accurate time sink across this systems. There are also tools that you can do something very similar to what we showed in Wi-Fi where we can provide qos for certain flows. That guarantee that you're going to be delivered within a deadline using some of the 5G POS formatting. That this relies on capabilities of the base stations to a schedule. and you know some of the implementations of these are being developed at this point. so In in general right again, I we don't have time to go into the details, but there's a lot of good reference to about that as well. So to sum up, right? One thing we try to do is not only develop this capability right within our products or our systems. But also we need the industry to adopt that right and for that to actually have to work with steam. There's organizations or industry alliances to do that. I mentioned already plays a role 3gp as well. Wi-Fi Alliance Is specific to the TSN area? We actually created a industrialized called Avenue Alliance. And its goal is really to provide testing for this TSN specific features. for internet Wi-Fi and 5G So in Avenue allies, we actually create test plans. we create. a test tools and we actually have plugfest where the vendors come and test their devices to make sure they interoperate. We actually launching a certification from program for TSA and for ethernet and working on the next one which will be a wireless TSN certification program. You can check the website. There are also a lot of white papers that are available about this Technologies there even there are opportunities for universities to get involved as well. So this is part of the industry, you know adoption that worked that we have to do. And with that I'd like to conclude briefly, you know. pointing to some directions that there still need for a lot more work, right? I talked a lot today about time and latency, but you know, we need time a little later she guarantees but there is more right for example you need security for this system and actually now you have new threads because you have time synchronization. You need to secure those those assets right time, right and you need to secure the network to avoid impact on latency for them. Resilience is very important like I mentioned there are some ways to address it through redundancy, But you know Mobility how you do this at fast speeds right with changing conditions and you have to be efficient. because you know you could have a dedicated Network for a single device, but that's not gonna cut in in the real business right case so How you be more efficient how you can dynamically manage this networks? And that's when everywhere automation learning AI is gonna play a big role, role, right? I think you know given our time I would stop here and and you open for questions in you know, I can share this lies later. You'll see a few other things I have about future Wireless Systems like Wi-Fi 8 and CG. But I think we we >> Christian Esteve Rothenberg: Great. >> Dave Cavalcanti: can stop here and open for questions at this point. >> Christian Esteve Rothenberg: Thanks. Thanks for taking care about our time. >> Dave Cavalcanti: Yes, I have to be precise right so >> Christian Esteve Rothenberg: Yes. >> Dave Cavalcanti: I've been talking about it, you know. >> Christian Esteve Rothenberg: 2PM the students have commitments. Otherwise we would be for sure spending a lot of time talking about this interesting topic as you know in we are trying in Academia accessing some. and simulators to understand and the TSA and 5G Wi-Fi domains and tackle some of the open questions open research opportunities and let me I have a couple of so there's one question here over the YouTube channel. That was supposed by Everton. He he says I think this is this. Is pointed very well to your protocol stack, so The wireless TSN is doing this so creating this determinist. What about the the TCP the stack? Is there something that is companion that is being developed. in closer to the application on the apis. He mentioned the l4s. Congestion control. Is there something to highlight there across layer from the transport layer to the that you you would like to highlight. >> Dave Cavalcanti: Yeah, that's that's a interesting. point to explore you know TCP as we know right highest concession control and changes the bandwidth change the data rate and Actually, we have a paper that was published last year. I can't point you to the reference that explore when you use TCP. for these real time application actually you end up with some Behavior. That's not deterministic actually because of the way of TCP working, right? So When I put TCP in this trust in this protocol stack what I see the industry is moving to is. for the real-time flows when you have TSN under underlying Network to guarantee that the time it isn't you those real-time flows over UDP. Actually TCP is not a good choice for that. Of course, you may be able to enhance it and make it determinist, but that kind of defeats the purpose of TCP, right which is to control congestion and it can still be used on top of the same network, but you know, maybe more for the best effort traffic, right? but most of the solutions that are using time critical flows, it's more effective to do that with I know there are some solutions in l4s. I'm aware. There are some discussions even in Wi-Fi in i-340 as well and other Wi-Fi groups to how to adopt that and in a less point, I would say >> Christian Esteve Rothenberg: right >> Dave Cavalcanti: Is a very important point of cross cross layer optimization here, right? again, TSN stays in the lower layer, but if you have delays in the stack in the device right higher layer, it's gonna affect your intu and performance. So start the delay across the stack and share information between layers is going to be important and it's something that you know, it's a little bit open area because not a lot has been done traditionally. We try to separate all these layers, >> Christian Esteve Rothenberg: yes, >> Dave Cavalcanti: right minimizing information across them. >> Christian Esteve Rothenberg: but but then the cross layer of demisations when they hit and they bring against them they >> Dave Cavalcanti: I'll just give you one example, you can check that out. There is a protocol highly protocol called DDS. that's using raw stew. over as a middleware over UDP and actually they we had some papers on these and there is a spec that's open. So openly available discussing. how you do cross-lay optimization from DDs to TSN? >> Christian Esteve Rothenberg: baby >> Dave Cavalcanti: I think that's a good reference to >> Christian Esteve Rothenberg: That's interesting. >> Dave Cavalcanti: to to check it. Yeah. >> Christian Esteve Rothenberg: So maybe maybe we can see something like a quick deed over UDP for TSN at >> Dave Cavalcanti: something >> Christian Esteve Rothenberg: TSN over you be a congestion control transport related. Yeah, it makes sense. So >> Dave Cavalcanti: Good. >> Christian Esteve Rothenberg: thanks. Yeah, please share. The pointers will forward to the students and we can post them also us as notes in the in the seminar web page. here is another question. I'm now going through the students questions before they watch the talk, Okay, and one was interesting about home users. So and you mentioned about applications of DSN in our homes that that you are aware of >> Dave Cavalcanti: Yes, so one interesting application I mentioned in a home is actually the main driver for that also Enterprises is external reality, right? so having you know as I mentioned, it's kind of a control loop as well, right because you track in a in Excel reality application you track the head movement you compute the rendering of the next scene. you're gonna show right or the next asset you're gonna show and then you have to transmit that back to the user and this has to happen in very, you know, you know with type deadlines if you have a lot of Jitter in that you will have a bad performance. So that is one of applications and We actually have it prototype that I will try to you know, actually miss if I can show this very quickly here where we applied TSN. to You can see here. We build a digital twin of a network using external reality and this is AR. And these application actually we are transmitting this between the compute and the network using Wireless TSN, you know capability and we can show How bad the performance is when you have when you don't have the TSN feature because you have You know congestion right and and register compared to a performance when you have a TSN, so That we are actively working in these areas as well. So I just want to point that you >> Christian Esteve Rothenberg: That's >> Dave Cavalcanti: know. >> Christian Esteve Rothenberg: that's very interesting where we are playing out also with the meta Quest right now. in our lab at Intrigue and and we are building a Not that digital dream, but something to be visualized Wi-Fi performance over space. maybe we can >> Dave Cavalcanti: and that's great and you know you feel if you search also find Intel and The Meta has a collaboration where they actually have some optimized Wi-Fi >> Christian Esteve Rothenberg: put >> Dave Cavalcanti: capabilities for these type of applications. That's also interesting you can see >> Christian Esteve Rothenberg: that's that's very good to know. okay here I spotted another question that again it's time late these days. It's about ai ai machine learning. And you pointed also your in your last slide about AI as a future. and Tool, let's say to >> Dave Cavalcanti: Right. Yeah. >> Christian Esteve Rothenberg: improve some of the can you share some of the that can be publicly shared? uses. So applications of AI or machine learning >> Dave Cavalcanti: Mm-hmm. >> Christian Esteve Rothenberg: to the challenges of TSN. I don't know maybe Scheduling I can imagine that you can learn and propose better schedulers after You observe what what would you say? What is the intersection of AI and ml for TSN and wireless TSN that you are aware of or that you can imagine that there are opportunities. >> Dave Cavalcanti: Yes. I think the one you mention is is very clear one area that we have to explore more. Is because this is scheduling in a TSN capable network is very complex actually computationally complex as well. And especially when they let Network increases and you have you know to compute updates in the schedule, so How can can machine learning do better than some of the Existing Solutions based on network. how close for example other traditional things aristics that are there? there? So that's a fairly one area. But also AI is playing a role everywhere now, right and it will impact the TSN in other ways. For example. this control look right case I know people are exploring robotics. where the model-based controls are being changed or it's exporting. how reinforcement learning can be used in the control loop as well to replace some of the control. Loops, right? So that's another area where AI is being used and that means you need if you do that, right the AI workload will need to meet strict Time Performance. Right? So you need to consider that as well. So that's another connection to the TSN. Right because in the end to end you need to meet the latency deadline. in the third one is how AI is using time right if you're doing sense of fusional things like that for databases time synchronization may play a role, right? But yeah, so those are I mean everyone is looking at AI tools right now and see how it improves but you know, I think these are few examples different they configuration. is one of the top >> Christian Esteve Rothenberg: and right >> Dave Cavalcanti: top areas, you know automating that with AI. >> Christian Esteve Rothenberg: right and you mentioned also here security and safety and we know security always goes into the other direction of time right because crypto functions take time. What is out there? What are what are opportunities over in the security space or what are challenges you mentioned a couple like securing the time, but can you elaborate maybe or give some examples? so if you know about >> Dave Cavalcanti: Yes, I can give some examples and these are things we are working on right now. I mean, I don't have things I can share here. But you know, we actually yeah, maybe that's some results on to share but you know one of the very straightforward things that come like When you're building a network that every device and it the actions of network is really based on accurate time. You know that time becomes an important asset right? So if someone's You know gets into one of these systems that propagates the time across the network start to add small. Changes to the time right that guess propagate and what that means. is that could result in a schedule being changed and then you know these Gates that I mention right will be operating at the wrong time. So is more impacting time can actually have a big but impacting the performance and these are things that have not really been explored when people think about security they think about the You know. That the usual issues like oh Jamie, right? Okay, you can jam my Wi-Fi network. Sure. But there are attacks that are most subtle and now You know you an expert could get into some of these systems is start changing time. how we protect those. systems against those attacks, you know, I think that's one at least one of the areas that we started to explore. that may be many other threads right, but I think at least this is One Direction that we are looking to to protect, you know. In this area, you know. >> Christian Esteve Rothenberg: Yeah. okay, I don't usually take this type of questions, but I think I would have take it for this time. It's a little bit more versus personal one. Yeah, so he asks he/she. I don't know. It's anonymous. peer review system that we are using so that the question is, how did you move from Brazil and get Harry Potter Intel to work on on this hot topics That would be very interested in working top leading tech companies such as Intel were made. You don't need to go into the details. But what would you recommend to this grad students for for a successful career internationally big companies What would be your your recommendation? >> Dave Cavalcanti: Yeah, I I >> Christian Esteve Rothenberg: I'm sorry to move out of the taking >> Dave Cavalcanti: Yeah. >> Christian Esteve Rothenberg: but I >> Dave Cavalcanti: Sure, and I'll happy to share my my approach to this. Right so and and I would say first you guys already. have a good head start because you are in a great University. Okay, that's a good start build the basis. Right? And I know in the company is great And that's it. one job and the way I get into this right is really coming. And I know people that came to Intel from Brazil, but it's very limited. They came to Acquisitions and you know to be honest like in Intel Advocate. We don't hire directly from Brazil's very rare. unless maybe we have a collaboration with the university and and we find a special case, right but I came to do my PhD in the US, you know, or you can do a postdoc right? But once you get into the us or Europe I think we have a lot more connections with big companies on the research side. You know, like I'm I'm within Intel labs. And we do a lot of collaboration with universities actually trying to start something with Christian as well. And participating those who open up opportunities for you to do internships. For example, that's how I got into the industry first when I first moved to us, I didn't know I'm gonna stay there, but then I did my PhD and then in the middle Phillips research actually all from the internship, so I go there for internship. They offer me a job and then I get you know and you just get into the system, so you know get to the US or to to Europe. I think that will give increase your chances right and or get collaborations from Brazil with these >> Christian Esteve Rothenberg: and right >> Dave Cavalcanti: companies. I think that's another option as well, you know. >> Christian Esteve Rothenberg: great. I think that's a great message. That's what we as you mentioned that we are doing also in the smartness engineering resource center with with Ericson and fabes support and we encourage a lot the mobility of PhD students and we have great opportunities of a past exchanges for for research and just to set a few more examples. Not from it's a it's in the industry. It's not Intel but it's Qualcomm that >> Dave Cavalcanti: Yeah. >> Christian Esteve Rothenberg: you also know well and you know, the CEO is Christiano Amon and alumni of our faculty. >> Dave Cavalcanti: That's great. Yes. >> Christian Esteve Rothenberg: and another example of talent that went internationally out and is successful and that's an excellent. reach to the next week topic the next week seminar will be by another alumni from our faculty. It's Gabriela surita, and she's a senior research engineer at Google and working on this Deep Mind AI tools over for engineering and reinforcement learning methods and so that's how we can close right at 2PM Brazil time. And thank you Dave. That was very nice. Well what you had to share on again, I appreciate your time and your message to encourage students to work hard and be successful. >> Dave Cavalcanti: Yeah, thank you Christian. Thanks everyone and and wish you all the best. >> Christian Esteve Rothenberg: Thank you that. >> Dave Cavalcanti: Okay, let's okay. Thank you. >> Christian Esteve Rothenberg: Take Care Bye. 
>> Christian Esteve Rothenberg: Good afternoon. Thursday seminar Thursday together and Today we have another alumni Gabriela surita. It was not so long. Ago that you were around in the faculty. and now you are in Google working on the Hot Topic probably today Hot Topic generative Ai and talking to machines hopefully Are real. I know you are real. And but with these times you never know so as usual I don't spend time in presenting our speakers. I let you to introduce yourself. We can Google you as as well and find more about you and thanks again for for being here and talk to you in about 40 minutes with the Q&A and the traditional chat. >> Gabriela Surita: No worries. Thanks for the introduction and the invite it's a pleasure to be here and also like always a pleasure to to talk. at unicamp, whatever and whenever I can for a curious like a fun fact Christian was my I think the professor of my last. a last class that I attended at fact, so yeah, and it was oh, I don't know. I'm very glad that yeah. It was a very good A network systems and network Labs. Yeah, so the title of this talk that I want to talk today is talking to machines practical introduction to generative AI so as as you might know, this is a very hot topic and I'll try to bring like a balanced and motivationary like overview of the topic. But yeah try to save some time for questions in the end as well. It's a new content. So you be my guinea pigs, and I hope that you enjoy it very yeah, it might not be super well-rounded a little bit about me. I work at Google deepmind. I've been here for slightly over two years now before that I spend five years. Trying to blogging some problem. and their internship at Mozilla and as you already know I worked I studied at unicomp a couple years ago. I started 2012 then I changed to Computer Engineering 10 2013. so, yeah, and For the last two years. I've been working Text generative Ai and mostly co-generation applications. What once you talk about today is I think first. I'm more introductionary presentation of generative AI. so I think how does one train so large sequence Model A Very like one million meters overview, but how does a modern chatbot works and this first part will be more Technical and more basic. So this is content that you can probably find elsewhere online, but I'll try to squeeze the most content I can in 20 minutes. I think the second part is more abstract. And it's about interpretations philosophy and discourse of AI. I think this is one of the particular topics. I'd like to study and research not only at my job, but at my free time it's more abstract, but hopefully very useful as well. It's also like a compilation that you won't find very easily in the internet. So feel free to ask questions and email me afterwards if you want to learn more. and finally, his applications so it's very Hands-On tips and traps to using these models I think like it's also basic very applied but also some of the tricks you only learn when you use these models on a daily basis. some assumptions we'll College of classification regression in your networks. This is not mandatory, but it will help it will be a data abstract if you don't know this, but I hope you can get something out of it. I highly recommend some pass interactions with GPT and Gemini if you haven't played with this language models, you should I think it's one of the hot topics of our age. So at least some experience playing with then is useful and most topics apply to multimodo agents, but of course my my examples are Skewed towards code and text these are my areas of expertise. So, okay. Let's Ivy. What's generating AI? the name generative AI is already telling every time someone talks about you when they say generative Aid probably differentiating this type of AI from another type of AI so I think Station is very very useful. Why is it generative? It's not classification. It is not regression and it's not like multitask like multi multiple choice action selection. So what it is, then we're usually talking about machines or systems capable of producing human consumable. I think there's gorgeous important digital content. So usually text videos images or audio. There are more but I think these are usually the main ones. so it's generative not predictive I think sometimes it's used for it is used to differentiate against. artificial general intelligence as well. So this is not necessarily General, but it's producing content. And sometimes use it to differentiate against narrow AI this term is disputed. basically the idea is the outputs are unbounded in they usually involve digital content. I think the most obvious examples are chatbots assistants. That includes check GPT Gemini Claude and some open source alternatives. and it also includes text to image text to audio audio to audio all these types of systems, but I think like the most obvious examples are these chat Bots? This chatbot assistance and the text to image systems. And they are often associated with creative tasks. So it's predictive AI is usually class associated with like predictive statistical tasks and generative AI is usually associated with open-ended creative tasks. I don't love this term. I'm going to talk about more later, but it's it's something that people talk about. So how does one how does a modern AI assistant? work the very usually the most interesting in the hottest one to talk about but let's try to understand I think a little bit what happens when you go into gemini or when you go into chat GPT and you press enter like you type something in presenter. I think one of the key probably the key component of this systems is sequence modeling. and here's there's a note like not all generative AI Model but most generative AI includes a sequence modeling component even textual image systems that sometimes have another diffusion or variational Auto encoder component. It's usually the key one of the key ideas is sequence modeling And I think understanding sequence Extremely useful to understand this systems. what is a sequence model? sequence model is a predictive system that is capable of produce predicting the next element of a sequence. In text I think. Texas probably the easiest one to understand this phenomena You can often think about this as predicting the next word or predicting the next character. So in a sequence when you start with the you can try to predict what comes next predicting what comes next after the is really hard. But if I start adding words and try to predict what comes after the quick brown Is probably Fox the quick brown fox jumps over the lazy dog. This is a very common. sentence in English, so you I think the main idea behind it is like if you have something that is represented by a sequence. And you take a subsequence of that sequence. Can you predict what comes next? Think this is the key idea of sequence model. another interesting example is like if you go to mathematical sequences if I give you you can maybe try to guess maybe you're gonna count with numbers repeat it so it could be one one to two three three four four, but it can the Fibonacci sequence. And in this case, I think if I give you zero woman, two three five. It's probably the Fibonacci sequence if I gave you 8 and 13, it's very very likely the the Fibonacci sequence also if I gave you some lyrics like if I start with what we're halfway there. You probably leave sending to Living a Prayer like it's of course, there are some examples that we can probably not do the same thing. But maybe you're halfway there something else better. The wall is quite telling so I think this is the key idea behind the sequence model. Can I predict what comes next in a sequence given just what I've seen so far. So this is the key idea no mathematics so far, but you can ask like, how do you do this? And but those familiar with the answer like for those familiar with the field. we usually do it with a probabilistic model. I tend to think that's specially for language. This is not this is not an obvious leap like and I think it's probably one of the most powerful ideas of information Theory and information transmitting in general like I think Shannon made this leap of faith that if you can. rap sentences out of their meaning and just model them as probabilities you can actually do something some. like quite powerful things like and basically the the key idea behind this is that you can express. the probability of the next element in a sequence for example the probability of fox given the quick Brown as You can express this as probability distributions over the next word and you can probably maximize probability of fox given to a quick brown and you can maximize probability of 13 given the beginning of the supernet. She's sequence. You can also measure like probabilities probably the probabilities of bananas given the experiment suggests is very unlikely. but I think this idea of expressing. sequence the next element of a sequence as a probability over all possible elements vocabulary and given the conditionals of the previous elements of the sequence a very very powerful idea dear and I think like is a direct result from Shannon's theory of communication. So this is quite important. We're gonna go back to this sometime soon. And just a reminder like outputs are probability distributions over the symbols. model things in the world like if you have a bunch of sequences that you want to model. You probably want these distributions to reflect. the natural distributions of these words in in the world. So if you start with the word and you try to predict what comes next this is usually a very wide distributions, there are plenty of words that can come out of after the but if you start with a very targeted sequence like the World War Two ended in the year. And you try to predict the word that comes next. This is a very narrow distribution Like I think the it's very unlikely someone is gonna say 2024 there. Like it's probably 90. and 1945 so when you're trying to optimize for this distributions your objective needs to account for that like you have sequences that are very easy to predict and you have sequences that have a very broad distributions one way to represent this. And to formulate this problem I won't dive into detail here. But it's a direct result of information Theory as well. You can optimize for the cross entropy between you're measuring. and the distribution of the underlying model This is the detail. but if you take a machine learning course like you will see this result being brought quite frequently. Okay, so we can predict the words given what comes before? you can maybe try to go and build a big table of all the words in the internet and say like I want to predict what comes next. like if you try to do that with a big table, like all the words that come before in the word that comes next. It's possible. This is not impossible. But the number of entries in your table depends on the number of permutations of symbols not all symbols are occur in nature like language is has structure. but if you try to build that big table This is Not Practical for sequences over 10 cent symbols, basically because this grows with a factorial of of the number of symbols. so the big table is works up to three four symbols, but this is a tech that is used. and it's available on phones for example for text completion. This is changing. Some phones are using deep learning Technologies now, but up to sometime in the past every time you got suggestions when typing on on your phone. This was usually the tech that you were using like you had this big table of three or four symbols that come before the one that you're typing right now and you you have a table of occurrences of how how the next symbol what what's the most next thing will So we skipped to neural networks. And again, I won't be able to teach neural networks in a slide, but neural networks are usually an approximation of this table using dense vectors I think the the word neural network is has very little resemblance with how the brain works, but you They have been historically being called neural networks, and but it's basically a very smart projection of these sequences of words in in And if you can approximate singles to a vector combine symbols of vectors. To a sequence Vector efficiently and create a predictor. from the vector of singles to the next symbolt and you can model much longer sequences. And this idea has been around for quite some time. I think like at least 45 years. but there has been like huge qualitative leaps in the last few years. So this is not a new idea. You can take a sequence map to a dance Factor. and try to predict the next element in the sequence given this dense vector but has recently been transformed. So this is a plot of like time versus number of parameters going back to this. Like usually this projection is represented by a matrix multiplication and the number of elements in this Matrix multiplications is usually called like the weights or the parameters of the neural network. And if you take like this number of elements used to do the projection and time. It's almost an exponential like a few 2022. There are questions whether this exponential is holding or not a lot. Lots of like tech companies are not. disclosing the sizes of their models anymore but it's being almost like a clear exponential in the last few years. So this projections are becoming more powerful and Powerful the number of the length of the sequences and the accuracy of the representation of the sequences has been improving and I think one of the key components again, I won't dive into too many too many details here. is to a new Not new anymore. Five year old five six year old. no architecture in in like newer networks called the Transformer. This is the T in GPT. And I think the key idea of a transformer is that you can format sequences. and train At us like at a single inference step. So it's a technological leap is like a I think it's like going from like steam engines to combustion engines, but like if you take this this architecture is much much more efficient to train and to to model these sequences so you can actually grow the number of parameters. again for those unfamiliar with this topic. This made is maybe a little bit abstract. But remember we're taking input sequences and predicting the next word. And we do this through a projection. to dance representation and this projection is done. by Matrix Matrix multiplications to with very very large tables. So this is a hard concept to grasp. But at least I find it hard. but this is how text neural network works. So but then there is a question like what what this is actually doing is a very fancy auto complete like it's a very very fancy way of predicting the next word. do you go from that to an assistant? And you don't have to understand the underlying techniques from the fancy autocomplete to understand these next exports. So if I wasn't clear so far maybe reset that content and try to understand this part. So how does an AI system works you have this very fancy autocomplete that the sequence model. There's Auto regressive decoding. but this is only a part of the system There is something that I like to highlight that is formatting. So when you press enter on a chat board. Formatting is the first step. So what is formatting doing? So formatting then tokenization tokenization is basically a conversion to individual words. Currently they are not always words. Sometimes they are pieces of words, but formatting I think is the first step and one that is really important to understand if you want to work with applications. so formatting, is this process that takes conversation and transforms into a completion autocompletion problem and how you usually works is that you have a very specific format to represent conversations on these models. and they can often look like this like you have a preamble that you call a system instruction. Where you start you are very helpful AI assistant that is designed to help. People or humans on their daily tasks. Your name is Bob. like and then can add some examples to that this conversation you can have the user say Well, hey, what's your name? Then you have the agent say hi. I'm Bob. Are your your help for your assistant? and then you can input the the question that someone is asking like so how do you work? So if I ask the question, how do you work? should you a chatbot? That is only doing formatting? You sometimes you can assume that there. is this brand Preamble that looks like a conversation but that's hidden away from the user like so this was how we designed chatbots up to a certain point in the past like You would basically create this Preamble that looks like a conversation. but you always had a new line to this conversation in the end of the of the of the text and that is creating like this notion of But in practice like this is just a completion system, but it's a completion system that was tricked to believe it is in a conversation already and you just asking another question. So this is a very key idea of how do you turn? a completion system into a conversational AI And this happens to search to a certain degree on all the modern AI systems formatting differs a lot. I think between Agent to agent I think this particular formatting is the one that Gemma the open source Google models use but it's a real example like you can go to GMI and type this put this format and it will work. Then I think there is something that you also do on top of this models that is called instruction tuning. So it's how you update this. So after you train on the internet, so you have that sequence model. that was trained on all the sequences that you could Use so usually all the public web. or you can also go there and in the end of your process like Update the sequences to follow a certain pattern and this certain patterns are. usually Chosen and very curated and you can do this during force them responses. You can do this. You better adhere to the format that you use doing during during inference when you use the system. you can also use it to tune tone and form. But so what does these can look like I think if you're building a general assistant, you don't want to get into trouble buying doors in one candidate or or the other candidate in a political status setting so you can train the model to never answer your question like who is the best candidate? you can train the model to don't reply things that it doesn't know you can train them all that you have a name and this is now now can be removed from this Preamble and putting to the weights via this process called instruction tuning. These are usually like done after the fact. But it's important to keep in mind that all chatbots. They have these process. I think all the major ones at least. I'd like to think as these as editorial decisions like it's the same as the newspaper like the editor takes a look at what goes in the end and chooses how how the agent should reply but all chatbots have editorial decisions either implicit or explicit. This process is like I won't dive into the underlying techniques, but there are methods they get increasingly complex. You can do supervised fine-tuning. This is a method that looks a lot like Modeling the web but you can also do reinforcement learning. But anyways, this is just a footnote. But there are very important unanswered questions by this description of AI like how does fancy autocomplete get so good at random stuff. like and for those who use these chatbots you probably seen it doesn't feel like how fancy autocomplete like it feels like something else. this is a question. that I think this descriptive analysis doesn't. answer like it's we we actually don't know and I think we needed more teary around issue answer this question. And why even though it's really good at some stuff. Sometimes it makes like stupid mistakes. Like we you probably seen one or two examples in the web. Like if you follow the media, sometimes you will see chatbots making mistakes the these are some time fascinations. but we also don't have a good theory for why that happens sometimes. but I think the main question is like why why do why do they feel so good like and why do they actually are very good at some some of these tasks? So then I get to my second part interpretation philosophy in this course. So why should why talk about philosophy of AI I think the mainstream discourse is often very ideally a logical. and no matter what source do you you like I think It is ideological and this is not a bad thing. Everyone subscribes you an ideology. The risk is not being aware of the ideologies that are in communicated. And I think often things presented as pure fact are often under heavy dispute and I think specially if you're not like in the field it can be very hard to identify But yeah, I think hey guys permeated by several ideological assumptions that can actually get in the way of using them correctly. I think this is a this is probably the most important if you're trying to apply AI. these ideological assumptions can make you make mistakes. and so try to be careful and at least be aware of those when you're using them. So there are five open questions that I want you to talk a little bit about today. but there are no obvious answers. There's no right answer to these like again. these are philosophical discussions, but it's good to be aware of them and and to identify them in this course and when you see it. I think the first one is emergent. ism Is philosophical belief that phenomena cannot be explained by the son of parts? So it's usually be just used to describe Consciousness economy. Consciousness how it arrives from like matter from from brain matter and sometimes people use it in economy like transactions individual transactions are make economy arise but it's hard to describe economy by individual transactions. You can also think like a river as flowing but it's it's not an accurate description of a Reaver or it's very hard to just reduce it to that. so emergencies shows a lot on AI discords. I just used to describe it. it's a philosophical belief. it's not clear. So yeah, it's been showing a lot on AI discourse for some time now. there is this notion that if you make the autocomplete more powerful There are emergent behaviors that start to appear. So if you start to increase your match Matrix projection in your in your autocomplete. Component it becomes like more and more powerful. but then again some people argue that this is a mirage like it's just in incremental ways, but depending on how you measure performance and what types of metrics are you using? this scenes emergent? but if you there's often often, there is exists a metric that if you look It's incremental. So. It's a disputed topic. I think like we don't fully understand why this happens, but some people argue that it is not a real phenomena. But yes, I'll I think it's good to be aware. Emergency is used a lot to talk about AI. The second one is a more technical one, but is the notion of scaling loss. So this is this is a measured phenomena. So this has this has science backing it up. So if you look at If you increase so this plot is the validation loss. So the validation. This equation here. Actually the log of it, but if you look at the validation laws over the number of that the number of parameters or equivalent the compute that you spend to train the model. It almost certainly looks like a power law so. What's the interpretation of this? double the compute usually that our Fancy Auto Complete component gets twice as better. This is the interpretation of the scaling pericola. again, it's an empirical law. It's not a it's not a natural law. And we don't fully understand why it happens. But it's a major phenomena. But twice is better than what like this. It just twice is better at predicting. the next word and there's some research showing like okay, it gets better at predicting the next Ward. But doesn't necessarily get better at things we care about. So there's this notion of scaling laws. Do not scale. I think this is a very interesting paper showing that okay, you get better at predicting you get twice as better at predicting the next word, but what does this makes as impacting the real world and there's some research showing that sometimes it actually gets worse at things you care about. Even though it gets better predicting the next word. So the notion of scaling loss over the over the validation function is verified in previously, but the notion that making your model larger and larger you always get better. This is under dispute. And I think like it's important to keep in mind because sometimes people will say if I just have more compute Everything would be solved. This is not this is not entirely like a consensus Third thing let's talk about the Turing test. So you probably heard about the Turing test. and it's this notion of if a machine can fool who human that it's intelligence like it that it it looks like a human then it's probably intelligent like and this is not an accurate description of the Turing test, but I highly recommend learning about the Turing test if you don't But I assume most of you have heard about the tuning test be at this point. the Turing test Is an increased interesting for experiment? But I think this leads to if you subscribe to the Turing test. This leads you a trap called the trap. This is a term coined by this guy. but this basically like the Turing test makes us obsessed about replicating human intelligence. And sometimes we're not good at recognizing intelligence. That doesn't look human. a good example of days I think is like for systems that predict protein structures. This is something that humans cannot do. but AIS are incredibly powerful it and I think like it's probably one of in my opinion the biggest transformations in. the biggest opportunities in in the AI field But it's hard to recognize this as intelligence because it doesn't look like human intelligence. so I think the Turing trap invites us to think what types of intelligence We we can identify. That do not look human but are incredibly powerful and can help us. Improve our lives and live more fulfilling lives. Finally, I think there is one called entrepromorphizing it's the second to last. You must end to apply human-like values you inanimate and human no human entities. I think this has been highly documented. Throughout history. It doesn't only happen with ai's I think like this goes back to myths since I don't know the problem materials or that they're even an older like tellings of of this myth. but every time we entropomorphize we tend to lower our guard and the info size with with the thing that we are anthropomorphizing and anthropomorphizing is not bad per se like I don't think there is a problem with entrepromorphizing but it can be used as a design choice and I think that is where things get a little bit sketchy like you This can be used to deceive and make AIS look like they are more interesting and more intelligence that they look. and I think like often it's a design choice to include them. So you should always ask yourself Like is this AI trying to flirt with me? Like I think we've seen this happen before we always be always be doubt when when this happens like always take it with pinch of salt. There's also does notification like it's making a look like Disney characters. I think that's that's also phenomena that happens. Yeah. So be aware of entrepromorphizing and I think the final thing that I want to talk about is the shenon Divide. I think this is a term that I invite invented but if you go back to this light on Shannon, I think like by construction We removed any notion of meaning from symbols and we built our whole system around this property that so by construction we removed meaning so it's hard to reintroduce meaning like when you take the outputs of assistance without a theory a theory of semantics. So this doesn't say that these series of semantics don't exist, but you need one like otherwise, it's there is a There's a flaw in your like. epistemic step by step. reasoning on how how many arises so I think the Practical conclusion here is be very careful when you apply meaning and this usually showing metaphors to the outputs of these systems. So be careful with these metaphors like things like the AI understands or the a reasons or the AI thinks like these are very very powerful. That danger is Mentor Force. And these often are applied to negative metaphors as well things like hallucinate misinterpret lie. These are not like we don't have a theory of meaning for these AIS and I think we it's a huge opportunity to build theories of meaning to these AIS, but we don't have one. that is widely acceptable. So careful with metaphors specially, you know technical setup. finally my last part of the talk and no I'm a bit late on time, but I think I'll try to leave time for questions of try to be fast here. Formatting is incredibly powerful tool. I think I mentioned that before and I will try to give some examples about this. so formatting is sometimes called prompting engineering. I think this term gets a lot of hype so I try to be try to not use it. but If you do formatting right it's an incredibly powerful tool to like Take the best out of these powerful fancy autocomplete systems. there are two things that I want to talk about one is demonstrations or their strength Chain of Thought and I think there's a third one that includes light for editing. So editing, how do you take an account how to complete and you make a model that can edit things in the middle of a sentence? There is a trick that you apply to. to these language models where you with a certain probability like you reshuffle sentences, and you add this special words like pre-made and stuff. to learn how to predict things in the middle of a sentence and if you apply this to a very small percentage of your data set you're basically get this ability without damaging all the other code completion capabilities. and the fun fact is that this is something that you can use in your applications and most people don't know about it. but if you are in the middle of a document you can add edit things there just by using this trick. There's also happens. There are other representations that apply to the same thing think the second thing I want to talk about is is few short prompting. sometimes people try to describe a problem to these motors, but remember they are very good how to complete. So they're very good at finding the the patterns in examples. So I always suggest try to give 50 demonstrations. It's usually much better than explaining her problem and highly recommend trying few short prompting. There's also the opportunity to do instruction tuning. It's an effective but more expensive tools. So I always recommend people to try few short prompting first. And then try instruction tuning if that works and usually can do it very Cloud apis. You don't have to build instruction tuning yourself. And finally, I think this is another. Tip very engineering one don't roll. your don't train your own base models. Like it's a incredibly hard and expensive Endeavor like the same way You don't rewrite Linux. You don't build your own file system and you don't write your own database. Don't try to build your own auto complete try to leverage the ones that you exist in the market of the Shelf. Or maybe with some fine tuning and unless you're doing this for research like avoid building your own language models use one from cloud. You're usually better off this way if you're trying to apply them. Finally be aware of bias. The internet is not a neutral place. So representation in these models predicting the next word can contain bias. my main take away here is lots of these issues can be fixed and mitigated by these editorial decisions in the chatbot. But don't delegate decisions where accountability is important. We don't know how to fully mitigate those yet. Try to apply mitigations whenever you can. I think that's it. I think it's an incredible moment to be in this field, but also very loud one. So try to listen think and plan before you launch. I hope this this tips are useful as if you ever decide to apply AI. I will open the questions. I have some ideas here besides my >> Christian Esteve Rothenberg: Thanks. Thanks, Gabriela. >> Gabriela Surita: content if >> Christian Esteve Rothenberg: And well a lot lots of interesting stuff. I loved your animations the the and and the pictures that help also in getting in this election in the message and they're interesting topics that are thoughts and you mentioned so I was browsing here through the questions of the students as you know, they one of their activity previous to each seminaries a little bit of homework trying to identify which are the five most top relevant literature giving at the similar abstract. I will share it with you later. but and I was roasted through their questions and most of them they touch your last topic on on ethics. AI considerations Do you are you aware about techniques or efforts? Say this very relevant issue around the multiple dimensions. behind ethics racism sexism and can you shed some light? so that this biases can be overcome in the future or or there is no hope or what. what are your thoughts? >> Gabriela Surita: Yeah, sure. I think this I wish I had more time to cover this, but I also wanted to talk about this broader philosophical things, but Umbias, I think. I think one important thing is like this systems. They are practically reproducing probabilities. We see in the internet. And it's these two statements are very important. I think the internet is not a neutral place and it's not an accurate representation of society. So sometimes people are associate like all the probabilities that are in the internet should be reflected in the world. I think that's that's a very heavy statement because like not everyone is in the internet and not everyone has been in the internet for the same time. so but I I strongly believe this is the best we can get as a picture of this Society. What I think is there is a lot of like at least a written one if you can even if you introduce books and everything like that, it's very hard to go beyond that. what I think there are solutions is lots of these issues can be mitigated during that instruction tuning process. and there are some really interesting ways of like for example mitigating gender bias doing that step like you can basically make sure your priors for Male doctor and female doctor are well represented when you do translation from English to Portuguese for example where gender shows in inwards like like doctor so there's techniques that can be applied there. And they should be applied. And so there are lots of re interesting research there. Measuring these effects. I think it's incredibly important so you should be always taking these into account when you launch a system. You should try to actually measure these and I think the best way to accurately measure and know that if you're making that you should probably post on own your product launch is have like a diverse pool of people testing your system like if you don't have those, it's much harder to detect those but I I believe there is opportunities in research to do like more. To include fairness in your in and reduce bias in your responses in this like especially in this post training instruction tuning step. >> Christian Esteve Rothenberg: Right. I like this statement that don't or this recommendation. Don't delegate decisions where there is a second ability behind them. >> Gabriela Surita: Mm-hmm. >> Christian Esteve Rothenberg: That's a that's a very good one. Let me jump now over the YouTube channel to the chat. There is a technical question that you I really don't know. What are these vanilla Auto encoders the Ariel asks, do you consider vanilla auton coders as generative AIS since I can explore the Latin space? I don't know. what's late in space. I'm not doing good. >> Gabriela Surita: Yeah. >> Christian Esteve Rothenberg: Maybe you can Say something. >> Gabriela Surita: I think I think that this yeah, yeah, I think like not not a specialist in the field also, but like vanilla Auto encoders, I think are these models where you try to recreate content with a network? Via bottleneck so you have like very small number of parameters at some point in your network. that basically give you the the chance to do to tweak these parameters to the generation. I consider them generative because I think there is a more technical. definition of generative AI that I didn't use in my in my slides that are That is basically like something where the output space can be bigger than the input space. so I can it can be considered. I think generative AI technology, but I think it's more borderline. I don't think everyone agrees with this. But yeah, I would say I would say yes >> Christian Esteve Rothenberg: Okay good. And I don't know if you would like to comment Popped up. I was browsing here a number of students. They they probably saw the release of GPT for o and they want your opinion And also what is next or would you comment would you like to come and if you want to skip about gptu for RO that's also fun or if and if not, they >> Gabriela Surita: I mean I yeah, I can comment I think on a high level like I don't I think it's a very the field is very very active and there's lots lots of things going out by all the players. I sometimes I think there's a risk of over indexing to one particular release. It's everyone is releasing through eodically. I think it's when you look when you zoom out of it. you can see incremental steps in each one of these models. I think this really is in particular I think. Like this this week there has been like two important releases, I think. The one from openai and also the one doing Google I/O. I think these are both like interesting releases. I what I like about them. The most is that I think there is like this. huge push for Smaller models that I think I potentially use for in some ways. >> Christian Esteve Rothenberg: Okay. >> Gabriela Surita: But I want comment on details about about the model because I I think >> Christian Esteve Rothenberg: it's >> Gabriela Surita: it's a territory. >> Christian Esteve Rothenberg: it's fine, but I had to do my my work >> Gabriela Surita: Yeah. >> Christian Esteve Rothenberg: as well here by browsing through the questions, which I will share with with you as well and about this smaller scales, LA llms. And is there some space for also like Smalls scale applications in embedded devices in Salesforce that they don't have so much compute but for some specialized very much specialized tasks could be useful. That's one another question that popped up or Or we need to rely on very large-scale Computing >> Gabriela Surita: That's an interesting question. And I think that touches a topic that is very close in my heart. I didn't talk about this today, but like I mostly work on code generation. I think that's that's one of my my favorite topics and I think it's one of point of view one of the most successful any useful applications over here. And I think a lot lots of the recent models are embedded models that you can run in your computer with reasonable latency. So I I think there is tons of Not so narrow but narrower applications that are not like chatbots and they're highly crafted to one particular use case. I think code completion is a very successful story. Yeah, so I personally believe in that direction a lot. and that's not just say that they less powerful. Like don't get me wrong if you take have more model that works on a smartphone now. It's probably better than like not probably it's definitely better than the best model from two years ago. So they're quite I highly believe this this application of like more targeted in own device use cases. >> Christian Esteve Rothenberg: and Andy >> Gabriela Surita: I also like the idea of use cases that do one thing but one thing >> Christian Esteve Rothenberg: right >> Gabriela Surita: really really well this gets into products, but I >> Christian Esteve Rothenberg: like specialized LL and aims for a certain domain for a certain task >> Gabriela Surita: yes. >> Christian Esteve Rothenberg: that a lot of sense right because a like there's this statement if you want to do everything. You think you do everything? Well, you don't do nothing. Well or at all, right, that's >> Gabriela Surita: Yeah. >> Christian Esteve Rothenberg: interesting And as you know, we in the electrical and Computing engineering we will also look at Hardware a lot. There were a few questions here Hardware related. So today most of the Computing are exploding gpus right fpga's. there is also in the AI domain the tensorflow the it has a specific tensor Processing Unit. Do you see also Hardware about Harvard that is going to be designed for specific and type of or generative AI applications to accelerate it. Maybe it's already happening and I don't know. Sorry. >> Gabriela Surita: I think it's definitely happening. Like I think the latest generation of tpus and gpus are mostly designed for AI and I think they are extremely like generative. here and they're extremely like overfitted to this problem like I think they're huge huge opportunities there like I think. if you again something that is Up For Debate, but if you think AI is a revolution. Similar Au and and this is the view of this revolution, right and usually if you look back to like revolution industrial Revolutions in the past like you need few to so you need energy and you need like maybe it's not the but it's the iron like I think you have to think about the best comparison here, but so lots of companies that focus on Hardware are extremely well valued and I think like there's tones of opportunity research opportunities on doing better hardware for for large language models and generative AI Technologies if you work on this field, I'm very happy for you. There are loads of career opportunities for you. >> Christian Esteve Rothenberg: good at last question the same that we had we are already 2PM So what would you recommend to our students rather than undergrad if they want to to succeed and have and be employed like you in Google deepmind? What what is the key there? What would you what would you be your We're talking to to this undergrads undergrads and grats. >> Gabriela Surita: Be curious. I think don't be afraid at applying to these hard positions. Try to learn. a niche very well that like on the things that are hot try to find a niche that you do very very well and other people don't know much about it. >> Christian Esteve Rothenberg: feel they >> Gabriela Surita: And yeah curious overall. Yeah, that's that's why me advice I'd say >> Christian Esteve Rothenberg: great. >> Gabriela Surita: and don't be afraid to replying to these positions like it's hard to get in at Google but not that hard and it's it's a pretty interesting company to work with to. >> Christian Esteve Rothenberg: I loved it. So, And if you step by of course come over the faculty and say hello and keep the good work and Hope looking forward for the upcoming releases over there. >> Gabriela Surita: It was a pleasure to be here. >> Christian Esteve Rothenberg: Yeah. >> Gabriela Surita: Thank you so much. Bye now. >> Christian Esteve Rothenberg: thank you. See you all next Thursday with the next talk. 
